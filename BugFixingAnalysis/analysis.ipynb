{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5faa2f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "print(\"Libraries loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61029fd1-6950-46e5-a55e-f7640e922f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully. First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>bug_commit_sha</th>\n",
       "      <th>file_path</th>\n",
       "      <th>commit_message</th>\n",
       "      <th>issue_title</th>\n",
       "      <th>issue_body</th>\n",
       "      <th>llm_model</th>\n",
       "      <th>complexity_before_cc</th>\n",
       "      <th>complexity_before_cognitive</th>\n",
       "      <th>...</th>\n",
       "      <th>complexity_after_llm_avg_params</th>\n",
       "      <th>complexity_after_llm_total_tokens</th>\n",
       "      <th>human_tests_passed</th>\n",
       "      <th>human_lines_added</th>\n",
       "      <th>human_lines_deleted</th>\n",
       "      <th>human_total_diff</th>\n",
       "      <th>complexity_after_human_cc</th>\n",
       "      <th>complexity_after_human_cognitive</th>\n",
       "      <th>complexity_after_human_avg_params</th>\n",
       "      <th>complexity_after_human_total_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-10-11T15:02:54.981034</td>\n",
       "      <td>mahmoud/boltons</td>\n",
       "      <td>4815fc8dd1768da5f2d903846d2ab994aa57b0cf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Test and fix for #348 (#349)</td>\n",
       "      <td>LRU .values() and dict return old entries</td>\n",
       "      <td>Hi,\\r\\n\\r\\nFirst of all thanks for the excelle...</td>\n",
       "      <td>manual_llm</td>\n",
       "      <td>198</td>\n",
       "      <td>135</td>\n",
       "      <td>...</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>True</td>\n",
       "      <td>44</td>\n",
       "      <td>9</td>\n",
       "      <td>53</td>\n",
       "      <td>199</td>\n",
       "      <td>135</td>\n",
       "      <td>1.68</td>\n",
       "      <td>5014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    timestamp        repo_name  \\\n",
       "0  2025-10-11T15:02:54.981034  mahmoud/boltons   \n",
       "\n",
       "                             bug_commit_sha  file_path  \\\n",
       "0  4815fc8dd1768da5f2d903846d2ab994aa57b0cf        NaN   \n",
       "\n",
       "                 commit_message                                issue_title  \\\n",
       "0  Test and fix for #348 (#349)  LRU .values() and dict return old entries   \n",
       "\n",
       "                                          issue_body   llm_model  \\\n",
       "0  Hi,\\r\\n\\r\\nFirst of all thanks for the excelle...  manual_llm   \n",
       "\n",
       "   complexity_before_cc  complexity_before_cognitive  ...  \\\n",
       "0                   198                          135  ...   \n",
       "\n",
       "   complexity_after_llm_avg_params  complexity_after_llm_total_tokens  \\\n",
       "0                          SKIPPED                            SKIPPED   \n",
       "\n",
       "  human_tests_passed human_lines_added human_lines_deleted human_total_diff  \\\n",
       "0               True                44                   9               53   \n",
       "\n",
       "  complexity_after_human_cc complexity_after_human_cognitive  \\\n",
       "0                       199                              135   \n",
       "\n",
       "  complexity_after_human_avg_params complexity_after_human_total_tokens  \n",
       "0                              1.68                                5014  \n",
       "\n",
       "[1 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data columns and types:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 29 columns):\n",
      " #   Column                               Non-Null Count  Dtype  \n",
      "---  ------                               --------------  -----  \n",
      " 0   timestamp                            1 non-null      object \n",
      " 1   repo_name                            1 non-null      object \n",
      " 2   bug_commit_sha                       1 non-null      object \n",
      " 3   file_path                            0 non-null      float64\n",
      " 4   commit_message                       1 non-null      object \n",
      " 5   issue_title                          1 non-null      object \n",
      " 6   issue_body                           1 non-null      object \n",
      " 7   llm_model                            1 non-null      object \n",
      " 8   complexity_before_cc                 1 non-null      int64  \n",
      " 9   complexity_before_cognitive          1 non-null      int64  \n",
      " 10  complexity_before_avg_params         1 non-null      float64\n",
      " 11  complexity_before_total_tokens       1 non-null      int64  \n",
      " 12  llm_patch_applied                    1 non-null      object \n",
      " 13  llm_tests_passed                     1 non-null      object \n",
      " 14  ai_lines_added                       1 non-null      object \n",
      " 15  ai_lines_deleted                     1 non-null      object \n",
      " 16  ai_total_diff                        1 non-null      object \n",
      " 17  complexity_after_llm_cc              1 non-null      object \n",
      " 18  complexity_after_llm_cognitive       1 non-null      object \n",
      " 19  complexity_after_llm_avg_params      1 non-null      object \n",
      " 20  complexity_after_llm_total_tokens    1 non-null      object \n",
      " 21  human_tests_passed                   1 non-null      bool   \n",
      " 22  human_lines_added                    1 non-null      int64  \n",
      " 23  human_lines_deleted                  1 non-null      int64  \n",
      " 24  human_total_diff                     1 non-null      int64  \n",
      " 25  complexity_after_human_cc            1 non-null      int64  \n",
      " 26  complexity_after_human_cognitive     1 non-null      int64  \n",
      " 27  complexity_after_human_avg_params    1 non-null      float64\n",
      " 28  complexity_after_human_total_tokens  1 non-null      int64  \n",
      "dtypes: bool(1), float64(3), int64(9), object(16)\n",
      "memory usage: 357.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "# load results.csv into pandas dataframe\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\"llm_bug_analysis/results/results.csv\")\n",
    "\n",
    "    # display first 5 rows to see if it loaded correctly\n",
    "    print(\"Data loaded successfully. First 5 rows:\")\n",
    "    display(df.head())\n",
    "\n",
    "    # print summary of data types, check for issues\n",
    "    print(\"\\nData columns and types:\")\n",
    "    df.info()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: results.csv not found. Make sure you have run the analysis pipeline first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5454f292-3135-488d-b24b-2810c7c7f491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No LLM results found in the data. Skipping AI success rate calculation.\n"
     ]
    }
   ],
   "source": [
    "# convert llm tests passed into numeric type\n",
    "\n",
    "df_ai = df[df['llm_tests_passed'] != 'SKIPPED'].copy()\n",
    "if df_ai.empty:\n",
    "    print(\"No LLM results found in the data. Skipping AI success rate calculation.\")\n",
    "else:\n",
    "    # this part only runs if there is data\n",
    "    df_ai['llm_success'] = df_ai['llm_tests_passed'].astype(bool)\n",
    "    overall_success_rate = df_ai['llm_success'].mean()\n",
    "    print(f\"Overall LLM Success Rate (where attempted): {overall_success_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3c32dee-a5c7-4f61-8ae0-ebf9ee295695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No LLM results found in the data. Skipping the repository summary plot.\n"
     ]
    }
   ],
   "source": [
    "# group data by repo and calculate success rate for each\n",
    "\n",
    "if df_ai.empty:\n",
    "    print(\"No LLM results found in the data. Skipping the repository summary plot.\")\n",
    "else:\n",
    "    # this part only runs if there is data about the llm fix\n",
    "    success_by_repo = df_ai.groupby('repo_name')['llm_success'].mean().sort_values(ascending=False)\n",
    "\n",
    "    print(\"LLM Success Rate by Repository:\")\n",
    "    print(success_by_repo.apply('{:.2%}'.format))\n",
    "\n",
    "    # Create a bar chart to visualize this.\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    success_by_repo.plot(kind='bar')\n",
    "    plt.title('LLM Test Pass Rate by Repository')\n",
    "    plt.ylabel('Success Rate')\n",
    "    plt.xlabel('Repository')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.gca().yaxis.set_major_formatter('{:.0%}'.format)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfbdadd-cc89-426f-9bcd-0899f5cd04b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
