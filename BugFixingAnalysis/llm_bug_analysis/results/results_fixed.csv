timestamp,repo_name,bug_commit_sha,source_file_path,test_file_path,context_classes_count,context_functions_count,context_snippets_count,context_structural_classes_count,context_recent_changes_count,context_related_commits_count,context_test_functions_count,commit_message,issue_title,issue_body,llm_model,llm_provider,llm_prompt_tokens,llm_completion_tokens,llm_thinking_tokens,llm_total_tokens,complexity_before_cc,complexity_before_cognitive,complexity_before_avg_params,complexity_before_total_tokens,llm_patch_applied,llm_tests_passed,llm_tests_passed_count,llm_tests_failed_count,llm_tests_skipped_count,llm_tests_errors_count,ai_lines_added,ai_lines_deleted,ai_total_diff,complexity_after_llm_cc,complexity_after_llm_cognitive,complexity_after_llm_avg_params,complexity_after_llm_total_tokens,human_tests_passed,human_tests_passed_count,human_tests_failed_count,human_tests_skipped_count,human_tests_errors_count,human_lines_added,human_lines_deleted,human_total_diff,complexity_after_human_cc,complexity_after_human_cognitive,complexity_after_human_avg_params,complexity_after_human_total_tokens,env_setup_time_seconds,llm_generation_time_seconds,ai_test_time_seconds,human_test_time_seconds,total_time_seconds
2025-11-15T17:43:53.410783,mahmoud/boltons,4815fc8dd1768da5f2d903846d2ab994aa57b0cf,boltons/cacheutils.py,tests/test_cacheutils.py,9,6,5,9,5,3,0,Test and fix for #348 (#349),LRU .values() and dict return old entries,"Hi,

First of all thanks for the excellent library!

I'm seeing strange results when using `LRU`: after replacing an existing entry with a new value, seems like the `keys()` and `values()` methods return the old value, instead of the new one. Using `__getitem__` and `.get` we have the expected behavior.

Here is a test showing this behavior:

```python
def test_lru():
    from boltons.cacheutils import LRU

    cache = LRU()

    # Add an entry.
    cache['a'] = 1
    
    # Normal __getitem__ access. 
    assert cache['a'] == 1  # passes.
    # Convert to dict.
    assert dict(cache) == {'a': 1}  # passes.
    # Another way to access the only value.
    assert list(cache.values())[0] == 1  # passes.

    # Replace the existing 'a' entry with a new value.
    cache['a'] = 200

    # __getitem__ works as expected.
    assert cache['a'] == 200  # passes.

    # Both dict and accessing via values() return the old entry: 1.
    assert dict(cache) == {'a': 200}  # fails.
    assert list(dict(cache).values())[0] == 200  # fails.
```

This test fails in the last asserts (of course to see the 2nd failure one needs to comment the first):

```pytest
λ pytest foo.py
======================== test session starts ========================
platform win32 -- Python 3.10.9, pytest-7.4.2, pluggy-1.3.0
rootdir: c:\Users\bruno\projects\boltons
configfile: pytest.ini
collected 1 item

foo.py F                                                       [100%]

============================= FAILURES ==============================
_____________________________ test_lru ______________________________

    def test_lru():
        from boltons.cacheutils import LRU

        cache = LRU()

        # Add an entry.
        cache['a'] = 1

        # Normal __getitem__ access.
        assert cache['a'] == 1  # passes.
        # Convert to dict.
        assert dict(cache) == {'a': 1}  # passes.
        # Another way to access the only value.
        assert list(cache.values())[0] == 1  # passes.

        # Replace the existing 'a' entry with a new value.
        cache['a'] = 200

        # __getitem__ works as expected.
        assert cache['a'] == 200  # passes.

        # Both dict and accessing via values() return the old entry: 1.
>       assert dict(cache) == {'a': 200}  # fails.
E       AssertionError: assert {'a': 1} == {'a': 200}
E         Differing items:
E         {'a': 1} != {'a': 200}
E         Use -v to get more diff

foo.py:23: AssertionError
====================== short test summary info ======================
FAILED foo.py::test_lru - AssertionError: assert {'a': 1} == {'a': 200}
========================= 1 failed in 0.04s =========================
```

At first it seems like a bug? Or perhaps this usage is not meant to be supported?",SKIPPED,SKIPPED,SKIPPED,SKIPPED,SKIPPED,SKIPPED,143,101,1.97,2891,SKIPPED,SKIPPED,0,0,0,0,0,0,0,SKIPPED,SKIPPED,SKIPPED,SKIPPED,TRUE,413,0,0,0,2,1,3,143,101,1.97,2892,3.1539220809936523,SKIPPED,SKIPPED,4.583956718444824,4.678085088729858
2025-11-15T17:50:56.052366,mahmoud/boltons,4815fc8dd1768da5f2d903846d2ab994aa57b0cf,boltons/cacheutils.py,tests/test_cacheutils.py,9,6,5,9,5,3,0,Test and fix for #348 (#349),LRU .values() and dict return old entries,"Hi,

First of all thanks for the excellent library!

I'm seeing strange results when using `LRU`: after replacing an existing entry with a new value, seems like the `keys()` and `values()` methods return the old value, instead of the new one. Using `__getitem__` and `.get` we have the expected behavior.

Here is a test showing this behavior:

```python
def test_lru():
    from boltons.cacheutils import LRU

    cache = LRU()

    # Add an entry.
    cache['a'] = 1
    
    # Normal __getitem__ access. 
    assert cache['a'] == 1  # passes.
    # Convert to dict.
    assert dict(cache) == {'a': 1}  # passes.
    # Another way to access the only value.
    assert list(cache.values())[0] == 1  # passes.

    # Replace the existing 'a' entry with a new value.
    cache['a'] = 200

    # __getitem__ works as expected.
    assert cache['a'] == 200  # passes.

    # Both dict and accessing via values() return the old entry: 1.
    assert dict(cache) == {'a': 200}  # fails.
    assert list(dict(cache).values())[0] == 200  # fails.
```

This test fails in the last asserts (of course to see the 2nd failure one needs to comment the first):

```pytest
λ pytest foo.py
======================== test session starts ========================
platform win32 -- Python 3.10.9, pytest-7.4.2, pluggy-1.3.0
rootdir: c:\Users\bruno\projects\boltons
configfile: pytest.ini
collected 1 item

foo.py F                                                       [100%]

============================= FAILURES ==============================
_____________________________ test_lru ______________________________

    def test_lru():
        from boltons.cacheutils import LRU

        cache = LRU()

        # Add an entry.
        cache['a'] = 1

        # Normal __getitem__ access.
        assert cache['a'] == 1  # passes.
        # Convert to dict.
        assert dict(cache) == {'a': 1}  # passes.
        # Another way to access the only value.
        assert list(cache.values())[0] == 1  # passes.

        # Replace the existing 'a' entry with a new value.
        cache['a'] = 200

        # __getitem__ works as expected.
        assert cache['a'] == 200  # passes.

        # Both dict and accessing via values() return the old entry: 1.
>       assert dict(cache) == {'a': 200}  # fails.
E       AssertionError: assert {'a': 1} == {'a': 200}
E         Differing items:
E         {'a': 1} != {'a': 200}
E         Use -v to get more diff

foo.py:23: AssertionError
====================== short test summary info ======================
FAILED foo.py::test_lru - AssertionError: assert {'a': 1} == {'a': 200}
========================= 1 failed in 0.04s =========================
```

At first it seems like a bug? Or perhaps this usage is not meant to be supported?",SKIPPED,SKIPPED,SKIPPED,SKIPPED,SKIPPED,SKIPPED,143,101,1.97,2891,SKIPPED,SKIPPED,0,0,0,0,0,0,0,SKIPPED,SKIPPED,SKIPPED,SKIPPED,TRUE,413,0,0,0,2,1,3,143,101,1.97,2892,2.5563182830810547,SKIPPED,SKIPPED,4.583728313446045,4.647908449172974
2025-11-15T17:57:31.321917,mahmoud/boltons,4815fc8dd1768da5f2d903846d2ab994aa57b0cf,boltons/cacheutils.py,tests/test_cacheutils.py,9,6,5,9,5,3,0,Test and fix for #348 (#349),LRU .values() and dict return old entries,"Hi,

First of all thanks for the excellent library!

I'm seeing strange results when using `LRU`: after replacing an existing entry with a new value, seems like the `keys()` and `values()` methods return the old value, instead of the new one. Using `__getitem__` and `.get` we have the expected behavior.

Here is a test showing this behavior:

```python
def test_lru():
    from boltons.cacheutils import LRU

    cache = LRU()

    # Add an entry.
    cache['a'] = 1
    
    # Normal __getitem__ access. 
    assert cache['a'] == 1  # passes.
    # Convert to dict.
    assert dict(cache) == {'a': 1}  # passes.
    # Another way to access the only value.
    assert list(cache.values())[0] == 1  # passes.

    # Replace the existing 'a' entry with a new value.
    cache['a'] = 200

    # __getitem__ works as expected.
    assert cache['a'] == 200  # passes.

    # Both dict and accessing via values() return the old entry: 1.
    assert dict(cache) == {'a': 200}  # fails.
    assert list(dict(cache).values())[0] == 200  # fails.
```

This test fails in the last asserts (of course to see the 2nd failure one needs to comment the first):

```pytest
λ pytest foo.py
======================== test session starts ========================
platform win32 -- Python 3.10.9, pytest-7.4.2, pluggy-1.3.0
rootdir: c:\Users\bruno\projects\boltons
configfile: pytest.ini
collected 1 item

foo.py F                                                       [100%]

============================= FAILURES ==============================
_____________________________ test_lru ______________________________

    def test_lru():
        from boltons.cacheutils import LRU

        cache = LRU()

        # Add an entry.
        cache['a'] = 1

        # Normal __getitem__ access.
        assert cache['a'] == 1  # passes.
        # Convert to dict.
        assert dict(cache) == {'a': 1}  # passes.
        # Another way to access the only value.
        assert list(cache.values())[0] == 1  # passes.

        # Replace the existing 'a' entry with a new value.
        cache['a'] = 200

        # __getitem__ works as expected.
        assert cache['a'] == 200  # passes.

        # Both dict and accessing via values() return the old entry: 1.
>       assert dict(cache) == {'a': 200}  # fails.
E       AssertionError: assert {'a': 1} == {'a': 200}
E         Differing items:
E         {'a': 1} != {'a': 200}
E         Use -v to get more diff

foo.py:23: AssertionError
====================== short test summary info ======================
FAILED foo.py::test_lru - AssertionError: assert {'a': 1} == {'a': 200}
========================= 1 failed in 0.04s =========================
```

At first it seems like a bug? Or perhaps this usage is not meant to be supported?",SKIPPED,SKIPPED,SKIPPED,SKIPPED,SKIPPED,SKIPPED,143,101,1.97,2891,SKIPPED,SKIPPED,0,0,0,0,0,0,0,SKIPPED,SKIPPED,SKIPPED,SKIPPED,TRUE,413,0,0,0,2,1,3,143,101,1.97,2892,2.5575592517852783,SKIPPED,SKIPPED,4.575561285018921,4.649102687835693
2025-11-15T18:03:01.875629,mahmoud/boltons,4815fc8dd1768da5f2d903846d2ab994aa57b0cf,boltons/cacheutils.py,tests/test_cacheutils.py,9,6,5,9,5,3,0,Test and fix for #348 (#349),LRU .values() and dict return old entries,"Hi,

First of all thanks for the excellent library!

I'm seeing strange results when using `LRU`: after replacing an existing entry with a new value, seems like the `keys()` and `values()` methods return the old value, instead of the new one. Using `__getitem__` and `.get` we have the expected behavior.

Here is a test showing this behavior:

```python
def test_lru():
    from boltons.cacheutils import LRU

    cache = LRU()

    # Add an entry.
    cache['a'] = 1
    
    # Normal __getitem__ access. 
    assert cache['a'] == 1  # passes.
    # Convert to dict.
    assert dict(cache) == {'a': 1}  # passes.
    # Another way to access the only value.
    assert list(cache.values())[0] == 1  # passes.

    # Replace the existing 'a' entry with a new value.
    cache['a'] = 200

    # __getitem__ works as expected.
    assert cache['a'] == 200  # passes.

    # Both dict and accessing via values() return the old entry: 1.
    assert dict(cache) == {'a': 200}  # fails.
    assert list(dict(cache).values())[0] == 200  # fails.
```

This test fails in the last asserts (of course to see the 2nd failure one needs to comment the first):

```pytest
λ pytest foo.py
======================== test session starts ========================
platform win32 -- Python 3.10.9, pytest-7.4.2, pluggy-1.3.0
rootdir: c:\Users\bruno\projects\boltons
configfile: pytest.ini
collected 1 item

foo.py F                                                       [100%]

============================= FAILURES ==============================
_____________________________ test_lru ______________________________

    def test_lru():
        from boltons.cacheutils import LRU

        cache = LRU()

        # Add an entry.
        cache['a'] = 1

        # Normal __getitem__ access.
        assert cache['a'] == 1  # passes.
        # Convert to dict.
        assert dict(cache) == {'a': 1}  # passes.
        # Another way to access the only value.
        assert list(cache.values())[0] == 1  # passes.

        # Replace the existing 'a' entry with a new value.
        cache['a'] = 200

        # __getitem__ works as expected.
        assert cache['a'] == 200  # passes.

        # Both dict and accessing via values() return the old entry: 1.
>       assert dict(cache) == {'a': 200}  # fails.
E       AssertionError: assert {'a': 1} == {'a': 200}
E         Differing items:
E         {'a': 1} != {'a': 200}
E         Use -v to get more diff

foo.py:23: AssertionError
====================== short test summary info ======================
FAILED foo.py::test_lru - AssertionError: assert {'a': 1} == {'a': 200}
========================= 1 failed in 0.04s =========================
```

At first it seems like a bug? Or perhaps this usage is not meant to be supported?",gemini-2.5-pro,gemini,5443,355,11330,17128,143,101,1.97,2891,FALSE,FALSE,0,0,0,0,14,13,27,N/A,N/A,N/A,N/A,TRUE,413,0,0,0,2,1,3,143,101,1.97,2892,2.558866500854492,107.48283433914185,N/A,4.575755358,114.61745619773865
2025-11-15T18:08:31.401919,mahmoud/boltons,4815fc8dd1768da5f2d903846d2ab994aa57b0cf,boltons/cacheutils.py,tests/test_cacheutils.py,9,6,5,9,5,3,0,Test and fix for #348 (#349),LRU .values() and dict return old entries,"Hi,

First of all thanks for the excellent library!

I'm seeing strange results when using `LRU`: after replacing an existing entry with a new value, seems like the `keys()` and `values()` methods return the old value, instead of the new one. Using `__getitem__` and `.get` we have the expected behavior.

Here is a test showing this behavior:

```python
def test_lru():
    from boltons.cacheutils import LRU

    cache = LRU()

    # Add an entry.
    cache['a'] = 1
    
    # Normal __getitem__ access. 
    assert cache['a'] == 1  # passes.
    # Convert to dict.
    assert dict(cache) == {'a': 1}  # passes.
    # Another way to access the only value.
    assert list(cache.values())[0] == 1  # passes.

    # Replace the existing 'a' entry with a new value.
    cache['a'] = 200

    # __getitem__ works as expected.
    assert cache['a'] == 200  # passes.

    # Both dict and accessing via values() return the old entry: 1.
    assert dict(cache) == {'a': 200}  # fails.
    assert list(dict(cache).values())[0] == 200  # fails.
```

This test fails in the last asserts (of course to see the 2nd failure one needs to comment the first):

```pytest
λ pytest foo.py
======================== test session starts ========================
platform win32 -- Python 3.10.9, pytest-7.4.2, pluggy-1.3.0
rootdir: c:\Users\bruno\projects\boltons
configfile: pytest.ini
collected 1 item

foo.py F                                                       [100%]

============================= FAILURES ==============================
_____________________________ test_lru ______________________________

    def test_lru():
        from boltons.cacheutils import LRU

        cache = LRU()

        # Add an entry.
        cache['a'] = 1

        # Normal __getitem__ access.
        assert cache['a'] == 1  # passes.
        # Convert to dict.
        assert dict(cache) == {'a': 1}  # passes.
        # Another way to access the only value.
        assert list(cache.values())[0] == 1  # passes.

        # Replace the existing 'a' entry with a new value.
        cache['a'] = 200

        # __getitem__ works as expected.
        assert cache['a'] == 200  # passes.

        # Both dict and accessing via values() return the old entry: 1.
>       assert dict(cache) == {'a': 200}  # fails.
E       AssertionError: assert {'a': 1} == {'a': 200}
E         Differing items:
E         {'a': 1} != {'a': 200}
E         Use -v to get more diff

foo.py:23: AssertionError
====================== short test summary info ======================
FAILED foo.py::test_lru - AssertionError: assert {'a': 1} == {'a': 200}
========================= 1 failed in 0.04s =========================
```

At first it seems like a bug? Or perhaps this usage is not meant to be supported?",gemini-2.5-pro,gemini,5443,355,11330,17128,143,101,1.97,2891,FALSE,FALSE,0,0,0,0,14,13,27,N/A,N/A,N/A,N/A,TRUE,413,0,0,0,2,1,3,143,101,1.97,2892,2.558607816696167,107.48283433914185,N/A,4.577775478363037,114.61921763420105
2025-11-15T20:17:21.188023,mahmoud/boltons,4815fc8dd1768da5f2d903846d2ab994aa57b0cf,boltons/cacheutils.py,tests/test_cacheutils.py,9,6,5,9,5,3,0,Test and fix for #348 (#349),LRU .values() and dict return old entries,"Hi,

First of all thanks for the excellent library!

I'm seeing strange results when using `LRU`: after replacing an existing entry with a new value, seems like the `keys()` and `values()` methods return the old value, instead of the new one. Using `__getitem__` and `.get` we have the expected behavior.

Here is a test showing this behavior:

```python
def test_lru():
    from boltons.cacheutils import LRU

    cache = LRU()

    # Add an entry.
    cache['a'] = 1
    
    # Normal __getitem__ access. 
    assert cache['a'] == 1  # passes.
    # Convert to dict.
    assert dict(cache) == {'a': 1}  # passes.
    # Another way to access the only value.
    assert list(cache.values())[0] == 1  # passes.

    # Replace the existing 'a' entry with a new value.
    cache['a'] = 200

    # __getitem__ works as expected.
    assert cache['a'] == 200  # passes.

    # Both dict and accessing via values() return the old entry: 1.
    assert dict(cache) == {'a': 200}  # fails.
    assert list(dict(cache).values())[0] == 200  # fails.
```

This test fails in the last asserts (of course to see the 2nd failure one needs to comment the first):

```pytest
λ pytest foo.py
======================== test session starts ========================
platform win32 -- Python 3.10.9, pytest-7.4.2, pluggy-1.3.0
rootdir: c:\Users\bruno\projects\boltons
configfile: pytest.ini
collected 1 item

foo.py F                                                       [100%]

============================= FAILURES ==============================
_____________________________ test_lru ______________________________

    def test_lru():
        from boltons.cacheutils import LRU

        cache = LRU()

        # Add an entry.
        cache['a'] = 1

        # Normal __getitem__ access.
        assert cache['a'] == 1  # passes.
        # Convert to dict.
        assert dict(cache) == {'a': 1}  # passes.
        # Another way to access the only value.
        assert list(cache.values())[0] == 1  # passes.

        # Replace the existing 'a' entry with a new value.
        cache['a'] = 200

        # __getitem__ works as expected.
        assert cache['a'] == 200  # passes.

        # Both dict and accessing via values() return the old entry: 1.
>       assert dict(cache) == {'a': 200}  # fails.
E       AssertionError: assert {'a': 1} == {'a': 200}
E         Differing items:
E         {'a': 1} != {'a': 200}
E         Use -v to get more diff

foo.py:23: AssertionError
====================== short test summary info ======================
FAILED foo.py::test_lru - AssertionError: assert {'a': 1} == {'a': 200}
========================= 1 failed in 0.04s =========================
```

At first it seems like a bug? Or perhaps this usage is not meant to be supported?",gemini-2.5-pro,gemini,5443,327,8867,14637,143,101,1.97,2891,FALSE,FALSE,0,0,0,0,12,13,25,N/A,N/A,N/A,N/A,TRUE,413,0,0,0,2,1,3,143,101,1.97,2892,2.6487579345703125,64.11321449279785,N/A,4.581784725189209,71.34375715255737
2025-11-15T21:08:55.559469,mahmoud/boltons,4815fc8dd1768da5f2d903846d2ab994aa57b0cf,boltons/cacheutils.py,tests/test_cacheutils.py,9,6,5,9,5,3,0,Test and fix for #348 (#349),LRU .values() and dict return old entries,"Hi,

First of all thanks for the excellent library!

I'm seeing strange results when using `LRU`: after replacing an existing entry with a new value, seems like the `keys()` and `values()` methods return the old value, instead of the new one. Using `__getitem__` and `.get` we have the expected behavior.

Here is a test showing this behavior:

```python
def test_lru():
    from boltons.cacheutils import LRU

    cache = LRU()

    # Add an entry.
    cache['a'] = 1
    
    # Normal __getitem__ access. 
    assert cache['a'] == 1  # passes.
    # Convert to dict.
    assert dict(cache) == {'a': 1}  # passes.
    # Another way to access the only value.
    assert list(cache.values())[0] == 1  # passes.

    # Replace the existing 'a' entry with a new value.
    cache['a'] = 200

    # __getitem__ works as expected.
    assert cache['a'] == 200  # passes.

    # Both dict and accessing via values() return the old entry: 1.
    assert dict(cache) == {'a': 200}  # fails.
    assert list(dict(cache).values())[0] == 200  # fails.
```

This test fails in the last asserts (of course to see the 2nd failure one needs to comment the first):

```pytest
λ pytest foo.py
======================== test session starts ========================
platform win32 -- Python 3.10.9, pytest-7.4.2, pluggy-1.3.0
rootdir: c:\Users\bruno\projects\boltons
configfile: pytest.ini
collected 1 item

foo.py F                                                       [100%]

============================= FAILURES ==============================
_____________________________ test_lru ______________________________

    def test_lru():
        from boltons.cacheutils import LRU

        cache = LRU()

        # Add an entry.
        cache['a'] = 1

        # Normal __getitem__ access.
        assert cache['a'] == 1  # passes.
        # Convert to dict.
        assert dict(cache) == {'a': 1}  # passes.
        # Another way to access the only value.
        assert list(cache.values())[0] == 1  # passes.

        # Replace the existing 'a' entry with a new value.
        cache['a'] = 200

        # __getitem__ works as expected.
        assert cache['a'] == 200  # passes.

        # Both dict and accessing via values() return the old entry: 1.
>       assert dict(cache) == {'a': 200}  # fails.
E       AssertionError: assert {'a': 1} == {'a': 200}
E         Differing items:
E         {'a': 1} != {'a': 200}
E         Use -v to get more diff

foo.py:23: AssertionError
====================== short test summary info ======================
FAILED foo.py::test_lru - AssertionError: assert {'a': 1} == {'a': 200}
========================= 1 failed in 0.04s =========================
```

At first it seems like a bug? Or perhaps this usage is not meant to be supported?",gemini-2.5-pro,gemini,5443,342,7760,13545,143,101,1.97,2891,FALSE,FALSE,0,0,0,0,12,13,25,N/A,N/A,N/A,N/A,TRUE,413,0,0,0,2,1,3,143,101,1.97,2892,2.631395101547241,71.98203539848328,N/A,4.574140787124634,79.18757128715515
2025-11-15T21:42:23.995067,mahmoud/boltons,4815fc8dd1768da5f2d903846d2ab994aa57b0cf,boltons/cacheutils.py,tests/test_cacheutils.py,9,6,5,9,5,3,16,Test and fix for #348 (#349),LRU .values() and dict return old entries,"Hi,

First of all thanks for the excellent library!

I'm seeing strange results when using `LRU`: after replacing an existing entry with a new value, seems like the `keys()` and `values()` methods return the old value, instead of the new one. Using `__getitem__` and `.get` we have the expected behavior.

Here is a test showing this behavior:

```python
def test_lru():
    from boltons.cacheutils import LRU

    cache = LRU()

    # Add an entry.
    cache['a'] = 1
    
    # Normal __getitem__ access. 
    assert cache['a'] == 1  # passes.
    # Convert to dict.
    assert dict(cache) == {'a': 1}  # passes.
    # Another way to access the only value.
    assert list(cache.values())[0] == 1  # passes.

    # Replace the existing 'a' entry with a new value.
    cache['a'] = 200

    # __getitem__ works as expected.
    assert cache['a'] == 200  # passes.

    # Both dict and accessing via values() return the old entry: 1.
    assert dict(cache) == {'a': 200}  # fails.
    assert list(dict(cache).values())[0] == 200  # fails.
```

This test fails in the last asserts (of course to see the 2nd failure one needs to comment the first):

```pytest
λ pytest foo.py
======================== test session starts ========================
platform win32 -- Python 3.10.9, pytest-7.4.2, pluggy-1.3.0
rootdir: c:\Users\bruno\projects\boltons
configfile: pytest.ini
collected 1 item

foo.py F                                                       [100%]

============================= FAILURES ==============================
_____________________________ test_lru ______________________________

    def test_lru():
        from boltons.cacheutils import LRU

        cache = LRU()

        # Add an entry.
        cache['a'] = 1

        # Normal __getitem__ access.
        assert cache['a'] == 1  # passes.
        # Convert to dict.
        assert dict(cache) == {'a': 1}  # passes.
        # Another way to access the only value.
        assert list(cache.values())[0] == 1  # passes.

        # Replace the existing 'a' entry with a new value.
        cache['a'] = 200

        # __getitem__ works as expected.
        assert cache['a'] == 200  # passes.

        # Both dict and accessing via values() return the old entry: 1.
>       assert dict(cache) == {'a': 200}  # fails.
E       AssertionError: assert {'a': 1} == {'a': 200}
E         Differing items:
E         {'a': 1} != {'a': 200}
E         Use -v to get more diff

foo.py:23: AssertionError
====================== short test summary info ======================
FAILED foo.py::test_lru - AssertionError: assert {'a': 1} == {'a': 200}
========================= 1 failed in 0.04s =========================
```

At first it seems like a bug? Or perhaps this usage is not meant to be supported?",gemini-2.5-pro,gemini,6350,473,11864,18687,143,101,1.97,2891,FALSE,FALSE,0,0,0,0,17,8,25,N/A,N/A,N/A,N/A,TRUE,413,0,0,0,2,1,3,143,101,1.97,2892,2.6596083641052246,94.80088282,N/A,4.590785264968872,102.0512764453888
2025-11-15T22:26:22.555455,mahmoud/boltons,46599bc0d498dd8adc3aea833ce1445feed349dd,boltons/dictutils.py,tests/test_dictutils.py,6,3,5,6,5,5,26,Support `ior` for `dictutils.OMD` (#341),Support in-place union for `dictutils.OrderedMultiDict`,"Not sure if this is a feature request or bug since `OMD` inherits from `dict`.

Python `3.11.3`
Version `23.0.0`

```python
# union with a `dict`
In [2]: a = boltons.dictutils.OrderedMultiDict({'a': 1})

In [3]: a | {'c': 4}
Out[3]: {'a': 1, 'c': 4}

In [4]: a |= {'d': 5}

In [5]: a
Out[5]: OrderedMultiDict([('a', 1)])


# union with another `OMD`
In [10]: a = boltons.dictutils.OrderedMultiDict({'a': 1})

In [11]: a | boltons.dictutils.OrderedMultiDict({'c': 4})
Out[11]: {'a': 1, 'c': 4}

In [12]: a |= boltons.dictutils.OrderedMultiDict({'d': 5})

In [13]: a
Out[13]: OrderedMultiDict([('a', 1)])
```

`dict`'s behaviour
```python
In [6]: a = {'a': 1}

In [7]: a | {'c': 4}
Out[7]: {'a': 1, 'c': 4}

In [8]: a |= {'d': 5}

In [9]: a
Out[9]: {'a': 1, 'd': 5}
```",gemini-2.5-pro,gemini,5070,235,4036,9341,227,183,2.08,4309,TRUE,TRUE,411,0,0,0,5,0,5,228,183,2.08,4324,TRUE,411,0,0,0,4,0,4,228,183,2.08,4324,2.3479955196380615,41.058093786239624,4.584897518157959,2.605785846710205,50.59677267074585
2025-11-15T22:26:27.832511,mahmoud/boltons,c2b04b4cb16410d7bf9ff781c78dee3ca908dc32,boltons/funcutils.py,tests/test_funcutils_fb_py3.py,5,25,5,5,5,3,9,fix: make copy_function work with keyword-only defaults (#336),fix: make copy_function work with keyword-only defaults,"At this point, we have:

```python
from boltons.funcutils import copy_function

f = lambda x, *, y=2: x * y

f_copy = copy_function(f)

assert f(21) == 42  # all goes well, but...

import pytest, re

with pytest.raises(TypeError):
    f_copy(21)  # missing 1 required keyword-only argument: 'y'
    
```

So `f_copy` isn't really a copy.

By adding

```python
...
    if hasattr(orig, ""__kwdefaults__""):
        ret.__kwdefaults__ = orig.__kwdefaults__
...
```

to the function, we get the desired behavior:

```python
f = lambda x, *, y=2: x * y
f_copy = copy_function(f)
assert f(21) == f_copy(21) == 42 
```

I also extended the `test_copy_function` test function to protect this behavior in future evolutions. ",gemini-2.5-pro,gemini,5411,500,7368,13279,162,149,2.47,3118,FALSE,FALSE,0,0,0,0,29,27,56,N/A,N/A,N/A,N/A,TRUE,410,0,0,0,2,0,2,163,150,2.47,3133,2.545583724975586,64.73266267776489,N/A,2.594463825,69.87271022796631
2025-11-15T22:26:36.226251,mahmoud/boltons,4815fc8dd1768da5f2d903846d2ab994aa57b0cf,boltons/cacheutils.py,tests/test_cacheutils.py,9,6,5,9,5,3,16,Test and fix for #348 (#349),LRU .values() and dict return old entries,"Hi,

First of all thanks for the excellent library!

I'm seeing strange results when using `LRU`: after replacing an existing entry with a new value, seems like the `keys()` and `values()` methods return the old value, instead of the new one. Using `__getitem__` and `.get` we have the expected behavior.

Here is a test showing this behavior:

```python
def test_lru():
    from boltons.cacheutils import LRU

    cache = LRU()

    # Add an entry.
    cache['a'] = 1
    
    # Normal __getitem__ access. 
    assert cache['a'] == 1  # passes.
    # Convert to dict.
    assert dict(cache) == {'a': 1}  # passes.
    # Another way to access the only value.
    assert list(cache.values())[0] == 1  # passes.

    # Replace the existing 'a' entry with a new value.
    cache['a'] = 200

    # __getitem__ works as expected.
    assert cache['a'] == 200  # passes.

    # Both dict and accessing via values() return the old entry: 1.
    assert dict(cache) == {'a': 200}  # fails.
    assert list(dict(cache).values())[0] == 200  # fails.
```

This test fails in the last asserts (of course to see the 2nd failure one needs to comment the first):

```pytest
λ pytest foo.py
======================== test session starts ========================
platform win32 -- Python 3.10.9, pytest-7.4.2, pluggy-1.3.0
rootdir: c:\Users\bruno\projects\boltons
configfile: pytest.ini
collected 1 item

foo.py F                                                       [100%]

============================= FAILURES ==============================
_____________________________ test_lru ______________________________

    def test_lru():
        from boltons.cacheutils import LRU

        cache = LRU()

        # Add an entry.
        cache['a'] = 1

        # Normal __getitem__ access.
        assert cache['a'] == 1  # passes.
        # Convert to dict.
        assert dict(cache) == {'a': 1}  # passes.
        # Another way to access the only value.
        assert list(cache.values())[0] == 1  # passes.

        # Replace the existing 'a' entry with a new value.
        cache['a'] = 200

        # __getitem__ works as expected.
        assert cache['a'] == 200  # passes.

        # Both dict and accessing via values() return the old entry: 1.
>       assert dict(cache) == {'a': 200}  # fails.
E       AssertionError: assert {'a': 1} == {'a': 200}
E         Differing items:
E         {'a': 1} != {'a': 200}
E         Use -v to get more diff

foo.py:23: AssertionError
====================== short test summary info ======================
FAILED foo.py::test_lru - AssertionError: assert {'a': 1} == {'a': 200}
========================= 1 failed in 0.04s =========================
```

At first it seems like a bug? Or perhaps this usage is not meant to be supported?",gemini-2.5-pro,gemini,6428,316,15137,21881,143,101,1.97,2891,TRUE,TRUE,408,5,0,0,9,0,9,145,103,1.99,2941,TRUE,413,0,0,0,2,1,3,143,101,1.97,2892,2.532303810119629,133.2868492603302,3.136240005493164,2.5818097591400146,141.5372028
2025-11-15T22:26:51.931921,python-attrs/attrs,6e3786c599279f24e91b0d57ef448f7a85d7a69f,src/attr/_make.py; src/attr/_next_gen.py,tests/test_make.py,10,68,5,10,10,5,141,Make `kw_only=True` behavior consistent with dataclasses (#1457),Suggestion: Overriding class-level `kw_only=True` option in attributes,"First of all, thank you for building such a wonderful package! It is now an essential part of both my work and my hobby projects.

This issue is similar to #481, but I think it's more suitable to open a new issue for discussion instead of reviving a 4-year-old thread.

`kw_only` can be set both on the class decorator and on attributes. When `kw_only=False` on the class (this is the default), one can set `kw_only=True` on attributes to make only those attributes keyword-only in the constructor. However, when `kw_only=True` on the class, all attributes (including those defined in base classes) become keyword-only, and setting `kw_only=False` on attributes will not work.

The following code snippet implements the logic described above:
https://github.com/python-attrs/attrs/blob/7091b1f89ac2436e8ba96f85a799b66c38a3c9f4/src/attr/_make.py#L564-L566

In my opinion, this is a little confusing, since many of the other options that exist on both the class and attributes do give you a way to opt-out for a subset of attributes. For example, `eq` (and friends), `repr`, and `init` all allow you to exclude an attribute from the auto-generated functions. It feels natural to assume the same for `kw_only`; I don't have exact stats, but I know quite a few coworkers who were surprised by the current behavior.

It would also be useful to allow overriding. For example, I have a class whose ideal constructor signature should look like this:
```python
SomeObject(x, y, *, kw1=..., kw2=..., kw3=..., ...)  # imagine there's a lot more kw-only args
```
where `x` and `y` are required, and all the rest are optional with defaults, usually some obscure setting that can be tweaked but are rarely tweaked. To implement such an interface, I would have to set `kw_only=False` on the class, and `kw_only=True` on most of the attributes. To make matters worse, I need to do the same for all new attributes in all of its subclasses as well. It would be beneficial if we could just set `kw_only=True` on the class, and override a few attributes with `kw_only=False`.

---

If we are to allow overriding `kw_only` on attributes, this is how I imagine it should behave:

- The class's `kw_only` defaults to False, as it does today.
- The attribute's `kw_only` defaults to None. If it is set to True or False, then it stays kw-only or non-kw-only regardless of the class's setting; if it is not set (None), then it reflects the class's `kw_only` value.
- Note that this also means that a subclass cannot control the `kw_only`-ness of its base attributes.

I'm open for debate on the semantics and I'd love input on how this can be improved or made more natural or useful.

Now, I would love to put up a pull request for this change, but I realize that this would break backwards-compability, so I'd like to seek permission from maintainers before I go ahead and do this. I'm also open to introducing a new class-level option (e.g. `kw_only_overridable`, or something less horrendous) that's disabled by default, similar to how `collect_by_mro` works. Please let me know if you think this proposal makes sense, and how we should move forward. Thanks!",gemini-2.5-pro,gemini,7740,1239,14201,23180,499,474,3.22,9733,FALSE,FALSE,0,0,0,0,80,106,186,N/A,N/A,N/A,N/A,TRUE,1354,0,7,0,58,11,69,501,476,3.27,9770,1.531670093536377,128.86852645874023,N/A,10.223142385482788,140.6233389377594
2025-11-15T22:27:01.725013,python-attrs/attrs,112dd1d49848da7a956c78415e0f571e0dc70d90,src/attr/_make.py; src/attr/_next_gen.py; src/attrs/__init__.py; typing-examples/baseline_examples.py,tests/test_make.py; tests/test_next_gen.py; tests/test_slots.py,25,70,5,25,15,5,214,Expose effective class construction properties (#1454),Accessing @attr.s() decorator parameter values,"In #python today someone wanted to find out if their class was frozen. `Cls.__dataclass_params__` keeps the `@dataclasses.dataclass()` arguments.

```py
_DataclassParams(init=True,repr=True,eq=True,order=False,unsafe_hash=False,frozen=True)
```

Is there a way to get that data in attrs?",gemini-2.5-pro,gemini,9283,4196,17665,31144,500,474,3.24,9779,FALSE,FALSE,0,0,0,0,303,231,534,N/A,N/A,N/A,N/A,TRUE,1362,0,8,0,347,93,440,507,482,3.2,10158,1.5218615531921387,159.67958188056946,N/A,7.9866533279418945,169.1880967617035
2025-11-15T22:27:27.574720,Textualize/rich,30e5ed61a6064220fe2ff40f4713463488ee6d07,rich/panel.py,tests/test_panel.py,1,1,5,1,5,5,6,fix(panel): fix title missing panel background,"[BUG]In the latest versions rich ""Title"" without color","rich v 13.0.1  Title is painted over, see screenshot
rich v13.9.4r Title unpainted, see screenshot

OS GNU/Linux and OS Windows

```python
from rich.console import Console
from rich.panel import Panel
from rich.style import Style
import os

console = Console()
console.print(Panel(str(os.environ), title='[bold white]Title[/bold white]', style=Style(color=""white"", bgcolor=""blue"")))
```
![rich](https://github.com/user-attachments/assets/9eb0fd2e-13f9-4e2a-9b09-710c7d48f7ff)
Title should be like on the first screen (white on blue), in the second case (updated rich) white on dark (bug?)
",gemini-2.5-pro,gemini,3516,1781,10091,15388,32,31,6,1294,FALSE,FALSE,0,0,0,0,147,136,283,N/A,N/A,N/A,N/A,TRUE,850,3,25,0,3,4,7,32,31,6,1291,9.259695529937744,95.84897017478943,N/A,8.112394094467163,113.22105979919434
2025-11-15T22:28:19.003778,Textualize/rich,f591471cf005062c9068f5ac348c01aec4ca13ef,rich/highlighter.py,tests/test_highlighter.py,6,1,5,6,5,1,7,Fix [BUG] @ breaks highlighting in hyperlink #3327,[BUG] `@` breaks highlighting in hyperlinks,"- [x] I've checked [docs](https://rich.readthedocs.io/en/latest/introduction.html) and [closed issues](https://github.com/Textualize/rich/issues?q=is%3Aissue+is%3Aclosed) for possible solutions.
- [x] I can't find my issue in the [FAQ](https://github.com/Textualize/rich/blob/master/FAQ.md).

**Describe the bug**

Hyperlinks containing an `@` symbol are not properly highlighted.
It is a valid character used by multiple large websites (youtube and twitter should be enough of an example)

Screenshot with minimal code:
![image](https://github.com/Textualize/rich/assets/29684689/5bd3411b-80be-4aff-b4b2-a6e6bb2a1b80)

Expected: The whole link to be highlighted instead of just before the `@`

**Platform**
<details>
<summary>Click to expand</summary>

Windows 10, Windows Terminal v1.19, Python 3.8.6

`python -m rich.diagnose`
![image](https://github.com/Textualize/rich/assets/29684689/4f7dc02d-ecc0-45a4-85e9-fc34c2fc16a0)
![image](https://github.com/Textualize/rich/assets/29684689/ed576f30-9b16-41d5-b24e-7201bc2b1053)

`pip freeze | grep rich`
rich==13.7.1

</details>
",FAILED,FAILED,0,0,0,0,13,9,1.83,240,FALSE,FALSE,0,0,0,0,0,0,0,PATCH_GENERATION_FAILED,PATCH_GENERATION_FAILED,PATCH_GENERATION_FAILED,PATCH_GENERATION_FAILED,TRUE,829,0,24,0,1,1,2,13,9,1.83,240,44.30294179916382,0,0,7.0497918128967285,51.35273361206055
2025-11-15T22:28:26.488993,Textualize/rich,59b1aca63bf9ec69ada93af960d8b2a7bd920477,rich/_wrap.py; rich/cells.py,tests/test_cells.py; tests/test_text.py,0,8,5,0,10,5,84,Fix double-width characters disappearing when wrapping (#3180),[BUG] Chunks of text go missing when writing Asian text (wrapping issue),"### Description

(Originally reported in Textual: https://github.com/Textualize/textual/issues/3567)

When you print Asian text (specifically Chinese and Japanese, which do not use spaces), portions of the text go missing, making it unreadable.

This seems to be related to wrapping, as the characters which go missing are at the end of a line.
Instead of being wrapped on to a new line, they disappear.

There are [rules for wrapping](https://en.m.wikipedia.org/wiki/Line_breaking_rules_in_East_Asian_languages) in these languages which would take more effort to adhere to, but at the very least, text should not go missing when printed.

### Examples

For example, running the snippet below, the `7` in `1670` disappears:

```python
from rich.console import Console
console = Console(width=20)
console.print(""アプリケーションは1670万色を使用でき"")
```

Output:

```
アプリケーションは16
0万色を使用でき
```

And in many cases, like those reported in this issue, multiple characters disappear:

```python
from rich.console import Console
console = Console(width=20)
console.print(""TextualはPythonの高速アプリケーション開発フレームワークです"")
```

Output:

```
TextualはPy
thonの高速アプリ
ケーション開発フレー
```

Notice that many of the characters at the end of the text are completely missing (`ムワークです`).
",gemini-2.5-pro,gemini,4144,454,22788,27386,41,67,1.88,721,FALSE,FALSE,0,0,0,0,18,21,39,N/A,N/A,N/A,N/A,FALSE,0,0,0,1,89,38,127,40,67,1.75,711,5.728027820587158,209.59104752540588,N/A,1.6117024421691895,216.93077778816223
2025-11-15T22:29:02.931639,arrow-py/arrow,05cd9b47d0db1c22b44cb739832f1b9701855313,arrow/arrow.py,tests/test_arrow.py,1,1,5,1,5,5,220,fix humanize month limits (#1224),"HH token does not support the number 24, even though the documentation says it should (00 to 24)","According to ISO 8601 `arrow.get('2019-10-29T24:00:00')` should parse to October 30, 2019 (`2019-10-30T00:00:00`), but instead is throwing an exception `ValueError: hour must be in 0..23`

It also fails when doing explicit formatting (same exception)
`arrow.get('2013-12-31T24:00:00', ['YYYY-MM-DD[T]HH:mm:ss'])`

If you look at the docs at https://arrow.readthedocs.io/en/latest/#supported-tokens it says `HH` token is from 00 to 24

if this is accepted as a bug,  it seems a fix for this can be done at  `arrow/parser.py` inside the `_build_datetime` function ( I can do it, including unit tests)",gemini-2.5-pro,gemini,4642,622,9462,14726,220,183,2.2,5174,FALSE,FALSE,0,0,0,0,33,133,166,N/A,N/A,N/A,N/A,TRUE,1859,0,0,0,34,14,48,223,197,2.2,5202,3.5912652015686035,80.1808238,N/A,30.945143222808838,114.71723222732544
2025-11-15T22:29:34.056214,arrow-py/arrow,97695bfce6977fb117459600933768827ef23aaf,arrow/arrow.py; arrow/factory.py; arrow/formatter.py; arrow/parser.py; tests/utils.py,tests/conftest.py; tests/test_arrow.py; tests/test_factory.py; tests/test_formatter.py; tests/test_parser.py,8,3,5,8,15,5,419,Migrate Arrow to use ZoneInfo for timezones (#1217),"HH token does not support the number 24, even though the documentation says it should (00 to 24)","According to ISO 8601 `arrow.get('2019-10-29T24:00:00')` should parse to October 30, 2019 (`2019-10-30T00:00:00`), but instead is throwing an exception `ValueError: hour must be in 0..23`

It also fails when doing explicit formatting (same exception)
`arrow.get('2013-12-31T24:00:00', ['YYYY-MM-DD[T]HH:mm:ss'])`

If you look at the docs at https://arrow.readthedocs.io/en/latest/#supported-tokens it says `HH` token is from 00 to 24

if this is accepted as a bug,  it seems a fix for this can be done at  `arrow/parser.py` inside the `_build_datetime` function ( I can do it, including unit tests)",gemini-2.5-pro,gemini,6948,865,9075,16888,405,342,2.45,9335,FALSE,FALSE,0,0,0,0,46,133,179,N/A,N/A,N/A,N/A,TRUE,1858,0,0,0,51,29,80,410,345,2.45,9426,3.040025472640991,84.77373194694519,N/A,27.824899196624756,115.63865661621094
2025-11-15T22:30:08.741742,arrow-py/arrow,016fb9eb9b3835bcd4efc69dc048a0436bd7bfec,arrow/locales.py,tests/test_locales.py,84,2,5,84,5,5,225,fix: adding persian names of months and month-abbreviations and day-abbreviations in Gregorian calendar (#1172),"HH token does not support the number 24, even though the documentation says it should (00 to 24)","According to ISO 8601 `arrow.get('2019-10-29T24:00:00')` should parse to October 30, 2019 (`2019-10-30T00:00:00`), but instead is throwing an exception `ValueError: hour must be in 0..23`

It also fails when doing explicit formatting (same exception)
`arrow.get('2013-12-31T24:00:00', ['YYYY-MM-DD[T]HH:mm:ss'])`

If you look at the docs at https://arrow.readthedocs.io/en/latest/#supported-tokens it says `HH` token is from 00 to 24

if this is accepted as a bug,  it seems a fix for this can be done at  `arrow/parser.py` inside the `_build_datetime` function ( I can do it, including unit tests)",FAILED,FAILED,0,0,0,0,170,121,2.56,3124,FALSE,FALSE,0,0,0,0,0,0,0,PATCH_GENERATION_FAILED,PATCH_GENERATION_FAILED,PATCH_GENERATION_FAILED,PATCH_GENERATION_FAILED,TRUE,1844,0,0,0,34,25,59,170,121,2.56,3124,2.545870542526245,0,0,31.883400917053223,34.42927145957947
2025-11-15T22:31:17.147761,Delgan/loguru,ffe1617122cc6c56f6fdfb1715a006fbf85bea1c,loguru/_simple_sinks.py,tests/test_standard_handler.py,4,1,5,4,5,5,15,"Fix the "".level"" of standard ""Handler"" previously ignored (#1410)",The configured level of standard logging Handler is ignored,"Loguru supports standard [`logging.Handler`](https://docs.python.org/3/library/logging.html#handler-objects) used as sink. Loguru honors the possible [`Filter`](https://docs.python.org/3/library/logging.html#filter-objects) object attached to such handler, but it bypasses the level configured by [`setLevel()`](https://docs.python.org/3/library/logging.html#logging.Handler.setLevel). That doesn't seem consistent. 

Reproducible example:

```python
import logging

from loguru import logger


class RejectAllFilter(logging.Filter):
    def filter(self, record):
        return False


handler = logging.StreamHandler()
filter_ = RejectAllFilter()


logger.remove()
logger.add(handler, format=""{message}"", level=0)

logger.info(""Logged"")

handler.addFilter(filter_)
logger.info(""Not logged (as expected)"")

handler.removeFilter(filter_)
logger.info(""Logged again"")

handler.setLevel(logging.CRITICAL)
logger.info(""Logged while it should not"")
```

Since this behavior is not unit tested and as I don't remember any good reason for ignoring the handler's level, I assume it was very likely an oversight (especially because from a technical standpoint, the filter is checked [by the handler itself](https://github.com/python/cpython/blob/d78d7a50b06c4ea10d13fc2dcb42607a97f9260c/Lib/logging/__init__.py#L1019-L1022), while the level is checked [by the logger](https://github.com/python/cpython/blob/d78d7a50b06c4ea10d13fc2dcb42607a97f9260c/Lib/logging/__init__.py#L1733-L1734)).

Note that, by default, the `level` is `0` (aka `NOTSET`).",gemini-2.5-pro,gemini,3839,612,6603,11054,34,17,1.6,590,FALSE,FALSE,0,0,0,0,35,33,68,N/A,N/A,N/A,N/A,TRUE,1561,0,0,0,2,0,2,35,18,1.6,607,2.0665791034698486,53.73669767379761,N/A,63.453089237213135,119.25636601448059
2025-11-15T22:32:19.832368,Delgan/loguru,bc2ec9abe9285ba016b904b6dd82ccb8ce0fb2c1,loguru/_colorama.py,tests/test_colorama.py,0,3,3,0,5,5,22,Support FORCE_COLOR (#1306),Support for FORCE_COLOR,"https://force-color.org/

Similar to #1178 / #1299 but forcing color to be enabled rather than disabled, which is useful for non TTY environments that do support color.

Prior art:
- [pytest](https://github.com/pytest-dev/pytest/blob/b0caf3d7adc45f773177424593431869fd2f82d8/src/_pytest/_io/terminalwriter.py#L43)
- [rich](https://github.com/Textualize/rich/blob/43d3b04725ab9731727fb1126e35980c62f32377/rich/console.py#L952)
- [uv](https://github.com/astral-sh/uv/blob/6d3614eece09b35d250bd0c1fe35bcc56fffebcc/crates/uv/src/settings.rs#L99)",gemini-2.5-pro,gemini,2843,626,8907,12376,27,31,1,299,FALSE,FALSE,0,0,0,0,41,38,79,N/A,N/A,N/A,N/A,TRUE,1553,0,28,0,8,1,9,28,32,1,310,2.5308728218078613,70.44844413,N/A,60.067119121551514,133.04643607139587
2025-11-15T22:33:17.565342,Delgan/loguru,7748b6cde69880101a5cfb5dcc653ad7cf23402d,loguru/_logger.py; tests/exceptions/source/modern/decorate_async_generator.py; tests/exceptions/source/modern/exception_formatting_async_generator.py,tests/test_exceptions_catch.py; tests/test_exceptions_formatting.py,4,4,5,4,5,5,49,"Add support for async generators to ""@logger.catch"" (#1303)","Make ""logger.catch()"" compatible with asynchronous generators","Eg in:

```python
import asyncio
from loguru import logger


@logger.catch
async def foo():
    yield 1
    yield 2
    raise ValueError


async def main():
    async for i in foo():
        print(i)

    print(""Done"")


asyncio.run(main())
```",gemini-2.5-pro,gemini,4393,512,7695,12600,203,202,3.36,4465,FALSE,FALSE,0,0,0,0,37,32,69,N/A,N/A,N/A,N/A,TRUE,1535,0,28,0,163,0,163,233,215,2.62,4990,1.5263285636901855,65.21005535125732,N/A,56.02442765235901,122.76081156730652
2025-11-15T22:33:27.360345,pallets/click,18d29196c508b249ef1111f1f29c9133e0a8b097,src/click/core.py,tests/test_arguments.py; tests/test_options.py,11,14,5,11,5,4,108,Ensure that 'type_cast_value' never gets 'UNSET',`Sentinel` is passed to `type_cast_value`,"Starting in version 8.3.0, a `Sentinel` object is passed in to the `type_cast_value` function of the `Parameter` class and its derivatives if no value is supplied.

Reproduction:

```python
# cli_bug.py
import click


class CSVOption(click.Option):
    def type_cast_value(self, ctx, value):
        if value:
            return value.split("","")
        return value


@click.command(name=""run"")
@click.option(""--csv"", cls=CSVOption)
def main(csv):
    click.echo(f""{csv!r}"")

if __name__ == ""__main__"":
    main()
```

```shell
$ uv run --with 'click==8.3.0' cli_bug.py

Installed 1 package in 1ms
Traceback (most recent call last):
  File ""/home/coder/workspace/scratch/click-8_3_0-bug/cli_bug.py"", line 17, in <module>
    main()
  File ""/home/coder/.cache/uv/archive-v0/9Q5N3REh-YZUOJbO3iHRv/lib/python3.10/site-packages/click/core.py"", line 1462, in __call__
    return self.main(*args, **kwargs)
  File ""/home/coder/.cache/uv/archive-v0/9Q5N3REh-YZUOJbO3iHRv/lib/python3.10/site-packages/click/core.py"", line 1382, in main
    with self.make_context(prog_name, args, **extra) as ctx:
  File ""/home/coder/.cache/uv/archive-v0/9Q5N3REh-YZUOJbO3iHRv/lib/python3.10/site-packages/click/core.py"", line 1206, in make_context
    self.parse_args(ctx, args)
  File ""/home/coder/.cache/uv/archive-v0/9Q5N3REh-YZUOJbO3iHRv/lib/python3.10/site-packages/click/core.py"", line 1217, in parse_args
    _, args = param.handle_parse_result(ctx, opts, args)
  File ""/home/coder/.cache/uv/archive-v0/9Q5N3REh-YZUOJbO3iHRv/lib/python3.10/site-packages/click/core.py"", line 2516, in handle_parse_result
    value = self.process_value(ctx, value)
  File ""/home/coder/.cache/uv/archive-v0/9Q5N3REh-YZUOJbO3iHRv/lib/python3.10/site-packages/click/core.py"", line 2401, in process_value
    value = self.type_cast_value(ctx, value)
  File ""/home/coder/workspace/scratch/click-8_3_0-bug/cli_bug.py"", line 7, in type_cast_value
    return value.split("","")
AttributeError: 'Sentinel' object has no attribute 'split'
```

```shell
uv run --with 'click<8.3.0' cli_bug.py
None
```

Based on comments in #3065 and #3068 it would make sense to pass through `None` here given that this function is public. At a minimum, a changelog denoting the new behavior would be nice.

Environment:

- Python version: 3.10
- Click version: 8.3.0
",gemini-2.5-pro,gemini,7181,380,11423,18984,563,542,2.76,11014,FALSE,FALSE,0,0,0,0,16,26,42,N/A,N/A,N/A,N/A,TRUE,1313,0,21,0,27,9,36,567,543,2.76,11053,2.0575196743011475,98.51975846290588,N/A,4.058968782424927,104.63624691963196
2025-11-15T22:33:32.311316,pallets/click,b64ea07128a6368b5f6f93035c75d5693c7ba572,src/click/_utils.py; src/click/core.py; src/click/parser.py; src/click/termui.py,tests/test_arguments.py; tests/test_basic.py; tests/test_options.py; tests/test_termui.py; tests/test_utils.py,15,36,5,15,15,5,196,Differentiate absence of value and value of absence for `default` and `flag_value`,Binary flags vs `default=None`: semantics and issue with prompt,"The current behavior is:
- if `default` is not provided, the option default is set to `False`
- if `default` is set to `None`, default is set to `None`.

I think this may be accidental behavior but it's not senseless: settings `default=None` you can force the prompt to require an explicit value to be provided (it'll be `[y/n]` rather than `[y/N]`). 

Nonetheless, I noticed that if you provide `default=None` to a flag not having a secondary option, the prompt message is broken: it won't include ""[y/n]"" at the end.

_Originally posted by @janluke in https://github.com/pallets/click/issues/1965#issuecomment-873600435_",gemini-2.5-pro,gemini,9074,758,10403,20235,738,711,2.95,14858,FALSE,FALSE,0,0,0,0,59,59,118,N/A,N/A,N/A,N/A,TRUE,1196,0,21,0,355,232,587,733,694,2.93,14838,1.020601511001587,89.56835651397705,N/A,3.574268102645874,94.16322612762451
2025-11-15T22:33:36.412072,pallets/click,1c20dc6e724cd5625faaa17b715ba928d44c08bf,src/click/core.py,tests/test_defaults.py,11,14,5,11,5,5,5,Fix default handling to defer UNSET normalization,`default=True` on feature flags is order-sensitive,"The following code:

```python
import click


@click.command()
@click.option(""--red"", ""color"", flag_value=""red"")
@click.option(""--green"", ""color"", flag_value=""green"", default=True)
@click.option(""--blue"", ""color"", flag_value=""blue"")
def main(color: str) -> None:
    print(repr(color))


if __name__ == ""__main__"":
    main()
```

when run with no arguments using click 8.2.1 outputs `'green'`, which is what I expect.  If click 8.3.0 is used instead, then it outputs `None` — but if the `--green` option is moved above the `--red` option in the script, click 8.3.0 will output `'green'`.

I believe that the script should output `'green'` regardless of the declaration order of the options.

Environment:

- Python version: 3.13.7
- Click version: 8.2.1 and 8.3.0",gemini-2.5-pro,gemini,6158,501,13787,20446,561,540,2.76,10982,FALSE,FALSE,0,0,0,0,24,26,50,N/A,N/A,N/A,N/A,TRUE,1302,0,21,0,11,5,16,563,542,2.76,11014,1.0223493576049805,116.68489789962769,N/A,2.733034372329712,120.44028162956238
2025-11-15T22:34:28.218147,fastapi/fastapi,5d40dfbc9bc1df1c7801acc53857ec7a072b7697,fastapi/_compat/v2.py,tests/test_schema_ref_pydantic_v2.py,3,25,5,3,3,2,0,"🐛 Fix handling of JSON Schema attributes named ""$ref"" (#14349)","🐛 Fix handling of JSON Schema attributes named ""$ref""","Fix handling of JSON Schema attributes named ""$ref""

Related to: https://github.com/fastapi/fastapi/issues/14344",gemini-2.5-pro,gemini,3389,495,5718,9602,67,58,1.88,1664,FALSE,FALSE,0,0,0,0,24,34,58,N/A,N/A,N/A,N/A,TRUE,2453,0,105,0,6,6,12,68,62,1.88,1667,5.100675344467163,48.68508458137512,N/A,38.790061712265015,92.57582164
2025-11-15T22:35:00.227451,fastapi/fastapi,282f372eda661c865fa1208bd0371bf47d190b2a,fastapi/dependencies/utils.py,tests/test_dependency_yield_scope.py,2,23,5,2,5,5,7,"🐛 Fix `Depends(func, scope='function')` for top level (parameterless) dependencies (#14301)","🐛 Fix `Depends(func, scope='function')` for top level (parameterless) dependencies","**Summary**

This PR enables proper support for `scope=""function""` on `APIRoute` dependencies by passing the dependency’s scope through to `get_dependant`. Previously, parameter-less dependencies created via `get_parameterless_sub_dependant` ignored `depends.scope`, which led to unexpected behavior for function-scoped dependencies.

**Motivation**

Users expect `scope=""function""` to work as defined, even when registered at the route level. Because `depends.scope` wasn’t forwarded, function scope behaved like the default(request). This change aligns behavior with user expectations and documentation around dependency scopes.
(Related: #14296)",gemini-2.5-pro,gemini,4390,347,3286,8023,204,202,2.65,4678,FALSE,FALSE,0,0,0,0,13,10,23,N/A,N/A,N/A,N/A,TRUE,2451,0,105,0,4,1,5,204,202,2.65,4685,2.5460610389709473,29.26995062828064,N/A,29.253597736358643,61.06960940361023
2025-11-15T22:35:33.647387,fastapi/fastapi,496de1816aa01a59c75a7dcd27fd3c6245fa255a,fastapi/dependencies/utils.py,tests/test_top_level_security_scheme_in_openapi.py,2,26,5,2,5,1,0,🐛 Fix security schemes in OpenAPI when added at the top level app (#14266),🐛 Fix security schemes in OpenAPI when added at the top level app,"🐛 Fix security schemes in OpenAPI when added at the top level app

Ref: https://github.com/fastapi/fastapi/discussions/14263
Ref: https://github.com/fastapi/fastapi/issues/14271",FAILED,FAILED,0,0,0,0,205,202,2.42,4735,FALSE,FALSE,0,0,0,0,0,0,0,INTENT_PARSE_FAILED,INTENT_PARSE_FAILED,INTENT_PARSE_FAILED,INTENT_PARSE_FAILED,TRUE,2430,0,105,0,8,10,18,205,198,2.42,4723,4.052780866622925,0,0,29.251138925552368,33.30391979217529
2025-11-15T22:35:56.409061,encode/httpx,49d74a2e7f773f88a84dd4af900d7a8543636ae2,httpx/_utils.py,tests/client/test_headers.py,2,22,5,2,5,5,11,Clarified error when header value is None (#3312),Clarified error when header value is None,"Previously, when the header value was None, a misleading error was thrown: ""AttributeError: Object 'NoneType' does not have attribute 'encode'."" Now it looks better.

![image](https://github.com/user-attachments/assets/ec12330f-aa7e-4084-8c81-c89aef8fdea9)
",gemini-2.5-pro,gemini,4114,303,4873,9290,99,67,1.33,1657,FALSE,FALSE,0,0,0,0,10,7,17,N/A,N/A,N/A,N/A,TRUE,1430,0,1,0,2,0,2,100,69,1.33,1672,2.7177841663360596,37.28115653991699,N/A,15.587911128997803,55.586851835250854
2025-11-15T22:36:17.791694,encode/httpx,7c0cda153d301bde9a011e1dd7157d7e2b20889d,httpx/_urlparse.py,tests/models/test_url.py,1,9,5,1,5,3,70,Improve InvalidURL error message. (#3250),Improve InvalidURL error message.,"Prompted by https://github.com/encode/httpx/discussions/3248.

*before*...

```python
>>> import httpx
>>> httpx.get(""https://www.example.com\n"")
Traceback (most recent call last):
  ...
  File ""/Users/tomchristie/GitHub/encode/httpx/httpx/_urlparse.py"", line 163, in urlparse
    raise InvalidURL(error)
httpx.InvalidURL: Invalid non-printable ASCII character in URL.
```

*after*...

```python
>>> import httpx
>>> httpx.get(""https://www.example.com\n"")
Traceback (most recent call last):
  ...
  File ""/Users/tomchristie/GitHub/encode/httpx/httpx/_urlparse.py"", line 168, in urlparse
    raise InvalidURL(error)
httpx.InvalidURL: Invalid non-printable ASCII character in URL, '\n' at position 23.
```
",gemini-2.5-pro,gemini,4638,1384,9922,15944,96,93,1.54,1530,FALSE,FALSE,0,0,0,0,96,138,234,N/A,N/A,N/A,N/A,TRUE,1429,0,1,0,14,3,17,102,93,1.54,1605,8.039428711,91.56331753730774,N/A,13.242570161819458,112.8453164100647
2025-11-15T22:36:39.354112,encode/httpx,a7092af2fda78d92daaad7627e4cf0cf5e94b019,httpx/_urlparse.py,tests/models/test_url.py,1,10,5,1,5,1,65,Resolve queryparam quoting (#3187),Resolve queryparam quoting,"* Percent-encoding in `params={}` should match browser form behavior. (Always percent-encode.)
* Percent-encoding in `url={}` should match browser URL bar behavior. (Percent-encode, preserving existing escape sequences.)

Closes #3135.
Closes #3140.",gemini-2.5-pro,gemini,4095,549,18799,23443,94,90,1.57,1535,FALSE,FALSE,0,0,0,0,23,21,44,N/A,N/A,N/A,N/A,TRUE,866,0,1,0,7,29,36,91,88,1.54,1491,7.563492059707642,152.3415813446045,N/A,13.89539361000061,173.80046701431274
2025-11-15T22:36:49.689235,psf/requests,3ff3ff21dd45957c9e143cd500291959bb15f690,src/requests/exceptions.py,tests/test_requests.py,25,0,5,25,1,1,223,Fix #6628 - JSONDecodeError are not deserializable,[BUG] JSONDecodeError can't be deserialized - invalid JSON raises a BrokenProcessPool and crashes the entire process pool,"Hi all,

I've stumbled upon a bug in the `requests` library, and have a proposal for a fix.

In short: I have a process pool running tasks in parallel, that are among other things doing queries to third-party APIs. One third-party returns an invalid JSON document as response in case of error.

However, instead of just having a JSONDecodeError as the result of my job, the entire process pool crashes due to a BrokenProcessPool error with the following stack trace:

```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.11/concurrent/futures/process.py"", line 424, in wait_result_broken_or_wakeup
    result_item = result_reader.recv()
                  ^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/multiprocessing/connection.py"", line 251, in recv
    return _ForkingPickler.loads(buf.getbuffer())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/requests/exceptions.py"", line 41, in __init__
    CompatJSONDecodeError.__init__(self, *args)
TypeError: JSONDecodeError.__init__() missing 2 required positional arguments: 'doc' and 'pos'
```

So I'm in a situation where receiving one invalid JSON as response is interrupting all my ongoing tasks because the entire process pool is crashing, and no more tasks can be submitted until the process pool recovered.

## Origin of the bug + Fix

After investigation, this is because the `requests.exception.JSONDecodeError` instances can't be deserialized once they've been serialized via `pickle`. So when the main process is trying to deserialize the error returned by the child process, the main process is crashing with to the error above.

I think this bug has been around for a while, I've found old tickets from different projects mentioning issues that are looking similar: https://github.com/celery/celery/issues/5712

I've pinpointed the bug to the following class: https://github.com/psf/requests/blob/main/src/requests/exceptions.py#L31

Basically, due the MRO/order of inheritance, the `__reduce__` method used will not be the one of `CompatJSONDecodeError`. Most of the args will therefore be ditched when pickling the instance and it can't be deserialised back because `CompatJSONDecodeError.__init__`  does expect those args. MRO below:

```
In [1]: from requests.exceptions import JSONDecodeError

In [2]: JSONDecodeError.__mro__
Out[2]:
(requests.exceptions.JSONDecodeError,
 requests.exceptions.InvalidJSONError,
 requests.exceptions.RequestException,
 OSError,
 simplejson.errors.JSONDecodeError,
 ValueError,
 Exception,
 BaseException,
 object)
```

I think the fix could be quite simple and should have very little side-effects: to specify a `JSONDecodeError.__reduce__` method that will call the one from the correct parent class (it will be regardless that it is json/simplejson via the Compat class, their respective methods having different signatures).

I've taken the initiative to write a fix + a test and will raise a pull request to that effect 🙏 

-----

## Expected Result

I've written a test for this case: the error can easily be reproduced by simply trying to `pickle.dumps()`  then `pickle.loads()` on a error:

```python
json_decode_error = requests.exceptions.JSONDecodeError(
    ""Extra data"",
    '{""responseCode"":[""706""],""data"":null}{""responseCode"":[""706""],""data"":null}',
    36,
)
deserialized_error = pickle.loads(pickle.dumps(json_decode_error))
assert repr(json_decode_error) == repr(deserialized_error)
```

This assertion should be true

## Actual Result

Currently, instead of passing it'll raise the following error:

```
>       CompatJSONDecodeError.__init__(self, *args)
E       TypeError: JSONDecodeError.__init__() missing 2 required positional arguments: 'doc' and 'pos'
```

## Reproduction Steps

As mentioned above, this bug is more impactful in a multi-process architecture as it'll break the entire process pool.
For something looking a bit more like a live-case, I've produced a little snippet with a really simple API returning an invalid JSON: 

```python
# File api.py

from fastapi import FastAPI
from starlette.responses import PlainTextResponse

app = FastAPI()


@app.get(""/"")
async def root():
    # An invalid json string returned by the endpoint that will trigger a JSONDecodeError when calling `res.json()`
    s = '{""responseCode"":[""706""],""data"":null}{""responseCode"":[""706""],""data"":null}'
    return PlainTextResponse(s, media_type=""application/json"", status_code=400)


# Run the API:
# $ uvicorn api:app --reload
#
# curl http://127.0.0.1:8000 will return the invalid json
```

and the following 

```python
from concurrent.futures import ProcessPoolExecutor
from concurrent.futures.process import BrokenProcessPool

import requests


def my_task():
    response = requests.get('http://127.0.0.1:8000/')
    response.json()

def my_main_func():
    with ProcessPoolExecutor(max_workers=4) as executor:
        future = executor.submit(my_task)
        for i in range(0, 5):
            try:
                future.result(timeout=100)
                print(f""Attempt {i} ok"")
            except BrokenProcessPool:
                print(f""Attempt {i} - the pool is broken"")
            except requests.JSONDecodeError:
                print(f""Attempt {i} raises a request JSONDecodeError"")


if __name__ == '__main__':
    my_main_func()
```

Instead of getting the following output:
```
Attempt 0 raises a request JSONDecodeError
Attempt 1 raises a request JSONDecodeError
Attempt 2 raises a request JSONDecodeError
Attempt 3 raises a request JSONDecodeError
Attempt 4 raises a request JSONDecodeError
```

One would currently have:
```
Attempt 0 - the pool is broken
Attempt 1 - the pool is broken
Attempt 2 - the pool is broken
Attempt 3 - the pool is broken
Attempt 4 - the pool is broken
```

An invalid JSON is crashing the entire process pool and no job can be submitted anymore.

## System Information

Tested with:
- request==2.31.0
- Python==3.11.7

-----

Thanks a lot for taking the time to read this long bug report!!",gemini-2.5-pro,gemini,5477,226,3450,9153,5,2,3,113,TRUE,FALSE,0,0,0,0,4,0,4,6,2,2.33,125,FALSE,0,0,0,0,10,0,10,6,2,2.33,126,4.562605142593384,29.618769884109497,1.5297529697418213,0.5241179466247559,36.23524594306946
2025-11-15T22:38:10.588845,psf/requests,2d5517682b3b38547634d153cea43d48fbc8cdb5,requests/models.py,tests/test_lowlevel.py,5,1,5,5,5,5,12,Fix inconsistent exception for JSONDecode error (#6097),Fix inconsistent exception for JSONDecode error,This PR should address #6084 by adding appropriate handling for alternative utf encodings. The previous change in #5856 only accounted for the happy path where we would attempt to load directly from `.text`. This patch will now ensure we're also catching and raising the appropriate exception when we guess the encoding based on byte sequencing.,gemini-2.5-pro,gemini,4366,724,8101,13191,191,191,2,3128,FALSE,FALSE,0,0,0,0,38,32,70,N/A,N/A,N/A,N/A,TRUE,562,2,0,0,2,0,2,192,192,2,3148,5.538353204727173,75.42706513404846,N/A,75.21124720573425,156.1766655445099
2025-11-15T22:39:28.788622,psf/requests,60389df6d69ce833164696dcf36cbb43336d3426,src/requests/adapters.py,tests/test_adapters.py,2,1,5,2,1,1,0,Trim excess leading path separators,Leading slash in uri followed by column fails,"Leading slash in uri followed by column fails.

## Expected Result

```python
requests.get('http://127.0.0.1:10000//v:h')
<Response [200]>
```

## Actual Result

```python
Traceback (most recent call last):
  File ""/usr/local/lib/python3.11/site-packages/urllib3/util/url.py"", line 425, in parse_url
    host, port = _HOST_PORT_RE.match(host_port).groups()  # type: ignore[union-attr]
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'groups'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.11/site-packages/requests/api.py"", line 73, in get
    return request(""get"", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/requests/api.py"", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/requests/sessions.py"", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/requests/sessions.py"", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/requests/adapters.py"", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py"", line 711, in urlopen
    parsed_url = parse_url(url)
                 ^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/urllib3/util/url.py"", line 451, in parse_url
    raise LocationParseError(source_url) from e
urllib3.exceptions.LocationParseError: Failed to parse: //v:h
```

## Reproduction Steps

```python
import requests
requests.get('http://127.0.0.1:10000//v:h')

```

## System Information

    $ python -m requests.help

```json
{
  ""chardet"": {
    ""version"": null
  },
  ""charset_normalizer"": {
    ""version"": ""3.3.2""
  },
  ""cryptography"": {
    ""version"": """"
  },
  ""idna"": {
    ""version"": ""3.6""
  },
  ""implementation"": {
    ""name"": ""CPython"",
    ""version"": ""3.11.8""
  },
  ""platform"": {
    ""release"": ""5.10.209-198.812.amzn2.x86_64"",
    ""system"": ""Linux""
  },
  ""pyOpenSSL"": {
    ""openssl_version"": """",
    ""version"": null
  },
  ""requests"": {
    ""version"": ""2.31.0""
  },
  ""system_ssl"": {
    ""version"": ""300000b0""
  },
  ""urllib3"": {
    ""version"": ""2.2.1""
  },
  ""using_charset_normalizer"": true,
  ""using_pyopenssl"": false
}
```

<!-- This command is only available on Requests v2.16.4 and greater. Otherwise,
please provide some basic information about your system (Python version,
operating system, &c). -->
",gemini-2.5-pro,gemini,4561,562,13757,18880,62,62,3.18,1473,FALSE,FALSE,0,0,0,0,30,28,58,N/A,N/A,N/A,N/A,TRUE,582,2,0,0,3,0,3,63,63,3.18,1485,3.5355231761932373,127.39473628997803,N/A,74.54575586,205.47601532936096
2025-11-15T22:40:45.495756,pydantic/pydantic,c9c31deb5b8b5dda8801ecdfd0aba93941d512ff,pydantic/_internal/_generate_schema.py,tests/test_types.py,5,22,5,5,5,1,308,Fix support for enums with `NamedTuple` as values (#12506),Enums with `NamedTuple` values not supported,"### Initial Checks

- [x] I confirm that I'm using Pydantic V2

### Description

I am using a custom `class` as branches of an `Enum`; And I would like to specify a model's property to be of type this `Enum`.

However, `pydantic` rejects instances of the `Enum` as valid values for the said property.

```shell
$ python src/test.py 
Traceback (most recent call last):
  File ""/app/src/yolo.py"", line 18, in <module>
    MyModel(s=MyEnum.FOO)
    ~~~~~~~^^^^^^^^^^^^^^
  File ""/app/.venv/lib/python3.13/site-packages/pydantic/main.py"", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
  File ""/usr/local/lib/python3.13/enum.py"", line 726, in __call__
    return cls.__new__(cls, value)
           ~~~~~~~~~~~^^^^^^^^^^^^
  File ""/usr/local/lib/python3.13/enum.py"", line 1203, in __new__
    raise ve_exc
ValueError: 'foo' is not a valid MyEnum
```

### Example Code

```Python
# src/test.py
from enum import Enum
from typing import NamedTuple
from pydantic import BaseModel


class _Something(NamedTuple):
    content: str


class MyEnum(_Something, Enum):
    FOO = ""foo""


class MyModel(BaseModel):
    s: MyEnum


MyModel(s=MyEnum.FOO)
```

### Python, Pydantic & OS Version

```Text
pydantic version: 2.11.7
        pydantic-core version: 2.33.2
          pydantic-core build: profile=release pgo=false
               python version: 3.13.5 (main, Jul 22 2025, 04:26:21) [GCC 12.2.0]
                     platform: Linux-6.12.49-x86_64-with-glibc2.36
             related packages: fastapi-0.116.1 typing_extensions-4.15.0 mypy-1.18.2
                       commit: unknown
```",gemini-2.5-pro,gemini,6258,703,9193,16154,520,549,2.27,12721,FALSE,FALSE,0,0,0,0,43,119,162,N/A,N/A,N/A,N/A,FALSE,0,0,0,0,5,3,8,520,549,2.27,12721,38.23156142234802,82.43680810928345,N/A,16.095366954803467,136.76373648643494
2025-11-15T22:41:16.334940,pydantic/pydantic,f787c339795f75b9e6bb195d4c1955a50bf5874f,pydantic/json_schema.py,tests/test_json_schema.py,7,14,5,7,5,5,253,Fix nested model schema deduplication in JSON schema generation (#12494),Duplicated models in JSON schema generation,"### Initial Checks

- [x] I confirm that I'm using Pydantic V2

### Description

When Pydantic models are ""deeply"" ""nested"" (one of the attribute is also a Pydantic model), the `GenerateJsonSchema.generate_definitions` function sometimes creates duplicated identical schema (e.g. Schema-Input and Schema-Output).

I initially created a [discussion in FastAPI repository]( https://github.com/fastapi/fastapi/discussions/14288#discussioncomment-14883512), thinking it was a bug in fastapi itself.

Turns out it's a bug in pydantic. See code to reproduce the bug.

### Example Code

```Python
from pydantic.json_schema import GenerateJsonSchema
from pydantic import BaseModel

REF_TEMPLATE = ""#/components/schemas/{model}""

schema_generator = GenerateJsonSchema(ref_template=REF_TEMPLATE)


class Level3(BaseModel):
    level_4: str


class Level2(BaseModel):
    # if level_3 is a str, the test passes. If it's Level3, it fails.
    level_3: Level3
    # level_3: str


class Level1(BaseModel):
    level_2: Level2


inputs = [
    (Level1, ""validation"", Level1.__pydantic_core_schema__),
    (Level1, ""serialization"", Level1.__pydantic_core_schema__),
]


field_mapping, definitions = schema_generator.generate_definitions(inputs=inputs)


assert ""Level1"" in definitions
assert ""Level1-Input"" not in definitions
assert ""Level1-Output"" not in definitions
```

### Python, Pydantic & OS Version

```Text
pydantic version: 2.12.2
        pydantic-core version: 2.41.4
          pydantic-core build: profile=release pgo=false
               python version: 3.13.3 (main, May 17 2025, 13:30:59) [Clang 20.1.4 ]
                     platform: macOS-15.6-arm64-arm-64bit-Mach-O
             related packages: fastapi-0.119.0 mypy-1.18.2 email-validator-2.3.0 typing_extensions-4.15.0
                       commit: unknown
```",gemini-2.5-pro,gemini,6403,750,14100,21253,507,450,2.14,10319,FALSE,FALSE,0,0,0,0,45,45,90,N/A,N/A,N/A,N/A,TRUE,5581,0,0,0,6,1,7,508,453,2.14,10336,1.5242087841033936,124.8768298625946,N/A,28.91860842704773,155.31964707374573
2025-11-15T22:41:45.360673,pydantic/pydantic,ad9c3775fef295111c5fc1a37d1d0cc88e219c88,pydantic/fields.py,tests/test_dataclasses.py,7,6,5,7,5,5,149,Fix `InitVar` being ignored when using with the `pydantic.Field()` function (#12495),Fix `InitVar` being ignored when using with the `pydantic.Field()` function,"

<!-- Thank you for your contribution! -->
<!-- Unless your change is trivial, please create an issue to discuss the change before creating a PR -->

## Change Summary

Fixes part of https://github.com/pydantic/pydantic/issues/12079. I'll see in a separate change if I can tackle support for `Field(init_var=True)`.

<!-- Please give a short summary of the changes. -->

## Related issue number

<!-- WARNING: please use ""fix #123"" style references so the issue is closed when this PR is merged. -->

## Checklist

* [ ] The pull request title is a good summary of the changes - it will be used in the changelog
* [ ] Unit tests for the changes exist
* [ ] Tests pass on CI
* [ ] Documentation reflects the changes where applicable
* [ ] My PR is ready to review, **please add a comment including the phrase ""please review"" to assign reviewers**
",gemini-2.5-pro,gemini,4956,454,17800,23210,184,194,3.67,4039,FALSE,FALSE,0,0,0,0,14,64,78,N/A,N/A,N/A,N/A,TRUE,5582,0,0,0,4,0,4,185,196,3.67,4052,1.020867109298706,149.02606749534607,N/A,27.80504035949707,177.85197496414185
2025-11-15T22:47:11.177585,pytest-dev/pytest,e3facc0f8115a52606151c436ab3cb0816dbc0c3,testing/python/raises.py; testing/python/raises_group.py,src/_pytest/raises.py,8,38,5,8,10,5,0,Improve clarity of 'expected exception' error messages (#13861),Refactor: Improve clarity of 'expected exception' error messages,"## Purpose

This PR addresses a `TODO` in `testing/python/raises_group.py` that pointed out a poorly structured sentence in a test's expected error message.

I traced this back to the source of the message in `src/_pytest/raises.py` and refactored it to be clearer and more direct.

* **Old format:** `expected exception must be a BaseException type, not '...'`
* **New format:** `Expected a BaseException type, but got '...'`

## Tests

Because the library's error message was changed, this PR also updates all the tests in the following files that were asserting the *old* message:

* `testing/python/raises.py`
* `testing/python/raises_group.py`

This resolves the `TODO` and makes the error messages more consistent and readable.",gemini-2.5-pro,gemini,4644,359,7877,12880,83,13,0.77,6001,FALSE,FALSE,0,0,0,0,13,30,43,N/A,N/A,N/A,N/A,TRUE,3925,0,0,0,8,15,23,83,13,0.77,6001,3.5706028938293457,72.11362123,N/A,313.27521681785583,388.9594409
2025-11-15T22:52:01.322088,pytest-dev/pytest,7ff334e7880d8aa03de7428ba2941fc92d8f892e,testing/python/approx.py,src/_pytest/python_api.py,9,4,5,9,5,5,0,`pytest.approx` returns a clearer error mesage when comparing mappings with different keys (#13818),`pytest.approx` gives confusing error when dictionaries have different keys,"Hello,

While working on #12444, I used `pytest.approx` to compare two dictionaries with different keys. 
The error message is pretty confusing. Instead of telling me that the keys don't match, I get an `AssertionError` with a representation issue and a `KeyError` for the missing key.

**What I expected:** A clearer message saying the dictionaries have different keys, something like: ""comparison failed. dictionary has different keys: expected (a, c), got (a, b)"".

**What I got instead:**
```
AssertionError: assert approx({'a': ... 4 ± 4.0e-06}) == {'a': 1, 'c': 3}

(pytest_assertion plugin: representation of details failed: /home/cmnemoi/code/repro_bugs/.venv/lib/python3.13/site-packages/_pytest/python_api.py:271: KeyError: 'b'.
 Probably an object has a faulty __repr__.)
```

## How to reproduce
```python
import pytest

def test_approx_dicts_with_mismatch_on_keys() -> None:
    expected = {""a"": 1, ""c"": 3}
    actual = {""a"": 1, ""b"": 4}

    assert pytest.approx(actual) == expected
```

## Environment

- **pytest version**: 8.4.2
- **Operating System**: Pop_OS! 22.04 (GNU/Linux)

## Installed Packages
```
Package                 Version   Editable project location
----------------------- --------- -----------------------------
annotated-types         0.7.0
certifi                 2025.10.5
charset-normalizer      3.4.4
click                   8.1.8
click-option-group      0.5.9
coverage                7.10.7
Deprecated              1.2.18
dotty-dict              1.3.1
filelock                3.20.0
gitdb                   4.0.12
GitPython               3.1.45
idna                    3.11
importlib_resources     6.5.2
iniconfig               2.1.0
Jinja2                  3.1.6
markdown-it-py          4.0.0
MarkupSafe              3.0.3
mdurl                   0.1.2
mypy                    1.18.2
mypy_extensions         1.1.0
packaging               25.0
pathspec                0.12.1
pip                     25.2
pluggy                  1.6.0
pydantic                2.12.2
pydantic_core           2.41.4
Pygments                2.19.2
pytest                  8.4.2
pytest-cov              7.0.0
pytest-mypy             1.0.1
pytest-watcher          0.4.3
python-gitlab           6.4.0
python-semantic-release 10.4.1
requests                2.32.5
requests-toolbelt       1.0.1
rich                    14.2.0
ruff                    0.14.0
shellingham             1.5.4
smmap                   5.0.2
tomlkit                 0.13.3
typing_extensions       4.15.0
typing-inspection       0.4.2
urllib3                 2.5.0
watchdog                6.0.0
wrapt                   1.17.3
```",FAILED,FAILED,0,0,0,0,118,53,1.41,6720,FALSE,FALSE,0,0,0,0,0,0,0,PATCH_GENERATION_FAILED,PATCH_GENERATION_FAILED,PATCH_GENERATION_FAILED,PATCH_GENERATION_FAILED,TRUE,3926,0,0,0,15,0,15,119,53,1.41,6776,3.035623312,0,0,286.98254203796387,290.0181653499603
2025-11-15T22:57:25.310995,pytest-dev/pytest,dd47a89d6eb03e7d18e2fea4fd0a198b1ec5f4a5,testing/python/metafunc.py; testing/typing_checks.py,src/_pytest/deprecated.py; src/_pytest/mark/structures.py; src/_pytest/python.py,15,12,5,15,10,5,0,Deprecate passing iterator argvalues to parametrize,Deprecate iterables which are not collections (e.g. generators) for tests parametrization,"# Description:
When using a generator function as the data source for a parameterized test, the second call to pytest.main() fails with an ID mismatch error. The test works correctly on the first run but fails on subsequent runs.

# How to reproduce:

Create a test file with the following content:
```
import pytest

def data_generator():
    yield 1
    yield 2

@pytest.mark.parametrize(""bar"", data_generator(), ids=lambda x: f""dynamic_{x}"")
def test_foo(bar):
    pass

if __name__ == '__main__':
    base_cmd = [
        ""-q"",
        ""--collect-only""
    ]
    pytest.main(base_cmd)  # First run - works
    pytest.main(base_cmd)  # Second run - fails
```
Run the test file

# Actual behavior:
First run succeeds:
```
tests/test_bar.py::test_foo[dynamic_1]
tests/test_bar.py::test_foo[dynamic_2]
```
Second run fails with:
```
ERROR collecting tests/test_bar.py
In test_foo: 1 parameter sets specified, with different number of ids: 2
```
# Expected behavior:
Both runs should successfully collect and run the tests with the same parameter sets.

# Versions:

Python: 3.11

pytest: 8.3.5

OS: macos

# Additional context:
The issue appears to be related to the generator being exhausted after the first run. When the test collection happens again in the second pytest.main() call, the generator has already been consumed, leading to the ID mismatch error.

A workaround is to convert the generator to a list:

```
@pytest.mark.parametrize(""bar"", list(data_generator()), ids=lambda x: f""dynamic_{x}"")
```
However, this defeats the purpose of using a generator for memory efficiency with large datasets.",gemini-2.5-pro,gemini,6147,638,16107,22892,177,15,1.61,7600,FALSE,FALSE,0,0,0,0,27,3,30,N/A,N/A,N/A,N/A,FALSE,1,1,0,1,84,0,84,183,15,1.6,7786,3.036558151245117,150.3336148262024,N/A,320.7250590324402,474.0952320098877
2025-11-15T22:57:34.128015,more-itertools/more-itertools,e733192454b73bbea8645e4dcbee95b13eba714c,more_itertools/more.py,tests/test_more.py,11,121,5,11,5,2,530,Issue 916: lt only for is_sorted (#917),is_sorted() should only depend on less-than comparisons,"The `sorted()` function is [documented to only use less-than comparisons](https://docs.python.org/3/library/functions.html#sorted).  The `is_sorted()` predicate should do the same:

```
def is_sorted(iterable, key=None, reverse=False, strict=False):
    it = iterable if (key is None) else map(key, iterable)
    a, b = tee(it)
    next(b, None)
    if reverse:
        b, a = a, b
    return all(map(lt, a, b)) if strict else not any(map(lt, b, a))
```

Note:  This breaks some of the tests with *NaN* values when `strict=True`.  However, I think all of the *NaN* tests are dubious.

For example, `math.nan <= 0 <= math.nan <= 0` is `False` but `is_sorted(['nan', 0, 'nan', 0], key=float)` currently returns `True`.

In general, it doesn't make sense to claim a dataset containing a *NaN* is sorted.  When `data = [8.0, 4.0, 0.0, math.nan, 6.0, 2.0]`, the call `is_sorted(sorted(data))` nonsensically returns `False`.",gemini-2.5-pro,gemini,7879,609,5403,13891,683,683,2.03,11605,FALSE,FALSE,0,0,0,0,29,29,58,N/A,N/A,N/A,N/A,TRUE,664,0,1,0,9,8,17,683,684,2.03,11618,1.5569040775299072,43.44615840911865,N/A,4.10649848,49.10956097
2025-11-15T22:57:43.372903,more-itertools/more-itertools,33ff9cd2edde12c3de18dc806b0397dcf8039011,more_itertools/more.py,tests/test_more.py,11,114,5,11,5,0,507,Add doublestarmap (closes #679),Consider introducing starstarmap,"### Description
`starstarmap` is the lost cousin of `itertools.starmap`, so whereas the relation between `map` and `starmap`  parallels the relation between `function(a,b)` and `function(*c)`, the relationship between `map` and `starstarmap` parallels the relation between `function(a,b)` and `function(**d)`.

And similar to how `starmap` can be implemented as:
```
def starmap(function, iterable):
    for args in iterable:
        yield function(*args)
```
`starstarmap` could be implemented as:
```
def starstarmap(function, iterable):
    for args in iterable:
        yield function(**args)
```

### References

The argument for inclusion would be the fact that it simply seems missing from `itertools`, and its usefulness follows from the same pattern as `starmap`.

### Examples
A REST interface returns a list of JSON objects:
```
[
    {""identifier"": 1, ""title"": ""The Cathedral and the Bazaar""},
    {""identifier"": 2, ""title"": ""Homesteading the Noosphere""},
   ...
]
```
Each object needs to be processed and transformed via a processing function:
```
def func(identifier, title):
    ...
    return Book(...)
```
Now to convert from the JSON-parsed list of dictionaries to processed books, we can simply do:
```
books = starstarmap(func, dictionaries)
```
",gemini-2.5-pro,gemini,8136,383,7766,16285,635,656,2.01,10841,TRUE,TRUE,634,8,1,0,20,0,20,637,657,2.01,10860,TRUE,642,0,1,0,19,0,19,637,657,2.01,10860,1.5497040748596191,69.29647445678711,4.091307163238525,3.0608575344085693,77.99834322929382
2025-11-15T22:58:14.868328,more-itertools/more-itertools,21d3d88359437146a6d8882328df4bd73edd685c,more_itertools/recipes.py,tests/test_recipes.py,1,61,5,1,5,5,139,Issue 1003: Multidimensional reshape() (#1062),Issue 1003: Multidimensional reshape(),"The core use case is to transform one multidimensional array into another multidimensioal array of the same size but with a different shape.  The remaining design decisions pertain to handling size mismatches and non-uniform input:

* The iterator stops when the requested shape has been built or the input has been exhausted, whichever comes first.
* Accordingly, an empty input gives an empty output.
* Like `collapse`, subarrays are not required to have equal sizes. This isn't typical but is easily supported.
* Unlike `collapse`, each dimension is presumed to uniformly consist of all arrays or all scalars. 
  * If an initial scalar is followed by an iterable, that iterable is treated as a scalar. 
  * If an initial iterable is followed by a non-iterable, a TypeError is raised.

The lazily evaluated transformation stack is constructed by a while-loop and a `reduce` call. The only slow part is the value lookahead and the  `is_scalar` check which are done only once for each dimension in the input.

Once constructed, the transformation stack runs at C speed. It consists of nested calls to `islice`, `batched`, and `chain`. Here is the transformation stack for converting a 3-d structure to a 4-d structure:

```python
# Input shape: 2 x 3 x 2
matrix = (((0, 1), (2, 3), (4, 5)), ((6, 7), (8, 9), (10, 11)))

# Transformation stack
iterator = iter(matrix)  
iterator = chain.from_iterable(chain((next(iterator),), iterator))
iterator = chain.from_iterable(chain((next(iterator),), iterator))
scalar_stream = chain((next(iterator),), iterator)
reshaped = islice(batched(batched(batched(scalar_stream, 2), 1), 1), 6)

# Output shape: 6 x 1 x 1 x 2
print(list(reshaped))
```",gemini-2.5-pro,gemini,5271,1197,20800,27268,161,122,1.81,2907,TRUE,TRUE,693,7,1,0,82,0,82,173,138,1.81,3096,TRUE,694,0,1,0,60,8,68,168,128,1.81,3031,1.021697759628296,190.49541354179382,14.719231367111206,15.598440170288086,221.8347828388214
2025-11-15T22:58:28.084604,dateutil/dateutil,79d2e486a34911ada6d81701f14dd3c4b9790afd,dateutil/parser/_parser.py,dateutil/test/test_parser.py,10,3,5,10,5,5,54,Fix TypeError in parser wrapper logic,parser raises `TypeError` in wrapper logic,"Apparently sometimes the parser catches a type of `ValueError` *other* than the ones we raise - specifically `IllegalMonthError`, which takes as an argument something other than a string. Minimal reproducer:

```python
>>> from dateutil import parser
>>> parser.parse(""0-100"")
>>> dateutil.parser.parse(""0-100"")
```

This raises the following error:
<details>

```
---------------------------------------------------------------------------
IllegalMonthError                         Traceback (most recent call last)
/usr/lib/python3.7/site-packages/dateutil/parser/_parser.py in parse(self, timestr, default, ignoretz, tzinfos, **kwargs)
    654         try:
--> 655             ret = self._build_naive(res, default)
    656         except ValueError as e:

/usr/lib/python3.7/site-packages/dateutil/parser/_parser.py in _build_naive(self, res, default)
   1237
-> 1238             if cday > monthrange(cyear, cmonth)[1]:
   1239                 repl['day'] = monthrange(cyear, cmonth)[1]

/usr/lib/python3.7/calendar.py in monthrange(year, month)
    123     if not 1 <= month <= 12:
--> 124         raise IllegalMonthError(month)
    125     day1 = weekday(year, month, 1)

IllegalMonthError: bad month number 0; must be 1-12

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
<ipython-input-3-f7db2d9dc78c> in <module>
----> 1 dateutil.parser.parse(""0-100"")

/usr/lib/python3.7/site-packages/dateutil/parser/_parser.py in parse(timestr, parserinfo, **kwargs)
   1372         return parser(parserinfo).parse(timestr, **kwargs)
   1373     else:
-> 1374         return DEFAULTPARSER.parse(timestr, **kwargs)
   1375
   1376

/usr/lib/python3.7/site-packages/dateutil/parser/_parser.py in parse(self, timestr, default, ignoretz, tzinfos, **kwargs)
    655             ret = self._build_naive(res, default)
    656         except ValueError as e:
--> 657             six.raise_from(ParserError(e.args[0] + "": %s"", timestr), e)
    658
    659         if not ignoretz:

TypeError: unsupported operand type(s) for +: 'int' and 'str'
```
</details>

The core of the issue is [this line](https://github.com/dateutil/dateutil/blob/fc9b1625ebc729f01e449879b6b140abd12ae621/dateutil/parser/_parser.py#L657):

```python
        try:
            ret = self._build_naive(res, default)
        except ValueError as e:
            six.raise_from(ParserError(e.args[0] + "": %s"", timestr), e)
```

Since `e.args[0]` is apparently occasionally an integer.

I guess we can solve this this way:

```python
except ValueError as e:
    six.raise_from(ParseError(str(e) + "": %s"", timestr), e)
```

This will need a regression test. This is apparently the cause of matplotlib/matplotlib#15726.",gemini-2.5-pro,gemini,5469,745,4353,10567,394,425,2.43,6805,FALSE,FALSE,0,0,0,0,47,88,135,N/A,N/A,N/A,N/A,FALSE,0,0,0,9,1,1,2,394,425,2.43,6803,1.6863532066345215,44.39069485664368,N/A,2.5973777770996094,48.67442584037781
2025-11-15T22:58:32.915188,dateutil/dateutil,82fc863aa62b624318fbc6e2945bc5d8e3b030d2,dateutil/tz/tz.py,dateutil/test/test_tz.py,13,6,5,13,5,5,35,Catch UnicodeEncodeError for Windows in tz.gettz,tz.gettz broken when passed non-ASCII string on Windows for Python 2.7,"Related to #800, running this on Windows with Python 2.7:

```python
from dateutil import tz
tz.gettz('🐼')
```

Raises an error:

```
self = tzwin(u'\U0001f43c'), name = '🐼'
    def __init__(self, name):
        self._name = name
    
        with winreg.ConnectRegistry(None, winreg.HKEY_LOCAL_MACHINE) as handle:
            tzkeyname = text_type(""{kn}\\{name}"").format(kn=TZKEYNAME, name=name)
>           with winreg.OpenKey(handle, tzkeyname) as tzkey:
E           UnicodeEncodeError: 'ascii' codec can't encode characters in position 56-57: ordinal not in range(128)
dateutil\tz\win.py:220: UnicodeEncodeError
```

Even if `tz.tzwin` doesn't support unicode names, `tz.gettz` should not be raising an error in this situation.",gemini-2.5-pro,gemini,5366,1266,8921,15553,322,416,1.93,5517,FALSE,FALSE,0,0,0,0,92,6,98,N/A,N/A,N/A,N/A,FALSE,0,0,0,10,2,1,3,322,416,1.93,5521,2.5318307876586914,75.42571830749512,N/A,2.1081182956695557,80.06566739082336
2025-11-15T22:58:37.751998,dateutil/dateutil,bd69e8e964b6572184ab6bf49b651fba617eae4f,dateutil/tz/tz.py,dateutil/test/test_tz.py,13,6,5,13,5,5,36,Fix tz.gettz() behavior with empty string,gettz('') fails on windows,"Given the following (tested with Python 3.7 and dateutil 2.8.0):
```python
import dateutil.tz
print(dateutil.tz.gettz(''))
```

On Linux:
```
None
```

On Windows:
```
Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
  File ""H:\src\env\desc\lib\site-packages\dateutil\tz\tz.py"", line 1551, in __call__
    rv = self.nocache(name=name)
  File ""H:\src\env\desc\lib\site-packages\dateutil\tz\tz.py"", line 1635, in nocache
    tz = tzwin(name)
  File ""H:\src\env\desc\lib\site-packages\dateutil\tz\win.py"", line 223, in __init__
    self._std_abbr = keydict[""Std""]
KeyError: 'Std'
```",gemini-2.5-pro,gemini,5701,939,14336,20976,324,421,1.93,5549,FALSE,FALSE,0,0,0,0,76,84,160,N/A,N/A,N/A,N/A,FALSE,0,0,0,9,1,1,2,324,421,1.93,5548,2.5434792041778564,128.6318130493164,N/A,2.097442388534546,133.2727346420288
2025-11-15T22:58:58.422069,jsonpickle/jsonpickle,58ab85ee5c2480380e4a024503497d8ce4881504,jsonpickle/ext/pandas.py,tests/pandas_test.py,9,7,5,9,5,4,26,Fix legacy pandas dataframe decoding (#562),Fix legacy pandas dataframe decoding,"This fixes an issue I discovered when typing the pandas extension where we'd get a JSONDecodeError on PandasDfHandler's restore. I also added a warning and a test, and this test adds coverage for the make_read_csv_params function (which was previously untested). The make_read_csv_params function is only used for the pre-3.4 decoding though, so it's just a nice-to-have in terms of coverage rather than essential, because the restore_v3_3 function (which is the only caller for make_read_csv_params) should never change anymore. This should be released in its own 4.1.1 release if approved by @davvid (I would appreciate if you could do the release davvid).

One thing I'm considering is whether the two neighboring try/excepts should be merged, the only issue I can think of though would be the loss of specificity for the error messages (as to whether we know they're decoding a pre-3.4 object or not). Not sure how big of a deal this would be, but I'm leaving it separate for now.",gemini-2.5-pro,gemini,4016,665,6275,10956,62,39,2.12,2079,FALSE,FALSE,0,0,0,0,39,30,69,N/A,N/A,N/A,N/A,TRUE,398,0,1,0,21,3,24,63,40,2.12,2119,5.612941265106201,51.18037486076355,N/A,12.878228425979614,69.67154455184937
2025-11-15T22:59:10.722148,jsonpickle/jsonpickle,10b1b4236823d4723e941f8e2cb89c44f58ecf6d,jsonpickle/unpickler.py,tests/jsonpickle_test.py,4,13,5,4,5,0,121,Fix handling of Exception subclasses with keywork-only arguments (#565),Cannot decode subclass of exception with keyword parameter,"I am using jsonpickle v4.1.1.

I am likely doing something dumb, but I am getting an error when trying to decode into a subclass of Exception with one keyword only argument. I can minimally reproduce this as follows:

```py
import jsonpickle

class Foo(Exception):
    def __init__(self, a, *, b):
        self.a = a
        self.b = b

>>> jsonpickle.encode(Foo(""a"", b=""b""))
'{""py/reduce"": [{""py/type"": ""__main__.Foo""}, {""py/tuple"": [""a""]}, {""a"": ""a"", ""b"": ""b""}]}'

>>> jsonpickle.decode(jsonpickle.encode(Foo(""a"", b=""b"")))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/dfarr/Developer/resonatehq-examples/deep-research/.venv/lib/python3.12/site-packages/jsonpickle/unpickler.py"", line 103, in decode
    return context.restore(data, reset=reset, classes=classes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/dfarr/Developer/resonatehq-examples/deep-research/.venv/lib/python3.12/site-packages/jsonpickle/unpickler.py"", line 420, in restore
    value = self._restore(obj)
            ^^^^^^^^^^^^^^^^^^
  File ""/Users/dfarr/Developer/resonatehq-examples/deep-research/.venv/lib/python3.12/site-packages/jsonpickle/unpickler.py"", line 402, in _restore
    return restore(obj)
           ^^^^^^^^^^^^
  File ""/Users/dfarr/Developer/resonatehq-examples/deep-research/.venv/lib/python3.12/site-packages/jsonpickle/unpickler.py"", line 555, in _restore_reduce
    stage1 = f(*args)
             ^^^^^^^^
TypeError: Foo.__init__() missing 1 required keyword-only argument: 'b'
```

I also get a similar error when Foo is defined with two positional arguments and I instantiate as `Foo(""a"", b=""b"")`. However, I do not get this error if I instantiate with two positional args in this case `Foo(""a"", ""b"")`",gemini-2.5-pro,gemini,4832,788,11265,16885,211,212,2.39,4172,FALSE,FALSE,0,0,0,0,50,73,123,N/A,N/A,N/A,N/A,TRUE,398,0,1,0,6,1,7,212,213,2.39,4188,4.544170856,85.65138792991638,N/A,7.621408939361572,97.81696772575378
2025-11-15T22:59:23.105891,jsonpickle/jsonpickle,9a6822734ea5d06967c0c514db3c5d577d6c1dc2,conftest.py; jsonpickle/backend.py; jsonpickle/errors.py; jsonpickle/ext/gmpy.py; jsonpickle/ext/numpy.py; jsonpickle/ext/pandas.py; jsonpickle/ext/yaml.py; jsonpickle/handlers.py; jsonpickle/pickler.py; jsonpickle/tags.py; jsonpickle/tags_pd.py; jsonpickle/unpickler.py; jsonpickle/util.py; jsonpickle/version.py,tests/backend_test.py; tests/jsonpickle_test.py; tests/object_test.py; tests/util_test.py,35,77,5,35,10,0,225,"V5: Add type hints, drop support, remove functions (#563)",Would it be possible to add a stubfile? ,"Hey all, 

I've started using typing in python and have managed to get literally all my packages working with pylint, with the exception of jsonpickle. 

![Screenshot 2023-03-31 at 4 08 57 PM](https://user-images.githubusercontent.com/61122332/229098432-60392b6d-cfcf-47f6-99c2-ca46cddbe3be.png)
![Screenshot 2023-03-31 at 4 09 32 PM](https://user-images.githubusercontent.com/61122332/229098553-6a2e7a5f-ef99-40dd-8632-70076977d395.png)

Any chance we could get a stub file included? ",FAILED,FAILED,0,0,0,0,767,723,2.22,14832,FALSE,FALSE,0,0,0,0,0,0,0,PATCH_GENERATION_FAILED,PATCH_GENERATION_FAILED,PATCH_GENERATION_FAILED,PATCH_GENERATION_FAILED,TRUE,397,0,1,0,648,535,1183,763,727,2.29,17035,4.554852724075317,0,0,7.615642786026001,12.170495510101318
2025-11-15T22:59:45.536483,python-jsonschema/jsonschema,9a957d770d6e63646f4fc874bf8df6be4593f8c8,jsonschema/exceptions.py,jsonschema/tests/test_exceptions.py,9,5,5,9,5,5,46,Unambiguously quote and escape properties in JSON path rendering,`_Error.json_path` produces invalid or questionable JSON paths,"The `json_path` property on the `_Error` class does not escape or otherwise account for property names that are either blank or have non-alphanumeric characters.

For example, a property named `.` currently renders to JSON path as `$..`. It should instead be `$['.']`.

Here is a script that demonstrates the `json_path` rendering of various valid property names, which result in invalid or questionable JSON paths:


```python
import jsonschema


# Keys that currently produce invalid or questionable JSON paths.
keys = ["""", ""["", ""."", ""\\"", ""'"", "" "", ""\"""", ""a[0]""]

schema = {
    # Require that each key be a string.
    ""properties"": {key: {""type"": ""string""} for key in keys},
}

instance = {
    # Set each key to a non-string value.
    key: None for key in keys
}

validator = jsonschema.Draft7Validator(schema)
for error in validator.iter_errors(instance):
    print(f""property={error.path[-1]!r}\t\tjson_path={error.json_path!r}"")
```

And the output:

```shell
$ python demo.py 
property=''             json_path='$.'
property='['            json_path='$.['
property='.'            json_path='$..'
property='\\'           json_path='$.\\'
property=""'""            json_path=""$.'""
property=' '            json_path='$. '
property='""'            json_path='$.""'
property='a[0]'         json_path='$.a[0]'
```",gemini-2.5-pro,gemini,4462,324,6828,11614,66,31,1.65,1388,FALSE,FALSE,0,0,0,0,11,8,19,N/A,N/A,N/A,N/A,TRUE,7762,0,53,0,7,1,8,67,31,1.65,1421,1.1737539768218994,54.84483766555786,N/A,17.570783138275146,73.58937478065491
2025-11-15T23:00:04.277519,python-jsonschema/jsonschema,9f7cae7f1f576b56ee93b5dc52a59e2f1c846ca6,jsonschema/exceptions.py,jsonschema/tests/test_exceptions.py,9,4,5,9,5,5,45,Don't reorder dicts (particularly schemas) when rendering errors.,Make json look prettier in errors,"I'm not sure if this is a real issue or not, but something that bothered me is that validation errors use `""%r""` as a format string which causes your json to look pretty ugly in the error message with all strings looking like   `{u'key':u'value'}` instead of  `{""key"":""value""}`.

In my json API when I return an error it might be confusing for the developer using my API that  there's suddenly  and `u` in a string because that's not valid JSON.

Of course I could build a custom error message from all the errors I get, but that would be a bit tedious. Would it be something to consider to print the schemas and validated data as json strings instead of python dicts in error messages?

Perhaps it's something worth discussing.
",gemini-2.5-pro,gemini,4608,388,12761,17757,65,31,1.64,1373,FALSE,FALSE,0,0,0,0,19,28,47,N/A,N/A,N/A,N/A,TRUE,7672,1,53,0,11,4,15,66,31,1.65,1407,2.0309009552001953,107.59238243103027,N/A,16.59396743774414,126.21725082397461
2025-11-15T23:00:24.052188,python-jsonschema/jsonschema,b20234e86c4dadf5d691400383a6fc0a1e9afc34,jsonschema/exceptions.py,jsonschema/tests/test_exceptions.py,9,4,5,9,5,5,41,Consider errors from earlier indices (in instances) to be better matches,Improve `best_match` when used with applicators,"Applicators have poor best matched errors in some cases even when there is only a single schema within the applicator!

A reproducer is:

```python
from jsonschema import Draft202012Validator as Validator, exceptions

instance = [12, 12]

for applicator in ""anyOf"", ""oneOf"":
    # Should match {""items"": {""const"": 37}}
    single = {applicator: [{""items"": {""const"": 37}}]}
    error = exceptions.best_match(Validator(single).iter_errors(instance))
    print(single, ""\n\n"", error)

    # Should match {""items"": {""const"": 37}} due to type matching
    multiple = {applicator: [{""type"": ""object""}, {""items"": {""const"": 37}}]}
    error = exceptions.best_match(Validator(multiple).iter_errors(instance))
    print(multiple, ""\n\n"", error)

    # Should probably? match {""items"": {""const"": 37}} due to giving False low priority in any/oneOf
    multiple_false = {applicator: [False, {""items"": {""const"": 37}}]}
    error = exceptions.best_match(Validator(multiple_false).iter_errors(instance))
    print(multiple_false, ""\n\n"", error)
```

where there seem to be 3 related but separate issues.

#1002 is likely related if not the same, but these examples are probably more minimized. We should recheck that issue when fixing.",gemini-2.5-pro,gemini,4823,941,13031,18795,65,31,1.64,1283,FALSE,FALSE,0,0,0,0,60,53,113,N/A,N/A,N/A,N/A,TRUE,7685,1,53,0,7,6,13,65,31,1.64,1287,2.0348381996154785,120.79162621498108,N/A,17.62316060066223,140.4496250152588
2025-11-15T23:00:34.984545,marshmallow-code/marshmallow,2b84b56e9a80d07b65d74811673e8e5cf46cd0f1,src/marshmallow/constants.py,tests/test_deserialization.py,1,0,5,1,1,4,156,(fix) missing constant with len validation (#2861),(fix) missing constant with len validation,"Currently if you utilize the `missing` constant as fallback value for a field in a schema alongside a length validator it fails with the following error `""object of type '_Missing' has no len()`

I'm using missing like this because I have some function fields that parse some optional data (in an annoying list of dictionaries), and if you include a standard pythonic default value of `None, [], {}, """"` the blank field is still included in `load()` results instead of being excluded, where as if you use missing it gets dropped from result as expected. 

If you'd like to know more about my use case or my schema just let me know",gemini-2.5-pro,gemini,2435,202,4539,7176,4,0,1.25,30,TRUE,TRUE,1118,0,0,0,4,0,4,5,0,1.2,37,TRUE,1118,0,0,0,3,0,3,5,0,1.2,37,1.5590660572052002,34.71806764602661,3.112048625946045,2.5463509559631348,41.93553328514099
2025-11-15T23:00:40.196571,marshmallow-code/marshmallow,c83679506045fe7aea4fb7270235f20a2f8641b2,src/marshmallow/schema.py,tests/mypy_test_cases/test_schema.py,4,3,5,4,5,5,0,Fix types for class Meta options (#2806),Type annotation for Schema.Meta.fields in marshmallow/schema.py,"Hi @sloria, in https://github.com/marshmallow-code/marshmallow/pull/2760 you added type annotations for `Schema.Meta.fields` and other parameters. At present, `Schema.Meta.fields` is typed as `typing.ClassVar[tuple[Field] | list[Field]]`: https://github.com/marshmallow-code/marshmallow/blob/dev/src/marshmallow/schema.py#L355

It seems that the documentation shows `fields` as `tuple[str]` and likely `list[str]` would also be compatible (e.g. https://marshmallow.readthedocs.io/en/stable/marshmallow.schema.html#marshmallow.schema.Schema.Meta). `tuple[str]` is also how I'm using `Schema.Meta.fields` in my schema subclasses.

Are the annotations potentially incorrect or is my understanding of them wrong?",FAILED,FAILED,0,0,0,0,188,201,3.56,4552,FALSE,FALSE,0,0,0,0,0,0,0,PATCH_GENERATION_FAILED,PATCH_GENERATION_FAILED,PATCH_GENERATION_FAILED,PATCH_GENERATION_FAILED,TRUE,1238,0,0,0,9,6,15,188,201,3.56,4552,1.543518304824829,0,0,3.551771640777588,5.095289945602417
2025-11-15T23:00:43.890155,marshmallow-code/marshmallow,76bc28ae74e723760d54f097290ba85e717d5fe4,src/marshmallow/validate.py,tests/test_validate.py,14,0,5,14,5,5,58,fix: add `file` handling to URL fields (#2800),`fields.Url` does not accept `file` URLs without host,"# Steps to reproduce

Run this test case:
```
import marshmallow as mm
class LOL(mm.Schema):
    url = mm.fields.Url(schemes={'file'})

LOL().load(dict(url=""file:///var/storage/somefile.zip""))
```

# Expected result
```
{'url': 'file:///var/storage/somefile.zip'}
```

# Actual behavior
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/nisimond/work/blastarea/lolwut/lib/python3.11/site-packages/marshmallow/schema.py"", line 756, in loads
    return self.load(data, many=many, partial=partial, unknown=unknown)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/nisimond/work/blastarea/lolwut/lib/python3.11/site-packages/marshmallow/schema.py"", line 722, in load
    return self._do_load(
           ^^^^^^^^^^^^^^
  File ""/Users/nisimond/work/blastarea/lolwut/lib/python3.11/site-packages/marshmallow/schema.py"", line 909, in _do_load
    raise exc
marshmallow.exceptions.ValidationError: {'url': ['Not a valid URL.']}
```

# Workaround

This works:
```
LOL().load(dict(url=""file://localhost/var/storage/somefile.zip""))
```
...but the `hostname` portion is entirely optional with `file` URLs.",gemini-2.5-pro,gemini,5058,1105,22271,28434,125,83,2.31,2386,FALSE,FALSE,0,0,0,0,59,53,112,N/A,N/A,N/A,N/A,TRUE,1238,0,0,0,9,1,10,127,85,2.31,2423,1.0279927253723145,168.48657846450806,N/A,2.553244113922119,172.0678153038025
2025-11-15T23:02:50.286222,Textualize/textual,98afc2214b8ab2f3cdf995d8f1da32d44c763c35,src/textual/widgets/_text_area.py,tests/snapshot_tests/test_snapshots.py,6,2,5,6,5,3,322,fix(text area): fix stuck background with `css` theme,"`TextArea` background in `css` theme becomes ""sticky"" after setting a non-`css` theme","The description for the `css` theme of the `TextArea` widget suggests that, when it is used, the background of the `TextArea` will be whatever the `background` styling is for that widget. Out of the box this is the case. With `TextArea.theme` set to `css` you can change the `background` and that's the background for the `TextArea`.

However, if you change the `TextArea.theme` to something other than `css`, then change it back to `css`, the `background` of that `TextArea` is no longer responsive to a change in `background`; it gets stuck with whatever the background colour was for the last theme that wasn't `css`.

To illustrate:

```python
from pathlib import Path

from textual import on
from textual.app import App, ComposeResult
from textual.containers import Vertical
from textual.widgets import Button, TextArea

class TextAreaThemeApp(App[None]):

    CSS = """"""
    Screen {
        layout: horizontal;
        Vertical {
            width: auto;
        }
    }
    """"""

    def compose(self) -> ComposeResult:
        with Vertical():
            yield Button(""github_light + textual-light"", id=""light"")
            yield Button(""CSS + textual-dark"", id=""default"")
        with Vertical():
            yield TextArea(Path(__file__).read_text(), language=""python"", id=""sticky"")
            yield TextArea(Path(__file__).read_text(), language=""python"")

    @on(Button.Pressed, ""#light"")
    def _go_light(self) -> None:
        self.theme = ""textual-light""
        self.query_one(""#sticky"", TextArea).theme = ""github_light""

    @on(Button.Pressed, ""#default"")
    def _go_css(self) -> None:
        self.theme = ""textual-dark""
        self.query_one(""#sticky"", TextArea).theme = ""css""

if __name__ == ""__main__"":
    TextAreaThemeApp().run()
```

On startup the app looks like this:

<img width=""962"" height=""790"" alt=""Image"" src=""https://github.com/user-attachments/assets/adf88055-86de-44a7-ab8a-42bff37d2893"" />

After pressing the ""gitub_light + textual-light"" button it looks like this:

<img width=""962"" height=""790"" alt=""Image"" src=""https://github.com/user-attachments/assets/3dc87964-885b-4cd7-876e-c60ea9ad7cfb"" />

The top `TextArea` is using `github_light` as its theme and the bottom is using `css` still. Everything looks as we'd expect. Now if we press the `CSS + textual-dark` button, we get this:

<img width=""962"" height=""790"" alt=""Image"" src=""https://github.com/user-attachments/assets/db972f27-791e-4c52-93c3-6664e8817db1"" />

In this case the top `TextArea`, despite being themed as `css`, still has the `github_light` background.",gemini-2.5-pro,gemini,4993,648,3521,9162,348,305,1.9,7762,FALSE,FALSE,0,0,0,0,34,25,59,N/A,N/A,N/A,N/A,TRUE,3272,0,0,0,8,0,8,348,305,1.9,7778,20.621339559555054,30.46900177001953,N/A,71.70775890350342,122.7981002
2025-11-15T23:04:06.215503,Textualize/textual,b148ab2aa9012c60632a6e0aaf2db5465b7a9b34,src/textual/widgets/_input.py,tests/input/test_input_properties.py,5,3,5,5,5,5,7,fix(input): fix invalid cursor position after updating value,Changing `Input` value doesn't update cursor position,"Changing the `Input` value doesn't update the cursor position, which can result in an 'invalid' cursor position when the new value is shorter.

Here's a quick example to demonstrate:

```python
from textual.app import App, ComposeResult
from textual.geometry import clamp
from textual.widgets import Button, Input, Label


class ExampleApp(App):
    def compose(self) -> ComposeResult:
        yield Label(""[b]Enter a number between 1-10:[/]"")
        yield Input(
            ""999999999999999999999"",
            type=""integer"",
            select_on_focus=False,
        )

        yield Label(""[dim]The input value will be clamped after being submitted.[/]"")
        yield Label(""[dim]Notice that the cursor seems to disappear.[/]"")

        yield Button(""Check cursor position"", variant=""warning"")

    def on_input_submitted(self, event: Input.Submitted) -> None:
        clamped_value = clamp(int(event.value), 1, 10)
        event.input.value = str(clamped_value)

    def on_button_pressed(self) -> None:
        cursor_position = self.query_one(Input).cursor_position
        severity = ""error"" if cursor_position > 2 else ""information""
        self.notify(
            str(cursor_position),
            title=""Cursor position"",
            severity=severity,
        )


if __name__ == ""__main__"":
    app = ExampleApp()
    app.run()
```",gemini-2.5-pro,gemini,3748,454,6533,10735,153,127,1.89,3397,FALSE,FALSE,0,0,0,0,20,18,38,N/A,N/A,N/A,N/A,TRUE,3207,36,0,0,4,0,4,153,127,1.89,3406,8.035436153411865,52.046119689941406,N/A,67.56465077400208,127.64620661735535
2025-11-15T23:05:21.004403,Textualize/textual,8431b9c8ec73a2c54db91e9adb5bb5273a48d736,src/textual/_slug.py,tests/test_slug.py,1,1,4,1,5,5,2,fix(slug): remove all Supplemental Symbols in slug,[BUG] Emojis in Markdown headers cause a crash,"## The bug

Including an emoji (specifically ""🤖"") in a markdown header causes a crash.


## Minimal Repro

Tested against v5.0.1 and main@e3bae0071311d30a89b756919f1fe4800bb6a7cf

```python
from textual.app import App, ComposeResult
from textual.widgets import Markdown


class TestApp(App):
    """"""Markdown append bug repro""""""

    def compose(self) -> ComposeResult:
        yield Markdown(""## 🤖 This heading is broken\n"")

if __name__ == ""__main__"":
    app = TestApp()
    app.run()
```

## Traceback

(captured in the console using markdown append, doesn't fail as loudly as the above repro but is easier to copy/paste the exception which is the same)

```
future: <_GatheringFuture finished exception=BadIdentifier(""'heading-%F0%9F%A4%96-this-heading-is-broken-4381065296' is an invalid id; identifiers must contain only letters,
numbers, underscores, or hyphens, and must not begin with a number."")>
Traceback (most recent call last):
  File ""/Users/chris/bootloop/textual/src/textual/widgets/_markdown.py"", line 1416, in await_append
    new_blocks = list(self._parse_markdown(tokens))
  File ""/Users/chris/bootloop/textual/src/textual/widgets/_markdown.py"", line 1274, in _parse_markdown
    block.id = f""heading-{slug(block._content.plain)}-{id(block)}""
    ^^^^^^^^
  File ""/Users/chris/bootloop/textual/src/textual/dom.py"", line 783, in id
    check_identifiers(""id"", new_id)
    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File ""/Users/chris/bootloop/textual/src/textual/dom.py"", line 98, in check_identifiers
    raise BadIdentifier(
    ...<2 lines>...
    )
textual.dom.BadIdentifier: 'heading-%F0%9F%A4%96-this-heading-is-broken-4381065296' is an invalid id; identifiers must contain only letters, numbers, underscores, or hyphens, and
must not begin with a number.
```


## Textual Diagnose Output

<!-- This is valid Markdown, please paste the following directly in to a GitHub issue -->
# Textual Diagnostics

## Versions

| Name    | Value  |
|---------|--------|
| Textual | 5.0.1  |
| Rich    | 14.1.0 |

## Python

| Name           | Value                                                    |
|----------------|----------------------------------------------------------|
| Version        | 3.13.5                                                   |
| Implementation | CPython                                                  |
| Compiler       | Clang 20.1.4                                             |
| Executable     | /Users/chris/bootloop/textual-bug-test/.venv/bin/python3 |

## Operating System

| Name    | Value                                                                                                  |
|---------|--------------------------------------------------------------------------------------------------------|
| System  | Darwin                                                                                                 |
| Release | 24.5.0                                                                                                 |
| Version | Darwin Kernel Version 24.5.0: Tue Apr 22 19:54:43 PDT 2025; root:xnu-11417.121.6~2/RELEASE_ARM64_T8132 |

## Terminal

| Name                 | Value              |
|----------------------|--------------------|
| Terminal Application | iTerm.app (3.5.14) |
| TERM                 | xterm-256color     |
| COLORTERM            | truecolor          |
| FORCE_COLOR          | *Not set*          |
| NO_COLOR             | *Not set*          |

## Rich Console options

| Name           | Value                |
|----------------|----------------------|
| size           | width=123, height=43 |
| legacy_windows | False                |
| min_width      | 1                    |
| max_width      | 123                  |
| is_terminal    | True                 |
| encoding       | utf-8                |
| max_height     | 43                   |
| justify        | None                 |
| overflow       | None                 |
| no_wrap        | False                |
| highlight      | None                 |
| markup         | None                 |
| height         | None                 |
```
",gemini-2.5-pro,gemini,3769,400,8216,12385,5,2,1.33,123,FALSE,FALSE,0,0,0,0,20,19,39,N/A,N/A,N/A,N/A,TRUE,3282,24,0,0,1,1,2,5,2,1.33,123,7.123219013214111,74.44766951,N/A,67.40654754638672,148.97743606567383
2025-11-15T23:05:29.280367,tartley/colorama,44dc8af9bc8111b70dd046a822bb26c851ba3687,colorama/ansitowin32.py,colorama/tests/ansitowin32_test.py,2,1,5,2,5,2,1,Fix #81: fix ValueError when a closed stream was used,"""ValueError: I/O operation on closed file"" with pytest since 0.3.5","Since upgrading colorama to 0.3.5, I get the following when running [pytest](http://www.pytest.org/) (on Linux):

```
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "".../python3.4/site-packages/colorama/initialise.py"", line 19, in reset_all
    AnsiToWin32(orig_stdout).reset_all()
  File "".../python3.4/site-packages/colorama/ansitowin32.py"", line 67, in __init__
    strip = conversion_supported or not is_a_tty(wrapped)
  File "".../python3.4/site-packages/colorama/ansitowin32.py"", line 17, in is_a_tty
    return hasattr(stream, 'isatty') and stream.isatty()
ValueError: I/O operation on closed file
```
",gemini-2.5-pro,gemini,3216,287,3371,6874,54,38,2.33,1400,FALSE,FALSE,0,0,0,0,7,2,9,N/A,N/A,N/A,N/A,FALSE,32,2,4,0,1,1,2,55,39,2.33,1407,4.083273649215698,29.462377786636353,N/A,1.1163043975830078,34.66195583343506
2025-11-15T23:05:34.540778,tartley/colorama,a15e4222f3dc10bca263faee36f68da79bd7c816,colorama/ansitowin32.py,colorama/tests/ansitowin32_test.py,2,1,5,2,5,0,1,Fix #80 - strip ANSI codes if output is redirected,"Consider no-tty handling, again","Following issue #47, reset codes are now emitted even when redirection is taking place (i.e., no tty).
Since colorama calls reset_all when the program terminates, there is now an undesired effect when using colorama and redirection -- a reset code will be emitted along with the redirected output. See:

```
> python -c ""import colorama; colorama.init(); print('a')"" | xargs python -c ""import sys; print(sys.argv)""
['-c', 'a', '\x1b[0m']
```

In other words, commands that take output from scripts with colorama will now get an extra ANSI reset code, which they probably don't expect.
- The bahavior before fixing #47: On Linux when there is no tty, reset_all codes were stripped but other ANSI codes were not stripped
- The current behavior: On Linux when there is no tty, nothing is stripped and both reset codes and other color codes are written to the redirected output
- New, suggested behavior: On Linux when there is no tty, all ANSI codes should be stripped

Note that on Windows when there is no tty, all ANSI codes are already stripped.
@tartley noted in #47 that colorama should behave like other unix utilities (like 'ls') and output color codes only on tty. I disagreed, but I changed my mind now.

The change in the code is simple, if I'm not mistaken: change the default `strip` value from:

   strip = conversion_supported

to 

```
strip = conversion_supported or not is_a_tty(wrapped)
```

The impact is that the behavior of colorama on Linux with redirection will change.
Does this make sense? Does everyone agree that colorama should strip the color codes and the reset codes when there is no tty? Does the code change look right?
@tartley, your thoughts?
",gemini-2.5-pro,gemini,3577,604,5298,9479,53,37,2.33,1394,FALSE,FALSE,0,0,0,0,32,32,64,N/A,N/A,N/A,N/A,FALSE,31,2,4,0,1,1,2,54,38,2.33,1400,4.067234516143799,43.447927713394165,N/A,1.1158528327941895,48.63101506233215
2025-11-15T23:05:39.764534,tartley/colorama,bfa8c22e8f9b51ad3428c165087540cea5d44a1f,colorama/ansi.py; colorama/ansitowin32.py,colorama/tests/ansitowin32_test.py,7,4,5,7,10,3,3,Fix #247: fix OSC regex and handling,Regular expression in AnsiToWin32 exhibits catastrophic backtracking,"Hi there! I've been working on a new Python static analysis tool called [Dlint](https://github.com/dlint-py/dlint). Most recently I've been working on a rule that searches for regular expression denial-of-service: [DUO138](https://github.com/dlint-py/dlint/blob/master/docs/linters/DUO138.md). When running this rule against your codebase I found a violation:

```
$ python -m flake8 --select=DUO138 colorama/
colorama/ansitowin32.py:71:19: DUO138 catastrophic ""re"" usage - denial-of-service possible
```

After further investigation, it appears the violation in `colorama/ansitowin32.py` is a true positive. If we dig in further:

```python
class AnsiToWin32(object):
    ...
    ANSI_OSC_RE = re.compile('\001?\033\\]((?:.|;)*?)(\x07)\002?')
```

The violation occurs due to `'\001?\033\\]((?:.|;)*?)(\x07)\002?'`. In particular, this portion of the expression: `(?:.|;)*`. This is due to [mutually inclusive alternation](https://www.regular-expressions.info/redos.html) within a quantifier. Since `.` and `;` have character overlap. We can confirm the bug with a specially crafted string and the following code:

```python
from colorama import ansitowin32
ansitowin32.AnsiToWin32.ANSI_OSC_RE.match('\033]' + ';' * 64 + '\x08')
...Spins...
```

To fix the issue we should avoid the character overlap between `.` and `;`. One way to accomplish this is with the following expression:

```python
ANSI_OSC_RE = re.compile('\001?\033\\]((?:[^;]|;)*?)(\x07)\002?')
```

Hope this is helpful, let me know if you have any questions!",gemini-2.5-pro,gemini,3909,582,15475,19966,75,49,2.07,1726,FALSE,FALSE,0,0,0,0,26,32,58,N/A,N/A,N/A,N/A,TRUE,43,2,4,0,11,10,21,76,53,2.07,1736,4.040353059768677,142.46846532821655,N/A,1.0952775478363037,147.60409593582153
2025-11-15T23:06:02.440701,prompt-toolkit/python-prompt-toolkit,2aec8202cda4d1b9d62e6cae0c667790ea371d7d,src/prompt_toolkit/document.py; src/prompt_toolkit/shortcuts/dialogs.py,tests/test_document.py,3,20,5,3,10,5,12,Fix nested completer which got broken in previous 'get_word_before_cursor' fix (#2034),Fix nested completer which got broken in previous 'get_word_before_cursor' fix,Fix bug introduced in https://github.com/prompt-toolkit/python-prompt-toolkit/pull/2025,gemini-2.5-pro,gemini,4691,467,7605,12763,235,224,2.1,5707,FALSE,FALSE,0,0,0,0,20,18,38,N/A,N/A,N/A,N/A,TRUE,154,0,0,0,6,4,10,236,226,2.1,5717,2.0561134815216064,59.759692430496216,N/A,17.282931804656982,79.09873772
2025-11-15T23:06:19.291101,prompt-toolkit/python-prompt-toolkit,75eca874f8b3abbe7643d075711917504f3e4f6b,src/prompt_toolkit/document.py,tests/test_document.py,3,2,5,3,5,5,11,Fix get_word_before_cursor behavior (#2025),get_word_before_cursor() incorrectly returns string containing trailing whitespace when a `pattern` value is provided,"## Problem

### Library version: 3.0.52

`Document.get_word_before_cursor()` incorrectly returns string containing trailing whitespace when a `pattern` value is provided.

This happens because `Document._is_word_before_cursor_complete()` fails to check for whitespace at the word boundary in the case where `pattern` is true-ish.

The docstring claims:
> If we have whitespace before the cursor this returns an empty string.

So this behavior is incorrect.

## Repro

Use the following program:

```python
import re
from prompt_toolkit.document import Document


def print_word_before_cursor(document: Document, pattern: re.Pattern | None = None):
    text = document.text
    word = document.get_word_before_cursor(pattern=pattern)
    print(f""Text: {text!r}, Cursor Position: {document.cursor_position}, Word Before Cursor: {word!r}"")


if __name__ == ""__main__"":
    text = ""Fubar ""
    document = Document(text=text, cursor_position=len(text))


    # Show correct documented behavior:
    #     The get_word_before_cursor() method returns '' since the cursor
    #     is at a whitespace character.

    print_word_before_cursor(document)  # Correct behavior: returns ''


    # Show incorrect behavior:
    #     Using the same word search pattern as the library does,
    #     the get_word_before_cursor() method now returns 'Fubar ' even though the cursor
    #     is at a whitespace character.

    _FIND_WORD_RE = re.compile(r""([a-zA-Z0-9_]+|[^a-zA-Z0-9_\s]+)"") # copied verbatim from prompt_toolkit/document.py

    print_word_before_cursor(document, pattern=_FIND_WORD_RE)   # Incorrect behavior: returns 'Fubar '
```

## Resulting Output

> Text: 'Fubar ', Cursor Position: 6, Word Before Cursor: ''
> Text: 'Fubar ', Cursor Position: 6, Word Before Cursor: 'Fubar '

## Expected Output

> Text: 'Fubar ', Cursor Position: 6, Word Before Cursor: ''
> Text: 'Fubar ', Cursor Position: 6, Word Before Cursor: ''

## Workaround

```python
text = """" if doc.char_before_cursor.isspace() else doc.get_word_before_cursor(pattern=mypattern)
```",gemini-2.5-pro,gemini,4372,486,11971,16829,210,221,2.03,4426,FALSE,FALSE,0,0,0,0,20,18,38,N/A,N/A,N/A,N/A,FALSE,146,8,0,0,2,4,6,211,222,2.03,4426,2.0406689643859863,99.06829595565796,N/A,14.668065786361694,115.77703070640564
2025-11-15T23:06:36.483577,prompt-toolkit/python-prompt-toolkit,75786f6a1e1faab49419681e7802259faa858e9e,src/prompt_toolkit/completion/nested.py; src/prompt_toolkit/formatted_text/utils.py,tests/test_formatted_text.py,1,5,5,1,9,5,13,Fix edge case with leading line ending in 'split_lines'. (#1982),Fix edge case with leading line ending in 'split_lines'.,No description provided.,gemini-2.5-pro,gemini,3666,575,16450,20691,26,12,1.56,544,FALSE,FALSE,0,0,0,0,29,28,57,N/A,N/A,N/A,N/A,TRUE,151,0,0,0,2,3,5,24,9,1.56,538,2.0220532417297363,126.31834888458252,N/A,15.079509735107422,143.41991186141968
2025-11-15T23:10:06.973690,agronholm/anyio,8175082ae1331fcd5816d965a909747c0584a984,src/anyio/_backends/_asyncio.py,tests/test_subprocesses.py,30,14,5,30,5,3,15,Fixed `Process.stdin.send` exceptions and checkpointing on asyncio (#1002),Fix `Process.stdin.send` exceptions and checkpointing on asyncio,"## Changes

Fixes the remaining issue reported in #671:

`_asyncio.Process.stdin.send` not raising `ClosedResourceError` or `BrokenResourceError`, but rather non-AnyIO exceptions. (The analogous `std{out,err}` case was already fixed in #752.)

I also fixed a missing checkpoint before writing to a subprocess send buffer on asyncio.

I also made `_close = False` unconditional in asyncio UDP `aclose`, similar to #980. I'm not sure that this is strictly necessary, but it seems safer and I don't see any downsides. See details in commit message.

## Checklist

- [x] You've added tests (in `tests/`) which would fail without your patch
- [ ] You've updated the documentation (in `docs/`), in case of behavior changes or new
features
- [x] You've added a new changelog entry (in `docs/versionhistory.rst`).",gemini-2.5-pro,gemini,7404,369,11892,19665,641,684,1.82,12720,FALSE,FALSE,0,0,0,0,12,3,15,N/A,N/A,N/A,N/A,TRUE,2,0,0,1,23,5,28,645,689,1.82,12786,2.5621700286865234,101.01012372970581,N/A,204.99152398109436,308.5638177394867
2025-11-15T23:13:31.165943,agronholm/anyio,1ed112c65628d3cce312e7b6875b9f914d174a71,src/anyio/_core/_sockets.py,tests/test_sockets.py,2,19,5,2,5,5,126,Ensure same port is used for IPv4/IPv6 when creating TCP listener with local_port=0 (#979),`create_tcp_listener(local_port=0)`: return listeners with the same port,"### Things to check first

- [X] I have searched the existing issues and didn't find my feature already requested there


### Feature description

> it would be nice if `multilistener = await create_tcp_listener(local_port=0)` used the same port for both of the listeners. This would be [implemented via something roughly] like
> 
> ```python
> def get_free_ipv4_ipv6_port(kind, bind_ipv4_addr, bind_ipv6_addr):
>     if platform_has_dualstack and bind_ipv4_addr == bind_ipv6_addr == """":
>         # We can use the dualstack_sock.bind(("""", 0)) trick
>         ...
>     else:
>         ...
> ```
> 
> Currently, folks who need this have to implement it downstream.)

Ow, that's definitely something I would like to fix! Strange that nobody has reported that as an issue. I definitely meant for the listener to bind to the same ephemeral port if the local port was omitted.

_Originally posted by @agronholm in https://github.com/agronholm/anyio/pull/856#discussion_r1907675607_

### Use case

.",gemini-2.5-pro,gemini,5188,2047,13877,21112,87,93,2.68,2331,FALSE,FALSE,0,0,0,0,160,78,238,N/A,N/A,N/A,N/A,TRUE,2,0,0,1,97,40,137,103,109,2.7,2565,2.528337240219116,115.3112280368805,N/A,201.55210304260254,319.39166831970215
2025-11-15T23:17:07.164473,agronholm/anyio,8bb9fe04a1c0a4b6615c843d4a88bba38a386059,src/anyio/_backends/_asyncio.py,tests/test_sockets.py,30,14,5,30,5,5,130,Fixed the inconsistent exception on sending to a closed TCP stream (#980),"`ClosedResourceError` vs. `BrokenResourceError` is sometimes backend-dependent, and is sometimes not raised at all (in favor of a non-AnyIO exception type)","### Things to check first

- [X] I have searched the existing issues and didn't find my bug already reported there

- [X] I have checked that my bug is still present in the latest release


### AnyIO version

main

### Python version

CPython 3.12.1

### What happened?

see the title. here are some reproducers for some issues that i noticed while working on a fix for #669: https://github.com/agronholm/anyio/compare/c6f0334e67818b90540dac20815cad9e0b2c7eee...b6576ae16a9b055e8109c8d2ca81e00bf439cb3f

the main question that these tests raise is: if a send stream is in a `Broken` state (i.e. our side of the connection learned that it was closed by the peer, or otherwise broken due to something external) and then is explicitly `aclose()`d by our side, should subsequent calls raise `BrokenResourceError` or should they raise `ClosedResourceError`? i.e. if you `aclose()` a `Broken` send stream, does that ""clear"" its `Broken`ness and convert it to just being `Closed`?

the documentation does not seem to discuss what should happen if the conditions for `ClosedResourceError` and `BrokenResourceError` are _both_ met. i am not sure what the behavior here is intended to be (or if it's intended to be undefined behavior?), as currently different streams do different things in this situation, with the behavior sometimes also varying between backends.

my initial intuition was that the intended behavior was to give precedence to `raise BrokenResourceError` over `raise ClosedResourceError` (this choice prevents the stream from changing from `Broken` to `Closed`). I thought this becuase this behavior looks like it was rather explicitly chosen when implementing the ""important"" asyncio-backend streams (TCP and UDP): they explicitly do _not_ set `self._closed` if they are already closing due to an external cause:

*   `SocketStream` https://github.com/agronholm/anyio/blob/c6efbe352705529123d55f87d6dbb366a3e0612f/src/anyio/_backends/_asyncio.py#L1173-L1184
*   `UDPStream` https://github.com/agronholm/anyio/blob/c6efbe352705529123d55f87d6dbb366a3e0612f/src/anyio/_backends/_asyncio.py#L1459-L1463

so I started to implement it: here is most of an implementation of that behavior: https://github.com/agronholm/anyio/compare/c6f0334e67818b90540dac20815cad9e0b2c7eee...1be403d20f94a8a6522f27f24a1830f2351aab3b [^1]

[^1]: note: github shows these commits out of order as it's sorting based on time rather than doing a correct topological sort, so it may be easier to look at these locally, in order.

however, `MemoryObjectSendStream` is also an ""important"" stream and it has the opposite behavior, even on the asyncio backend.

### How can we reproduce the bug?

see above",gemini-2.5-pro,gemini,8433,266,19245,27944,639,680,1.82,12688,FALSE,FALSE,0,0,0,0,5,4,9,N/A,N/A,N/A,N/A,TRUE,2,0,0,1,1,1,2,639,680,1.82,12688,1.5658011436462402,161.48200488090515,N/A,214.0998752117157,377.1476812362671
2025-11-15T23:17:14.696156,rubik/radon,ec8b5c34567ad7b150bcd16df0e9ac8c11ea0225,radon/cli/tools.py,radon/tests/test_cli_tools.py,0,23,5,0,5,2,10,fix: typo in Markdown export,Misspelled 'clasification' in markdown exports,Misspelled 'clasification' in markdown exports.  Should be spelled 'classification'.,gemini-2.5-pro,gemini,3935,354,6867,11156,102,153,1.83,2006,FALSE,FALSE,0,0,0,0,16,25,41,N/A,N/A,N/A,N/A,TRUE,400,0,0,0,2,2,4,102,153,1.83,2006,2.1096842288970947,59.54089140892029,N/A,2.6357617378234863,64.28633737564087
2025-11-15T23:17:19.342538,rubik/radon,d329a8da25633bbbf9cbbbda45ac1d949588956b,radon/cli/colors.py,radon/tests/test_cli_colors.py,0,0,0,0,3,5,0,feat(output): allow forcing colored/coloured output (#218),Feature Request: Force coloured output,"When I pipe radon into something else (`less`) it disables coloured output. This is obviously reasonable default behaviour. But a flag for forcing coloured output would be great, for use with a pager that accepts colours.",FAILED,FAILED,0,0,0,0,0,1,0,0,FALSE,FALSE,0,0,0,0,0,0,0,PATCH_GENERATION_FAILED,PATCH_GENERATION_FAILED,PATCH_GENERATION_FAILED,PATCH_GENERATION_FAILED,TRUE,400,0,0,0,12,1,13,4,4,0,38,3.0380313396453857,0,0,1.5691251754760742,4.607156515
2025-11-15T23:17:24.046657,rubik/radon,0613a8ab5a7d2fdb6352bb2e6d048192bbf4e8d1,radon/cli/tools.py,radon/tests/test_cli_tools.py,0,23,5,0,5,3,10,fix: CC type field value in Markdown output (#213),Fix CC Mardown type field value,"I encountered a small bug inspecting a library by `radon cc --md`. All the `Type` field values were identical to `C`.

How it looked like:

| Filename | Name | Type | Start:End Line | Complexity | Clasification |
| -------- | ---- | ---- | -------------- | ---------- | ------------- |
| tinydb/middlewares.py | CachingMiddleware.flush | C | 110:117 | 2 | A |
| tinydb/middlewares.py | CachingMiddleware.write | C | 101:108 | 2 | A |
| tinydb/middlewares.py | CachingMiddleware.read | C | 93:99 | 2 | A |
| tinydb/middlewares.py | CachingMiddleware | C | 73:124 | 2 | A |
| tinydb/middlewares.py | Middleware | C | 7:70 | 2 | A |
| tinydb/middlewares.py | CachingMiddleware.close | C | 119:124 | 1 | A |
| tinydb/middlewares.py | CachingMiddleware.__init__ | C | 85:91 | 1 | A |
| tinydb/middlewares.py | Middleware.__getattr__ | C | 64:70 | 1 | A |
| tinydb/middlewares.py | Middleware.__call__ | C | 22:62 | 1 | A |
| tinydb/middlewares.py | Middleware.__init__ | C | 18:20 | 1 | A |
| tinydb/operations.py | decrement | C | 62:69 | 1 | A |


How it looks now:

| Filename | Name | Type | Start:End Line | Complexity | Clasification |
| -------- | ---- | ---- | -------------- | ---------- | ------------- |
| tinydb/middlewares.py | CachingMiddleware.flush | M | 110:117 | 2 | A |
| tinydb/middlewares.py | CachingMiddleware.write | M | 101:108 | 2 | A |
| tinydb/middlewares.py | CachingMiddleware.read | M | 93:99 | 2 | A |
| tinydb/middlewares.py | CachingMiddleware | C | 73:124 | 2 | A |
| tinydb/middlewares.py | Middleware | C | 7:70 | 2 | A |
| tinydb/middlewares.py | CachingMiddleware.close | M | 119:124 | 1 | A |
| tinydb/middlewares.py | CachingMiddleware.__init__ | M | 85:91 | 1 | A |
| tinydb/middlewares.py | Middleware.__getattr__ | M | 64:70 | 1 | A |
| tinydb/middlewares.py | Middleware.__call__ | M | 22:62 | 1 | A |
| tinydb/middlewares.py | Middleware.__init__ | M | 18:20 | 1 | A |
| tinydb/operations.py | decrement | F | 62:69 | 1 | A |
",gemini-2.5-pro,gemini,4724,279,24471,29474,105,157,1.83,2011,FALSE,FALSE,0,0,0,0,5,5,10,N/A,N/A,N/A,N/A,TRUE,397,0,0,0,4,7,11,102,153,1.83,2006,3.0385875701904297,204.16958928108215,N/A,1.5612006187438965,208.76937747001648
2025-11-15T23:18:06.057593,pytest-dev/pytest-asyncio,746c11462e1c14f41146e9217ea43df99dc982c2,pytest_asyncio/plugin.py,tests/async_fixtures/test_async_fixtures_contextvars.py,8,50,5,8,5,5,0,Maintain contextvars.Context in fixtures and tests,ContextVar not propagated from fixture to test,"I'm trying to figure out why this code sample isn't working. I'm not experienced with asyncio yet, so this is very likely a simple misunderstanding on my part.

```python
from _contextvars import ContextVar

import pytest

blah = ContextVar(""blah"")


@pytest.fixture
async def my_context_var():
    blah.set(""hello"")
    assert blah.get() == ""hello""
    yield blah


@pytest.mark.asyncio
async def test_blah(my_context_var):
    assert my_context_var.get() == ""hello"" # this fails
```",gemini-2.5-pro,gemini,4697,342,13795,18834,167,138,1.32,3730,FALSE,FALSE,0,0,0,0,7,6,13,N/A,N/A,N/A,N/A,TRUE,1,0,0,0,65,7,72,173,142,1.32,3878,2.571934461593628,125.23329520225525,N/A,37.694772243499756,165.50000190734863
2025-11-15T23:18:45.507753,pytest-dev/pytest-asyncio,6f33fedd9d398267398d8189c1c08d2b812d8a9d,pytest_asyncio/plugin.py,tests/async_fixtures/test_shared_module_fixture.py,8,50,5,8,5,5,0,fix: Fixes a bug that caused module-scoped async fixtures to fail when reused in other modules.,`event_loop` not found for multiple test modules,"I just want to use module scope fixture but I get an error:

**[environment]**
python==3.10.14
pytest==8.2.2
pytest-asyncio==0.23.7

**[Directory]**

`tests/conftest.py`:
```python
import pytest_asyncio
@pytest_asyncio.fixture(scope=""module"")
async def foo():
    yield ""a value""
```
`tests/test_1.py`:
```python
import pytest
@pytest.mark.asyncio
async def test_func(foo):
        print(""test_1 test_func"", foo)
```
`tests/test_2.py`:
```python
import pytest
@pytest.mark.asyncio
async def test_func(foo):
    print(""test_2 test_func"", foo)
```
**[error]**

```text
# pytest -s tests/
============================= test session starts ==============================
platform linux -- Python 3.10.14, pytest-8.2.2, pluggy-1.5.0
rootdir: /home/sunyw
plugins: asyncio-0.23.7
asyncio: mode=strict
collected 2 items                                                              

tests/test_1.py .                                                        [ 50%]
tests/test_2.py E                                                        [100%]

==================================== ERRORS ====================================
_________________________ ERROR at setup of test_func __________________________
file /home/xxx/tests/test_2.py, line 3
  @pytest.mark.asyncio
  async def test_func(foo):
      print(""test_2 test_func"", foo)
file /home/xxx/tests/conftest.py, line 13
  @pytest_asyncio.fixture(scope=""module"")
  async def foo():
      yield ""a value""
E       fixture 'tests/test_1.py::<event_loop>' not found
>       available fixtures: _session_event_loop, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, event_loop, event_loop_policy, foo, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tests/test_2.py::<event_loop>, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/xxx/tests/conftest.py:13
=========================== short test summary info ============================
ERROR tests/test_2.py::test_func
========================== 1 passed, 1 error in 0.01s ==========================
```

according to the document [Decorators](https://pytest-asyncio.readthedocs.io/en/latest/reference/decorators/index.html):
> All scopes are supported, but if you use a non-function scope you will need to redefine the event_loop fixture to have the same or broader scope. Async fixtures need the event loop, and so must have the same or narrower scope than the event_loop fixture.

I add `event_loop` fixture in `conftest.py`:

```python
import pytest_asyncio
import asyncio
import pytest

@pytest.fixture(scope=""module"")
def event_loop(request):
    loop = asyncio.get_event_loop_policy().get_event_loop()
    yield loop
    loop.close()

@pytest_asyncio.fixture(scope=""module"")
async def foo():
    yield ""a value""
```
But this error is still there.


",gemini-2.5-pro,gemini,5789,515,14169,20473,161,134,1.22,3492,FALSE,FALSE,0,0,0,0,25,56,81,N/A,N/A,N/A,N/A,TRUE,1,0,0,0,43,49,92,163,136,1.16,3514,2.023522138595581,118.1775414943695,N/A,34.71813154220581,154.9191951751709
2025-11-15T23:20:06.795172,python-poetry/poetry,6f31f56c3ecb3357d75cd58496e215a6a67e79bc,src/poetry/console/commands/remove.py,tests/console/commands/test_remove.py,1,0,3,1,5,5,12,fix error of removing packages if nested groups (include-group) are used in pep735 style (#10587),`poetry remove` crashes when removing a package from a legacy dependency group (PEP 735) containing `include-group`,"### Description

When attempting to remove a package from a dependency group that is defined using the PEP 735 format, the `poetry remove` command crashes and throws `AttributeError: 'InlineTable' object has no attribute 'split'` if nested group is used (the groups's dependency list contains { include-group = ""...""} ), which is supported for Poetry 2.2.1: [Including dependencies from other groups](https://python-poetry.org/docs/managing-dependencies/#including-dependencies-from-other-groups). 

### Analysis
The root cause appears to be in the implementation of the remove command. When processing a dependency group defined under the legacy [dependency-groups] key, the code iterates through a list that can contain mixed types (package strings and include-group tables). The command's logic does not filter out the non-string table objects before passing them to Dependency.create_from_pep_508, which expects a string.

### Workarounds

May use the poetry style of nested poetry group:
```toml
[tool.poetry.group.test.dependencies]
pytest = ""^8.0.0""

[tool.poetry.group.lint.dependencies]
ruff = ""^0.11""

[tool.poetry.group.dev]
include-groups = [
    ""test"",
    ""lint"",
]
```

### Poetry Installation Method

pipx

### Operating System

MacOS Tahoe 26.0.1

### Poetry Version

Poetry (version 2.2.1)

### Poetry Configuration

```bash session
cache-dir = ""/Users/wells/Library/Caches/pypoetry""
data-dir = ""/Users/wells/Library/Application Support/pypoetry""
installer.max-workers = null
installer.no-binary = null
installer.only-binary = null
installer.parallel = true
installer.re-resolve = true
keyring.enabled = true
python.installation-dir = ""{data-dir}/python""  # /Users/wells/Library/Application Support/pypoetry/python
requests.max-retries = 0
solver.lazy-wheel = true
system-git-client = false
virtualenvs.create = true
virtualenvs.in-project = null
virtualenvs.options.always-copy = false
virtualenvs.options.no-pip = false
virtualenvs.options.system-site-packages = false
virtualenvs.path = ""{cache-dir}/virtualenvs""  # /Users/wells/Library/Caches/pypoetry/virtualenvs
virtualenvs.prompt = ""{project_name}-py{python_version}""
virtualenvs.use-poetry-python = false
```

### Python Sysconfig

<details>
  <summary>sysconfig.log</summary>
  <!-- Please leave one blank line below for enabling the code block rendering. -->

  ```
  Paste the output of 'python -m sysconfig', over this line.
  ```
</details>


### Example pyproject.toml

```TOML
[project]
name = ""xx""
version = ""0.1.1""
description = ""This is the recommendation engine.""
requires-python = "">=3.12""
dynamic = [ ""readme"" ]
dependencies = [
    ""redis (>=6.4.0,<7.0.0)"",
    ""fastapi (>=0.119.0,<0.120.0)""
]

[dependency-groups]
lint = [
    ""mypy"",
]
test = [
    ""pytest"",
]
dev = [
    { include-group = ""lint"" }, 
    { include-group = ""test"" },
    ""requests""
]

[build-system]
requires = [""poetry-core>=2.0.0,<3.0.0""]
build-backend = ""poetry.core.masonry.api""

[tool.mypy]
ignore_missing_imports = true
disallow_untyped_defs = true
check_untyped_defs = true
strict_equality = true
enable_error_code='explicit-override'

[tool.poetry]
package-mode = false
readme = [""docs/README.rst""]
```

### Poetry Runtime Logs

<details>
  <summary>poetry-runtime.log</summary>
  <!-- Please leave one blank line below for enabling the code block rendering. -->

  ```
Stack trace:

  10  ~/.local/pipx/venvs/poetry/lib/python3.14/site-packages/cleo/application.py:327 in run
       325│ 
       326│             try:
     → 327│                 exit_code = self._run(io)
       328│             except BrokenPipeError:
       329│                 # If we are piped to another process, it may close early and send a

   9  ~/.local/pipx/venvs/poetry/lib/python3.14/site-packages/poetry/console/application.py:260 in _run
       258│ 
       259│             try:
     → 260│                 exit_code = super()._run(io)
       261│             except PoetryRuntimeError as e:
       262│                 io.write_error_line("""")

   8  ~/.local/pipx/venvs/poetry/lib/python3.14/site-packages/cleo/application.py:431 in _run
       429│             io.input.interactive(interactive)
       430│ 
     → 431│         exit_code = self._run_command(command, io)
       432│         self._running_command = None
       433│ 

   7  ~/.local/pipx/venvs/poetry/lib/python3.14/site-packages/cleo/application.py:473 in _run_command
       471│ 
       472│         if error is not None:
     → 473│             raise error
       474│ 
       475│         return terminate_event.exit_code

   6  ~/.local/pipx/venvs/poetry/lib/python3.14/site-packages/cleo/application.py:457 in _run_command
       455│ 
       456│             if command_event.command_should_run():
     → 457│                 exit_code = command.run(io)
       458│             else:
       459│                 exit_code = ConsoleCommandEvent.RETURN_CODE_DISABLED

   5  ~/.local/pipx/venvs/poetry/lib/python3.14/site-packages/cleo/commands/base_command.py:117 in run
       115│         io.input.validate()
       116│ 
     → 117│         return self.execute(io) or 0
       118│ 
       119│     def merge_application_definition(self, merge_args: bool = True) -> None:

   4  ~/.local/pipx/venvs/poetry/lib/python3.14/site-packages/poetry/console/commands/installer_command.py:39 in execute
        37│     def execute(self, io: IO) -> int:
        38│         PoetryKeyring.preflight_check(io, self.poetry.config)
     →  39│         return super().execute(io)
        40│ 

   3  ~/.local/pipx/venvs/poetry/lib/python3.14/site-packages/cleo/commands/command.py:61 in execute
        59│ 
        60│         try:
     →  61│             return self.handle()
        62│         except KeyboardInterrupt:
        63│             return 1

   2  ~/.local/pipx/venvs/poetry/lib/python3.14/site-packages/poetry/console/commands/remove.py:96 in handle
        94│ 
        95│             for group_name, standard_section, poetry_section in group_sections:
     →  96│                 removed |= self._remove_packages(
        97│                     packages=packages,
        98│                     standard_section=standard_section,

   1  ~/.local/pipx/venvs/poetry/lib/python3.14/site-packages/poetry/console/commands/remove.py:187 in _remove_packages
       185│             normalized_name = canonicalize_name(package)
       186│             for requirement in standard_section.copy():
     → 187│                 if Dependency.create_from_pep_508(requirement).name == normalized_name:
       188│                     standard_section.remove(requirement)
       189│                     removed.add(package)

  AttributeError

  'InlineTable' object has no attribute 'split'

  at ~/.local/pipx/venvs/poetry/lib/python3.14/site-packages/poetry/core/packages/dependency.py:363 in create_from_pep_508
      359│         from poetry.core.vcs.git import ParsedUrl
      360│         from poetry.core.version.requirements import parse_requirement
      361│ 
      362│         # Removing comments
    → 363│         parts = name.split("" #"", 1)
      364│         name = parts[0].strip()
      365│         if len(parts) > 1:
      366│             rest = parts[1]
      367│             if "" ;"" in rest:

  ```
</details>",gemini-2.5-pro,gemini,5829,491,3239,9559,37,59,3,807,FALSE,FALSE,0,0,0,0,27,25,52,N/A,N/A,N/A,N/A,TRUE,2711,0,0,0,6,2,8,38,63,3,827,12.111371278762817,27.798296451568604,N/A,62.609193325042725,102.51886105537415
2025-11-15T23:21:16.220722,python-poetry/poetry,d4fb6874693e7f85ef86399d93394f3e21a0fafa,src/poetry/console/commands/python/__init__.py; src/poetry/console/commands/python/install.py; src/poetry/console/commands/python/list.py; src/poetry/console/commands/python/remove.py; src/poetry/utils/env/python/installer.py; src/poetry/utils/env/python/manager.py; src/poetry/utils/env/python/providers.py; tests/types.py,tests/conftest.py; tests/console/commands/python/test_python_install.py; tests/console/commands/python/test_python_list.py; tests/console/commands/python/test_python_remove.py; tests/utils/env/python/test_manager.py; tests/utils/env/python/test_python_installer.py; tests/utils/env/python/test_python_providers.py,27,0,5,27,3,5,41,improve python management for free-threaded python (#10606),Add support for non pure python projects,We should have a way to build non pure-python projects (from instance if they need to build C extensions).,gemini-2.5-pro,gemini,6872,355,9603,16830,125,115,1.59,2426,FALSE,FALSE,0,0,0,0,12,12,24,N/A,N/A,N/A,N/A,TRUE,2747,0,0,0,78,18,96,134,124,1.72,2618,7.533684492111206,92.52778744697571,N/A,61.72424889,161.7857208251953
2025-11-15T23:22:24.264987,python-poetry/poetry,27253f2ac7912903051814dd9c3ab0dc563e892e,src/poetry/puzzle/solver.py,tests/puzzle/test_solver_internals.py,3,7,5,3,5,5,18,fix calculated markers if a dependency is required by several groups with different markers (#10613),fix calculated markers if a dependency is required by several groups with different markers,"This also fixes the case where a dependency is required in an extra and a group.

# Pull Request Check List

<!-- This is just a reminder about the most common mistakes. Please make sure that you tick all *appropriate* boxes.  But please read our [contribution guide](https://python-poetry.org/docs/contributing/) at least once, it will save you unnecessary review cycles! -->

- [x] Added **tests** for changed code.
- [ ] Updated **documentation** for changed code.

<!-- If you have *any* questions to *any* of the points above, just **submit and ask**!  This checklist is here to *help* you, not to deter you from contributing! -->

## Summary by Sourcery

Improve marker calculation to correctly handle dependencies required by multiple groups or extras by grouping edge-specific markers and updating both DFS and transitive marker aggregation

Bug Fixes:
- Fix incorrect marker propagation for dependencies shared across different groups or extras

Enhancements:
- Refactor internal marker storage to key edge markers by the set of dependency groups

Tests:
- Add tests for propagating markers when a dependency appears in multiple groups and in extras",gemini-2.5-pro,gemini,4113,726,19340,24179,105,146,2.27,2349,FALSE,FALSE,0,0,0,0,45,23,68,N/A,N/A,N/A,N/A,TRUE,2749,0,0,0,36,10,46,108,168,2.27,2404,6.057171106338501,171.63481783866882,N/A,61.85159087181091,239.54357981681824
2025-11-15T23:22:36.503185,theskumar/python-dotenv,8411987b9301f716245074872afa30646e9b9eb7,src/dotenv/main.py,tests/test_is_interactive.py,1,12,5,1,5,5,0,fix: ensure find_dotenv work reliably on python 3.13 (#563),load_dotenv fails to find .env file by default under python 3.13,"When using python 3.13, load_dotenv() requires an explicit path to a dotenv file in the current directory.

```
% cat .env
FOO=""bar""
```

```python
Python 3.12.9 (main, Mar 24 2025, 18:36:22) [Clang 16.0.0 (clang-1600.0.26.6)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import dotenv, dotenv.version
>>> dotenv.version.__version__
'1.0.1'
>>> dotenv.load_dotenv()
True

Python 3.12.9 (main, Mar 24 2025, 18:36:22) [Clang 16.0.0 (clang-1600.0.26.6)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import dotenv, dotenv.version
>>> dotenv.version.__version__
'1.1.0'
>>> dotenv.load_dotenv()
True

Python 3.13.3 (main, May 17 2025, 17:59:15) [Clang 17.0.0 (clang-1700.0.13.3)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import dotenv, dotenv.version
>>> dotenv.version.__version__
'1.0.1'
>>> dotenv.load_dotenv()
False
>>> dotenv.load_dotenv('.env')
True

Python 3.13.3 (main, May 17 2025, 17:59:15) [Clang 17.0.0 (clang-1700.0.13.3)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import dotenv, dotenv.version
>>> dotenv.version.__version__
'1.1.0'
>>> dotenv.load_dotenv()
False
>>> dotenv.load_dotenv('.env')
True
```",gemini-2.5-pro,gemini,4036,380,5440,9856,72,77,2.56,1675,FALSE,FALSE,0,0,0,0,12,7,19,N/A,N/A,N/A,N/A,TRUE,162,0,0,0,2,0,2,74,80,2.56,1692,3.0712087154388428,52.30174708366394,N/A,6.606460332870483,61.97941613197327
2025-11-15T23:22:44.290731,theskumar/python-dotenv,1ecb57dd7dd516d3ecedf7e58ba84520281fbd00,src/dotenv/main.py,tests/test_main.py,1,11,5,1,5,5,28,"Fix out of scope error when ""dest"" variable is undefined",Error Handling in rewrite is incorrect.,"Look at:
https://github.com/theskumar/python-dotenv/blob/master/src/dotenv/main.py#L136

If lines 136, 137 or 140 there are ever hit, an error ""local variable 'dest' referenced before assignment"" will be thrown, because the `dest` variable only exists within the scope of the `with` block above.",gemini-2.5-pro,gemini,3828,417,8789,13034,69,79,2.71,1664,FALSE,FALSE,0,0,0,0,18,17,35,N/A,N/A,N/A,N/A,FALSE,140,8,0,0,9,11,20,68,76,2.71,1652,2.574888229370117,65.91562533378601,N/A,5.12188983,73.61240339279175
2025-11-15T23:22:52.562126,theskumar/python-dotenv,c715d19fb88e81f04cf3506a3c2c2812621d1b46,src/dotenv/main.py,tests/test_main.py,1,12,5,1,5,5,29,feat:  add `PYTHON_DOTENV_DISABLED` flag to disable load_dotenv (fixes #510) (#569),How to disable python-dotenv,"Quite a lot of packages use this library. I have an .env file which I use for a different purpose and cannot be parsed by python-dotenv. I get a lot of messages like:

```
Python-dotenv could not parse statement starting at line 1
Python-dotenv could not parse statement starting at line 2
```

Is there an environmental variable that I can set that will disable python-dotenv?

I am not asking about a specific library as I've already encountered this error in flask, pipenv and nltk. Since this library deals with the environment already I think having an environmental variable to disable it is not unreasonable.",gemini-2.5-pro,gemini,3933,380,14059,18372,74,80,2.56,1692,FALSE,FALSE,0,0,0,0,21,15,36,N/A,N/A,N/A,N/A,TRUE,209,0,0,0,15,0,15,77,82,2.42,1748,2.5621447563171387,97.88659787,N/A,5.609556198120117,106.05829882621765
2025-11-15T23:24:56.939951,matplotlib/matplotlib,419eb3e265edd2c80412d26a4c650fad22b3f6d1,lib/matplotlib/_api/__init__.py,lib/matplotlib/tests/test_api.py,2,16,5,2,5,5,8,FIX: Gracefully handle numpy arrays as input to check_in_list() (#30714),"[Bug]: Axes.grouped_bar() with non-string orientation (e.g., NumPy array) raises ambiguous truth-value error instead of clean ValueError","### Bug summary

Passing a non-string value (like a NumPy array) to orientation in Axes.grouped_bar() triggers a misleading “ambiguous truth value” error from _api.check_in_list. The function should instead raise a clear ValueError stating the value is not valid.


### Code for reproduction

```Python
import numpy as np
import matplotlib.pyplot as plt

fig, ax = plt.subplots()
ax.grouped_bar([[1, 2, 3], [3, 2, 1]], orientation=np.array([1, 2, 3]))
```

### Actual outcome

Axes.grouped_bar() fails inside _api.check_in_list() because NumPy arrays cannot be evaluated in a boolean context.
Instead of raising a clear validation error, it produces a misleading NumPy truth-value error, causing the test to fail.

<img width=""1600"" height=""1172"" alt=""Image"" src=""https://github.com/user-attachments/assets/0a24242f-ae50-45ba-a15e-af44768f4333"" />

### Expected outcome

After applying the fix, invalid inputs (including NumPy arrays) are handled gracefully, and all tests expecting a clear ValueError now pass.



### Additional information


1. Conditions under which this bug happens:

The bug occurs whenever the orientation parameter passed to Axes.grouped_bar() is not a string, such as:
orientation = np.array([1, 2, 3])  # NumPy array  
orientation = 1                    # integer  
orientation = None                 # NoneType

These types trigger an internal truth-value evaluation inside _api.check_in_list, which expects a scalar comparable value (like a string).

2. Edge cases affected:

NumPy arrays (np.array([...]))
Non-string types (integers, floats, None, lists, etc.)
Any custom object that doesn’t implement __eq__ safely with strings

3. Behavior in earlier versions:

This issue has likely existed since the introduction of Axes.grouped_bar() (Matplotlib 3.11, provisional API).
Other similar Matplotlib APIs (e.g., barh, stem, etc.) already validate string-type enums, so they are not affected.

4. Root cause (why this happens):

The function directly calls:

_api.check_in_list([""vertical"", ""horizontal""], orientation=orientation)

When orientation is a NumPy array, the expression val not in values inside check_in_list() performs elementwise comparison, returning an array of booleans.

Python then attempts to interpret that array as a single truth value, which triggers NumPy’s error:
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().

5. Proposed fix (confirmed working):

Add an early type guard before calling _api.check_in_list():

if not isinstance(orientation, str):
    raise ValueError(f""{orientation!r} is not a valid value for orientation"")
_api.check_in_list([""vertical"", ""horizontal""], orientation=orientation)

-- This ensures non-string inputs are rejected immediately and consistently,
-- prevents the ambiguous truth-value error, and
-- aligns grouped_bar() with Matplotlib’s standard API validation behavior.

### Operating system

MacOS

### Matplotlib Version

3.11.0.dev1446+g319295e28.d20251030

### Matplotlib Backend

macosx

### Python version

Python 3.13.7

### Jupyter version

_No response_

### Installation

pip",gemini-2.5-pro,gemini,5313,1139,4896,11348,70,75,2.05,1143,FALSE,FALSE,0,0,0,0,74,301,375,N/A,N/A,N/A,N/A,FALSE,0,0,0,5,15,1,16,71,77,2.05,1154,89.78745508,42.096978425979614,N/A,3.199493646621704,135.08392715454102
2025-11-15T23:30:27.437195,astropy/astropy,4e5b79280683ad0a5adfb30dfa6a6366a90722e2,astropy/io/fits/scripts/fitsdiff.py,astropy/io/fits/tests/test_fitsdiff.py,2,4,5,2,5,5,19,BUG: warn instead of raise when failing to read specific files in fitsdiff script (#18882),BUG: a failing `io.fits` test,"
```
pytest astropy/io/fits/tests/test_fitsdiff.py::TestFITSDiff_script::test_path -ra --run-slow
```

<details><summary>Detailled output</summary>

```
astropy/io/fits/tests/test_fitsdiff.py F                                                                                                                                                                     [100%]

===================================================================================================== FAILURES =====================================================================================================
__________________________________________________________________________________________ TestFITSDiff_script.test_path ___________________________________________________________________________________________

self = <astropy.io.fits.diff.FITSDiff object at 0x1056d75c0>, a = '/Users/clm/dev/orgs/astropy-project/coordinated/astropy/astropy/io/fits/tests/data/lzw.fits.Z'
b = '/Users/clm/dev/orgs/astropy-project/coordinated/astropy/astropy/io/fits/tests/data/lzw.fits.Z', ignore_hdus = [], ignore_keywords = [], ignore_comments = [], ignore_fields = [], numdiffs = 10, rtol = 0.0
atol = 0.0, ignore_blanks = True, ignore_blank_cards = True

    def __init__(
        self,
        a,
        b,
        ignore_hdus=[],
        ignore_keywords=[],
        ignore_comments=[],
        ignore_fields=[],
        numdiffs=10,
        rtol=0.0,
        atol=0.0,
        ignore_blanks=True,
        ignore_blank_cards=True,
    ):
        """"""
        Parameters
        ----------
        a : str or `HDUList`
            The filename of a FITS file on disk, or an `HDUList` object.

        b : str or `HDUList`
            The filename of a FITS file on disk, or an `HDUList` object to
            compare to the first file.

        ignore_hdus : sequence, optional
            HDU names to ignore when comparing two FITS files or HDU lists; the
            presence of these HDUs and their contents are ignored.  Wildcard
            strings may also be included in the list.

        ignore_keywords : sequence, optional
            Header keywords to ignore when comparing two headers; the presence
            of these keywords and their values are ignored.  Wildcard strings
            may also be included in the list.

        ignore_comments : sequence, optional
            A list of header keywords whose comments should be ignored in the
            comparison.  May contain wildcard strings as with ignore_keywords.

        ignore_fields : sequence, optional
            The (case-insensitive) names of any table columns to ignore if any
            table data is to be compared.

        numdiffs : int, optional
            The number of pixel/table values to output when reporting HDU data
            differences.  Though the count of differences is the same either
            way, this allows controlling the number of different values that
            are kept in memory or output.  If a negative value is given, then
            numdiffs is treated as unlimited (default: 10).

        rtol : float, optional
            The relative difference to allow when comparing two float values
            either in header values, image arrays, or table columns
            (default: 0.0). Values which satisfy the expression

            .. math::

                \\left| a - b \\right| > \\text{atol} + \\text{rtol} \\cdot \\left| b \\right|

            are considered to be different.
            The underlying function used for comparison is `numpy.allclose`.

            .. versionadded:: 2.0

        atol : float, optional
            The allowed absolute difference. See also ``rtol`` parameter.

            .. versionadded:: 2.0

        ignore_blanks : bool, optional
            Ignore extra whitespace at the end of string values either in
            headers or data. Extra leading whitespace is not ignored
            (default: True).

        ignore_blank_cards : bool, optional
            Ignore all cards that are blank, i.e. they only contain
            whitespace (default: True).
        """"""
        if isinstance(a, (str, os.PathLike)):
            try:
>               a = fitsopen(a)
                    ^^^^^^^^^^^

astropy/io/fits/diff.py:293:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
astropy/io/fits/hdu/hdulist.py:227: in fitsopen
    return HDUList.fromfile(
astropy/io/fits/hdu/hdulist.py:491: in fromfile
    return cls._readfrom(
astropy/io/fits/hdu/hdulist.py:1174: in _readfrom
    fileobj = _File(
astropy/io/fits/file.py:240: in __init__
    self._open_filename(fileobj, mode, overwrite)
astropy/io/fits/file.py:700: in _open_filename
    if not self._try_read_compressed(self.name, magic, mode, ext=ext):
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <astropy.io.fits.file._File None>, obj_or_name = '/Users/clm/dev/orgs/astropy-project/coordinated/astropy/astropy/io/fits/tests/data/lzw.fits.Z', magic = b'\x1f\x9d\x90S\x924', mode = 'readonly'
ext = '.Z'

    def _try_read_compressed(self, obj_or_name, magic, mode, ext=""""):
        """"""Attempt to determine if the given file is compressed.""""""
        is_ostream = mode == ""ostream""
        if (is_ostream and ext == "".gz"") or magic.startswith(GZIP_MAGIC):
            if mode == ""append"":
                raise OSError(
                    ""'append' mode is not supported with gzip files.""
                    ""Use 'update' mode instead""
                )
            # Handle gzip files
            kwargs = {""mode"": IO_FITS_MODES[mode]}
            if isinstance(obj_or_name, str):
                kwargs[""filename""] = obj_or_name
            else:
                kwargs[""fileobj""] = obj_or_name
            self._file = gzip.GzipFile(**kwargs)
            self.compression = ""gzip""
        elif (is_ostream and ext == "".zip"") or magic.startswith(PKZIP_MAGIC):
            # Handle zip files
            self._open_zipfile(self.name, mode)
            self.compression = ""zip""
        elif (is_ostream and ext == "".bz2"") or magic.startswith(BZIP2_MAGIC):
            # Handle bzip2 files
            if mode in [""update"", ""append""]:
                raise OSError(
                    ""update and append modes are not supported with bzip2 files""
                )
            if not HAS_BZ2:
                raise ModuleNotFoundError(
                    ""This Python installation does not provide the bz2 module.""
                )
            # bzip2 only supports 'w' and 'r' modes
            bzip2_mode = ""w"" if is_ostream else ""r""
            self._file = bz2.BZ2File(obj_or_name, mode=bzip2_mode)
            self.compression = ""bzip2""
        elif (is_ostream and ext == "".xz"") or magic.startswith(LZMA_MAGIC):
            # Handle lzma files
            if mode in [""update"", ""append""]:
                raise OSError(
                    ""update and append modes are not supported with lzma files""
                )
            if not HAS_LZMA:
                raise ModuleNotFoundError(
                    ""This Python installation does not provide the lzma module.""
                )
            lzma_mode = ""w"" if is_ostream else ""r""
            self._file = lzma.LZMAFile(obj_or_name, mode=lzma_mode)
            self.compression = ""lzma""
        elif (is_ostream and ext == "".Z"") or magic.startswith(LZW_MAGIC):
            # Handle LZW files
            if mode in [""update"", ""append"", ""ostream""]:
                raise OSError(f""{mode} mode not supported with LZW files"")
            if not HAS_UNCOMPRESSPY:
>               raise ModuleNotFoundError(
                    ""The optional package uncompresspy is necessary for reading""
                    "" LZW compressed files (.Z extension).""
                )
E               ModuleNotFoundError: The optional package uncompresspy is necessary for reading LZW compressed files (.Z extension).

astropy/io/fits/file.py:604: ModuleNotFoundError

The above exception was the direct cause of the following exception:

self = <astropy.io.fits.tests.test_fitsdiff.TestFITSDiff_script object at 0x105655790>, capsys = <_pytest.capture.CaptureFixture object at 0x1056dcc20>

    @pytest.mark.slow
    def test_path(self, capsys):
        os.mkdir(self.temp(""sub/""))
        tmp_b = self.temp(""sub/ascii.fits"")

        tmp_g = self.temp(""sub/group.fits"")
        tmp_h = self.data(""group.fits"")
        with hdulist.fitsopen(tmp_h) as hdu_b:
            hdu_b.writeto(tmp_g)

        writeto(tmp_b, np.arange(100).reshape(10, 10))

        # one modified file and a directory
        assert fitsdiff.main([""-q"", self.data_dir, tmp_b]) == 1
        assert fitsdiff.main([""-q"", tmp_b, self.data_dir]) == 1

        # two directories
        tmp_d = self.temp(""sub/"")
        assert fitsdiff.main([""-q"", self.data_dir, tmp_d]) == 1
        assert fitsdiff.main([""-q"", tmp_d, self.data_dir]) == 1
>       assert fitsdiff.main([""-q"", self.data_dir, self.data_dir]) == 0
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

astropy/io/fits/tests/test_fitsdiff.py:259:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
astropy/io/fits/scripts/fitsdiff.py:401: in main
    diff = fits.diff.FITSDiff(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <astropy.io.fits.diff.FITSDiff object at 0x1056d75c0>, a = '/Users/clm/dev/orgs/astropy-project/coordinated/astropy/astropy/io/fits/tests/data/lzw.fits.Z'
b = '/Users/clm/dev/orgs/astropy-project/coordinated/astropy/astropy/io/fits/tests/data/lzw.fits.Z', ignore_hdus = [], ignore_keywords = [], ignore_comments = [], ignore_fields = [], numdiffs = 10, rtol = 0.0
atol = 0.0, ignore_blanks = True, ignore_blank_cards = True

    def __init__(
        self,
        a,
        b,
        ignore_hdus=[],
        ignore_keywords=[],
        ignore_comments=[],
        ignore_fields=[],
        numdiffs=10,
        rtol=0.0,
        atol=0.0,
        ignore_blanks=True,
        ignore_blank_cards=True,
    ):
        """"""
        Parameters
        ----------
        a : str or `HDUList`
            The filename of a FITS file on disk, or an `HDUList` object.

        b : str or `HDUList`
            The filename of a FITS file on disk, or an `HDUList` object to
            compare to the first file.

        ignore_hdus : sequence, optional
            HDU names to ignore when comparing two FITS files or HDU lists; the
            presence of these HDUs and their contents are ignored.  Wildcard
            strings may also be included in the list.

        ignore_keywords : sequence, optional
            Header keywords to ignore when comparing two headers; the presence
            of these keywords and their values are ignored.  Wildcard strings
            may also be included in the list.

        ignore_comments : sequence, optional
            A list of header keywords whose comments should be ignored in the
            comparison.  May contain wildcard strings as with ignore_keywords.

        ignore_fields : sequence, optional
            The (case-insensitive) names of any table columns to ignore if any
            table data is to be compared.

        numdiffs : int, optional
            The number of pixel/table values to output when reporting HDU data
            differences.  Though the count of differences is the same either
            way, this allows controlling the number of different values that
            are kept in memory or output.  If a negative value is given, then
            numdiffs is treated as unlimited (default: 10).

        rtol : float, optional
            The relative difference to allow when comparing two float values
            either in header values, image arrays, or table columns
            (default: 0.0). Values which satisfy the expression

            .. math::

                \\left| a - b \\right| > \\text{atol} + \\text{rtol} \\cdot \\left| b \\right|

            are considered to be different.
            The underlying function used for comparison is `numpy.allclose`.

            .. versionadded:: 2.0

        atol : float, optional
            The allowed absolute difference. See also ``rtol`` parameter.

            .. versionadded:: 2.0

        ignore_blanks : bool, optional
            Ignore extra whitespace at the end of string values either in
            headers or data. Extra leading whitespace is not ignored
            (default: True).

        ignore_blank_cards : bool, optional
            Ignore all cards that are blank, i.e. they only contain
            whitespace (default: True).
        """"""
        if isinstance(a, (str, os.PathLike)):
            try:
                a = fitsopen(a)
            except Exception as exc:
>               raise OSError(f""error opening file a ({a})"") from exc
E               OSError: error opening file a (/Users/clm/dev/orgs/astropy-project/coordinated/astropy/astropy/io/fits/tests/data/lzw.fits.Z)

astropy/io/fits/diff.py:295: OSError
```

</details>


bisected to 92b38aebbba6422890f1023031558f6df738e2a5 (#17960), which was merged more than 6 months ago.
While the test is rarely run, because it's marked as `@pytest.mark.slow`, I don't understand why I'm only seeing it in logs *now*.
",gemini-2.5-pro,gemini,6901,1067,11514,19482,46,46,2.38,1537,FALSE,FALSE,0,0,0,0,80,67,147,N/A,N/A,N/A,N/A,TRUE,31328,0,0,0,23,14,37,49,57,2.38,1581,75.33295059204102,108.30206608772278,N/A,201.92781519889832,385.5628318786621
2025-11-15T23:34:21.649366,astropy/astropy,4934c0a0cb2ed2e9b608650ee3ce4840f63591c7,astropy/table/_dataframes.py; astropy/table/table.py,astropy/table/tests/test_df.py,7,13,5,7,6,5,14,Fix Table class methods `from_pandas/from_df()` to return correct class (#18911),"QTable.from_pandas returns instance of Table, rather than QTable","### Description

The QTable.from_pandas method  is returning an instance of Table in astropy version 7.2.0rc1 (and dev version) and an instance of QTable in 7.1.1.

The example in the docstring of the from_pandas method in table.py demonstrates the effect.  This is causing problems in aiapy.calibrate.util.get_pointing_table because it now returns a Table, rather than a QTable, and indexed values are not Quantities

### Expected behavior

QTable.from_pandas returns an instance of QTable.

### How to Reproduce

Slightly modified version of docstring example

```python
import numpy as np
import pandas as pd
from astropy.table import QTable
import astropy.units as u

time = pd.Series(['1998-01-01', '2002-01-01'], dtype='datetime64[ns]')
dt = pd.Series(np.array([1, 300], dtype='timedelta64[s]'))
df = pd.DataFrame({'time': time})
df['dt'] = dt
df['x'] = [3., 4.]
with pd.option_context('display.max_columns', 20):
    print(df)

qtable = QTable.from_pandas(df)
qtable['x'].unit = u.meter
print(type(qtable))
qtable['x'][0]
```

Version 7.2.0rc1 returns:
```
        time              dt    x
0 1998-01-01 0 days 00:00:01  3.0
1 2002-01-01 0 days 00:05:00  4.0
<class 'astropy.table.table.Table'>
3.0
```
Version 7.1.1 returns:
```
  time              dt    x
0 1998-01-01 0 days 00:00:01  3.0
1 2002-01-01 0 days 00:05:00  4.0
<class 'astropy.table.table.QTable'>
3 m
```

### Versions

```python
import astropy
astropy.system_info()
```
```
PyDev console: using IPython 9.7.0
Python 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:13:34) [MSC v.1944 64 bit (AMD64)] on win32
import astropy
astropy.system_info()
platform
--------
platform.platform() = 'Windows-11-10.0.26200-SP0'
platform.version() = '10.0.26200'
platform.python_version() = '3.12.12'
packages
--------
astropy              7.2.0rc1
numpy                1.26.4
scipy                1.16.3
matplotlib           3.10.7
pandas               2.3.3
pyerfa               2.0.1.5
```
",FAILED,FAILED,0,0,0,0,746,692,2.68,13434,FALSE,FALSE,0,0,0,0,0,0,0,INTENT_PARSE_FAILED,INTENT_PARSE_FAILED,INTENT_PARSE_FAILED,INTENT_PARSE_FAILED,TRUE,31329,1,0,0,15,12,27,746,692,2.69,13444,71.97874999046326,0,0,161.83503699302673,233.813787
2025-11-15T23:38:02.704466,astropy/astropy,d3cfb63b923d17bfe93e992ccfbc9d4f9aa80215,astropy/units/quantity_helper/function_helpers.py,astropy/units/tests/test_quantity_non_ufuncs.py,1,98,5,1,5,5,322,"BUG: fix forward compatibility for `np.arange(<Quantity>, ...)` against numpy 2.4 (dev) (#18849)","BUG: fix forward compatibility for `np.arange(<Quantity>, ...)` against numpy 2.4 (dev)","### Description

xref: https://github.com/numpy/numpy/pull/30147

This one was unusually hard to pull of, because `np.arange`, mimicking the builtin `range`, has a very peculiar signature, where the meaning of the first positional argument depends on the rest of the arguments. A full refactor of the wrapper function ended up being necessary to make this forward compatible. Hopefully inline comments should help understanding why.

Because it's taken so long to wrap things up (I've been playing whac-a-mole on-and-off on this for 2 days), I haven't yet collapsed my commits. I'd like to take the current state for a spin on CI first.

<!-- Optional opt-out -->

- [ ] By checking this box, the PR author has requested that maintainers do **NOT** use the ""Squash and Merge"" button. Maintainers should respect this when possible; however, the final decision is at the discretion of the maintainer that merges the PR.
",gemini-2.5-pro,gemini,6040,1523,16477,24040,280,202,3.56,7291,FALSE,FALSE,0,0,0,0,95,48,143,N/A,N/A,N/A,N/A,TRUE,31326,0,0,0,99,52,151,281,203,3.53,7368,70.01312375068665,144.95385336875916,N/A,150.7721905708313,365.7391676902771
2025-11-15T23:40:25.464150,mwaskom/seaborn,5023f2ee885a45200f5b63156a158ddf7272c29e,seaborn/_compat.py; seaborn/categorical.py,tests/test_base.py; tests/test_categorical.py,5,21,5,5,10,5,302,Address some upcoming deprecations (#3820),PendingDeprecationWarning: vert: bool will be deprecated in a future version with box plot,"Use of `boxplot` is producing the following warning when combined with `matplotlib==3.10.0`

```
PendingDeprecationWarning: vert: bool will be deprecated in a future version. Use orientation: {'vertical', 'horizontal'} instead.
```",gemini-2.5-pro,gemini,5436,1482,7044,13962,437,471,6.21,15428,FALSE,FALSE,0,0,0,0,98,155,253,N/A,N/A,N/A,N/A,TRUE,2363,0,0,0,16,2,18,440,472,6.13,15483,2.072216510772705,68.99442244,N/A,130.09279918670654,201.15943813323975
2025-11-15T23:42:30.131933,mwaskom/seaborn,2c115ed7626a236c772ee8a4eacdfa300c570e68,seaborn/categorical.py,tests/test_categorical.py,4,13,5,4,5,5,232,"Avoid error when dodge=True, hue=None (#3605)",Boxplot errors when `dodge=True` without `hue`,"Hello,

I want to report a bug when using the latest version of Seaborn. I checked with 0.12.2 there was no error.The bug is:
  File ""/mnt/c/Users/*****/Desktop/Gent_presentation_2023/1_Effect_of_Ti_addition/2-formation_energy/all_Tix_analysis/./boxplot.py"", line 287, in <module>
    statplot_cols()
  File ""/mnt/c/Users/******/Desktop/Gent_presentation_2023/1_Effect_of_Ti_addition/2-formation_energy/all_Tix_analysis/./boxplot.py"", line 59, in statplot_cols
    pp = sns.boxplot(
  File ""/home/******/miniconda3/lib/python3.10/site-packages/seaborn/categorical.py"", line 1619, in boxplot
    p.plot_boxes(
  File ""/home/******/miniconda3/lib/python3.10/site-packages/seaborn/categorical.py"", line 637, in plot_boxes
    self._dodge(sub_vars, data)
  File ""/home/******/miniconda3/lib/python3.10/site-packages/seaborn/categorical.py"", line 391, in _dodge
    hue_idx = self._hue_map.levels.index(keys[""hue""])
KeyError: 'hue'

Can you check why is that?
",gemini-2.5-pro,gemini,5058,372,4551,9981,407,449,6.96,14864,FALSE,FALSE,0,0,0,0,11,9,20,N/A,N/A,N/A,N/A,TRUE,2197,1,0,0,5,0,5,408,450,6.96,14873,1.524404764175415,43.45633506774902,N/A,122.80918860435486,167.7899284362793
2025-11-15T23:44:29.587769,mwaskom/seaborn,45a098fd254b6b7385e708b29a556017808ecd40,seaborn/_core/plot.py,tests/_core/test_plot.py,8,4,5,8,5,5,179,FIX: Enable xticklabels for all bottom axes (#3600),seaborn.objects: facet doesn't show x tick labels for the bottom subplots in each column if the corresponding last row is empty,"See the title. In the example below, I would expect the subplots H and I show the x tick labels, just like subplot J. Thank you. 

```python
import seaborn as sns
import seaborn.objects as so

diamonds = sns.load_dataset(""diamonds"")

p = so.Plot(diamonds, x=""carat"", y=""price"").add(so.Dots())
p.facet(""color"", wrap=3)
```
![test](https://github.com/mwaskom/seaborn/assets/47764802/2684b871-4ee7-4ff2-af47-1e186881c647)

",gemini-2.5-pro,gemini,4601,1785,14800,21186,306,385,2.56,7323,FALSE,FALSE,0,0,0,0,126,99,225,N/A,N/A,N/A,N/A,TRUE,2195,3,0,0,2,0,2,306,385,2.56,7349,1.5297117233276367,140.03667783737183,N/A,117.71121597290039,259.27760553359985
2025-11-15T23:46:20.286144,scikit-learn/scikit-learn,b4238b237b8767f6e39ba0a10ced0651341041ef,sklearn/utils/_array_api.py,sklearn/utils/tests/test_array_api.py,0,44,5,0,5,2,30,Return `'cpu'` for `device(numpy_array)` when dispatch is enabled (#32705),Return `'cpu'` for `device(numpy_array)` when dispatch is enabled,"This PR will check using the CI whether we can change the current behavior for `device` to return `""cpu""` on numpy inputs when `array_api_dispatch is True`.

Context: https://github.com/scikit-learn/scikit-learn/pull/31829#issuecomment-3523758213",gemini-2.5-pro,gemini,5781,349,3259,9389,160,146,2.75,4189,FALSE,FALSE,0,0,0,0,13,15,28,N/A,N/A,N/A,N/A,FALSE,0,0,0,0,1,2,3,159,146,2.75,4174,92.01454067230225,26.29410433769226,N/A,1.0832126140594482,119.39185762405396
2025-11-15T23:47:46.035375,scikit-learn/scikit-learn,62b6acde1b5fefb9f7c09b8cf066b0df885896b0,sklearn/covariance/_elliptic_envelope.py; sklearn/covariance/_robust_covariance.py,sklearn/covariance/tests/test_robust_covariance.py,2,4,5,2,10,1,7,FIX: Reduce bias of  `covariance.MinCovDet` with consistency correction (#32117),MinCovDet estimation of covariance with strong bias?,"### Discussed in https://github.com/scikit-learn/scikit-learn/discussions/23161

<div type='discussions-op-text'>

<sup>Originally posted by **hongfei0224** April 20, 2022</sup>
I was playing with MinCovDet (Minimum Covariance Determinant) in sklearn:
[https://scikit-learn.org/stable/modules/generated/sklearn.covariance.MinCovDet.html#sklearn.covariance.MinCovDet](url)

I see the idea of MinCovDet is to make the covariance matrix estimation robust when there is outlier in the data.
To understand the behavior, I tried the multivariate normal data **without** outliers.

What I did:
- generate multivariate_normal samples with given covariance matrix (the true covariance matrix)
- use `np.cov` to estimate empirical covariance matrix
- use `MinCovDet` to estimate robust covariance matrix
- repeat above with same ""true covariance matrix""
- check the distribution of empirical covariance matrix and robust covariance matrix

I was expecting both `np.cov` and `MinCovDet` estimation should be centered around the true covariance matrix, since there is no outlier in data.

but seems not the case.

![image](https://user-images.githubusercontent.com/51205365/164061364-1daa7614-5178-46ad-b3be-0ff2e3c38a42.png)
![image](https://user-images.githubusercontent.com/51205365/164061425-00155926-f523-45a3-b872-ed83660aba5e.png)

the code to replicate results

```python
import pandas as pd
import numpy as np
from sklearn.covariance import EmpiricalCovariance, MinCovDet
import matplotlib.pyplot as plt
from tqdm.autonotebook import tqdm

res = []
res2= []
a = 2
b = 5
covar = 3 

for i in tqdm(range(1000)):
    x=np.random.multivariate_normal(mean=[0,0],cov=[[a,covar],[covar,b]], size=250).T
    res.append(np.cov(x))
    mcd = MinCovDet().fit(x.T)
    res2.append(mcd.covariance_)

pd.Series([xx[0][0] for xx in res]).hist(bins=30, label='empirical',alpha=0.7)
pd.Series([xx[0][0] for xx in res2]).hist(bins=30, label='cvd',alpha=0.7)
plt.axvline(a, ls='--', color='black')
plt.title('variance')
plt.legend()
plt.show()

pd.Series([xx[0][1] for xx in res]).hist(bins=30, label='empirical',alpha=0.7)
pd.Series([xx[0][1] for xx in res2]).hist(bins=30, label='cvd',alpha=0.7)
plt.axvline(covar, ls='--', color='black')
plt.title('covariance')
plt.legend()
plt.show()
```",gemini-2.5-pro,gemini,4602,944,16454,22000,45,39,4.07,2395,FALSE,FALSE,0,0,0,0,64,52,116,N/A,N/A,N/A,N/A,FALSE,0,0,0,0,65,17,82,46,39,3.93,2443,84.96871328353882,147.89098286628723,N/A,0.5781967639923096,233.43789291381836
2025-11-15T23:49:09.695938,scikit-learn/scikit-learn,d5396757a97c622908f2649a0c01ecda986a353d,sklearn/linear_model/_logistic.py,sklearn/linear_model/tests/test_logistic.py,2,4,5,2,5,5,72,Raise error logreg liblinear (#31888),LogisticRegression freezed when X containing large number,"The following code will freeze

``` python
X = np.array([0, 1e+100]).reshape(-1, 1)
y = np.array([0, 1])
clf = LogisticRegression(solver='liblinear').fit(X, y)
```

It seems to be a liblinear problem, the while loop [here](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/svm/src/liblinear/tron.cpp#L170) is not breaking.

Maybe a warning could be shown suggesting user to standardize data...
",gemini-2.5-pro,gemini,4267,5351,15187,24805,159,190,7,5597,FALSE,FALSE,0,0,0,0,404,498,902,N/A,N/A,N/A,N/A,FALSE,0,0,0,0,6,0,6,160,192,7,5614,82.85414123535156,172.51645684242249,N/A,0.57461071,255.9452087879181
2025-11-15T23:49:25.428322,pylint-dev/pylint,dd1275875c157bc594420cb995bd0de4a577a7e9,pylint/config/config_initialization.py; pylint/lint/parallel.py; pylint/lint/pylinter.py; pylint/reporters/reports_handler_mix_in.py,tests/test_check_parallel.py,3,8,5,3,15,4,16,Fix two multiprocessing issues (#10642),`consider-using-augmented-assign` doesn't work with `--jobs` > 1,"### Bug description

Suppose I create 2 files with the same content:
```
from pathlib import Path

A = Path(""."")

A = A / ""test""
```

If I run `pylint --load-plugins pylint.extensions.code_style -j 1 --enable R --disable C x.py y.py`, I get some R6104 lints as desired:
```
% pylint --load-plugins pylint.extensions.code_style -j 1 --enable R --disable C x.py y.py
************* Module x
x.py:5:0: R6104: Use '/=' to do an augmented assign directly (consider-using-augmented-assign)
************* Module y
y.py:5:0: R6104: Use '/=' to do an augmented assign directly (consider-using-augmented-assign)

-------------------------------------------------------------------
Your code has been rated at 6.67/10 (previous run: 10.00/10, -3.33)
```

But, if I pick a `--jobs` value more than 1, then I get no lints (of this type) at all:

```
% pylint --load-plugins pylint.extensions.code_style -j 2 --enable R --disable C x.py y.py

--------------------------------------------------------------------
Your code has been rated at 10.00/10 (previous run: 10.00/10, +0.00)
```

I've only noticed this for this check, but I can't say for sure that no other checks are impacted by this.

### Configuration

(default)

### Command used

pylint --load-plugins pylint.extensions.code_style -j 2 --enable R --disable C x.py y.py

### Pylint output

% pylint --load-plugins pylint.extensions.code_style -j 2 --enable R --disable C x.py y.py

--------------------------------------------------------------------
Your code has been rated at 10.00/10 (previous run: 10.00/10, +0.00)

### Expected behavior

************* Module x
x.py:5:0: R6104: Use '/=' to do an augmented assign directly (consider-using-augmented-assign)
************* Module y
y.py:5:0: R6104: Use '/=' to do an augmented assign directly (consider-using-augmented-assign)

-------------------------------------------------------------------
Your code has been rated at 6.67/10 (previous run: 10.00/10, -3.33)

### Pylint version

pylint 3.3.1
astroid 3.3.5
Python 3.11.10

### OS / Environment

Debian 11

### Additional dependencies

_No response_",gemini-2.5-pro,gemini,6097,495,6653,13245,253,282,2.43,6239,FALSE,FALSE,0,0,0,0,20,19,39,N/A,N/A,N/A,N/A,FALSE,0,0,0,0,43,3,46,260,288,2.42,6446,3.073721408843994,63.055519342422485,N/A,2.5266799926757812,68.65592074394226
2025-11-15T23:51:54.687832,sphinx-doc/sphinx,245dea0911d526aae41321230c4d6194805218d7,sphinx/util/inspect.py,tests/test_util/test_util_inspect.py,8,46,5,8,5,2,50,Fix ``autodoc_type_aliases`` when appearing in PEP 604 unions (``X | Y``) (#14007),Failure to resolve signature when TypeAliasForwardRef appears in union expressions,"### Describe the bug

When `TypeAliasForwardRef` appears in expressions like `Alias | None`, signature evaluation fails with TypeError: unsupported operand type(s) for |: 'type' and 'TypeAliasForwardRef'.

Adding `__or__` and `__ror__` to `TypeAliasForwardRef` would fix this, I'll send a patch later today if it's ok (I've heard that you're working on rewrite around autodoc, but this is a small change.)

### How to Reproduce

Example: when autodoc renders function `test` from this code:

```python
from __future__ import annotations
from typing import *

Alias: TypeAlias = int

def test() -> Alias | None: ...
```

if Alias appears in `autodoc_type_aliases`, the resulting signature will not evaluate, leading to further errors down the line.

### Environment Information

```text
Platform:              linux; (Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39)
Python version:        3.13.8 (main, Oct 12 2025, 16:57:39) [GCC 13.3.0])
Python implementation: CPython
Sphinx version:        8.2.3
Docutils version:      0.21.2
Jinja2 version:        3.1.6
Pygments version:      2.19.2
```

### Sphinx extensions

```python

```

### Additional context

_No response_",gemini-2.5-pro,gemini,6007,274,4519,10800,262,257,1.81,4481,TRUE,TRUE,2368,1,0,0,8,0,8,264,257,1.81,4521,TRUE,2369,0,9,0,8,0,8,264,257,1.81,4521,1.559861421585083,43.77686810493469,78.49075913,58.073408126831055,181.90089678764343
2025-11-15T23:52:55.084592,sphinx-doc/sphinx,14717292b096f4107ef8e3781e410ab24bd89bc7,sphinx/ext/doctest.py,tests/test_extensions/test_ext_doctest.py,10,6,5,10,5,5,7,Use config value as default doctest group name (#13859),sphinx.ext.doctest config variable `doctest_test_doctest_blocks` is not respected,"### Describe the bug

The documentation for `doctest_test_doctest_blocks` claims that ```If this is a nonempty string, standard reStructuredText doctest blocks will be tested too. They will be assigned to the group name given.```.

It does not. Regardless of the configuration value, the doctest blocks are assigned to the `default` group.

### How to Reproduce


index.rst
``` 
bug-demo documentation
======================

.. testsetup:: Custom

   from module import *

.. autoclass:: module.MyClass

```

conf.py
```
import sys
sys.path.append(""."")

extensions = [
    ""sphinx.ext.autodoc"",
    ""sphinx.ext.doctest"",
]

doctest_test_doctest_blocks = ""Custom""
```

module.py
```

class MyClass:
    """"""
    MyClass docstring

    Examples
    --------
    >>> instance = MyClass(""value"")

    """"""
    def __init__(self, value: str):
        self._value = value
```

When running `sphinx-build -M doctest`, observe failure:
```
**********************************************************************                                                                                                                                                              
1 item had failures:
   1 of   1 in default
1 test in 1 item.
0 passed and 1 failed.
***Test Failed*** 1 failure.

Doctest summary
===============
    1 test
    1 failure in tests
    0 failures in setup code
    0 failures in cleanup code
build finished with problems.
```

Changing the `..testsetup` directive group to `default` makes the build succeed, even though `doctest_test_doctest_blocks` specifies that doctest blocks should be added to a `Custom` group

### Environment Information

```text
Platform:              win32; (Windows-11-10.0.26100-SP0)
Python version:        3.13.0 (tags/v3.13.0:60403a5, Oct  7 2024, 09:38:07) [MSC v.1941 64 bit (AMD64)])
Python implementation: CPython
Sphinx version:        8.2.3
Docutils version:      0.21.2
Jinja2 version:        3.1.6
Pygments version:      2.19.2
```

### Sphinx extensions

```python
extensions = [
    ""sphinx.ext.autodoc"",
    ""sphinx.ext.doctest"",
]
```

### Additional context

It seems to me that the configuration variable simply isn't wired through:
https://github.com/sphinx-doc/sphinx/blob/9a08711e0e18c63c609070aa0a79019b4db45a78/sphinx/ext/doctest.py#L463",gemini-2.5-pro,gemini,5497,694,4520,10711,120,141,2.11,2970,FALSE,FALSE,0,0,0,0,41,91,132,N/A,N/A,N/A,N/A,TRUE,2342,2,9,0,1,1,2,120,141,2.11,2974,1.547675609588623,42.85486626625061,N/A,58.607999324798584,103.01054120063782
2025-11-15T23:53:55.407137,sphinx-doc/sphinx,f578eedb106f696796d47ac2f1d14de269b97e08,sphinx/domains/python/_object.py,tests/test_domains/test_domain_py.py,5,0,5,5,5,5,33,Fix incorrect index treatment of the ``:canonical:`` option in the ``py:type`` directive (#13926),Fix incorrect index treatment of :canonical: option in py:type,"## Purpose

Previously, two aliases for the same canonical type resulted in a duplicate object description warning.
",gemini-2.5-pro,gemini,4263,641,12617,17521,68,63,3.36,1754,FALSE,FALSE,0,0,0,0,28,27,55,N/A,N/A,N/A,N/A,TRUE,2346,0,9,0,11,5,16,69,65,3.36,1762,1.5283052921295166,106.34460544586182,N/A,58.625420570373535,166.49833130836487
2025-11-16T00:05:50.837312,sympy/sympy,fd5fc5b874f715083904a593c1f446f4c9581a20,sympy/ntheory/elliptic_curve.py,sympy/ntheory/tests/test_elliptic_curve.py,2,0,5,2,5,5,1,Fix EllipticCurvePoint.neg to return canonical infinity point,`EllipticCurvePoint.neg` returns non-canonical infinity point,"## Description
`EllipticCurvePoint.point_at_infinity(curve)` defines the identity as `(0, 1, 0)`. However `EllipticCurvePoint.__neg__` always returns `(x, -y - a1*x - a3, z)` even when `z == 0`. For the infinity point this produces `(0, -1, 0)` instead of `(0, 1, 0)`, so `-O` is not the canonical infinity and breaks invariants such as `P + (-P) = O` when comparing coordinates.

```python
from sympy.ntheory.elliptic_curve import EllipticCurve, EllipticCurvePoint

E = EllipticCurve(0, 0)
O = EllipticCurvePoint.point_at_infinity(E)
neg_O = -O
assert (neg_O.x, neg_O.y, neg_O.z) == (O.x, O.y, O.z)
```

```python
Traceback (most recent call last):
  File ""/data/src/test.py"", line 9, in <module>
    assert (neg_O.x, neg_O.y, neg_O.z) == (O.x, O.y, O.z)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError
```

## Easy fix:
```diff
diff --git a/sympy/ntheory/elliptic_curve.py b/sympy/ntheory/elliptic_curve.py
@@
     def __neg__(self):
-        return EllipticCurvePoint(self.x, -self.y - self._curve._a1*self.x - self._curve._a3, self.z, self._curve)
+        if self.z == 0:
+            return self  # O is its own inverse
+        return EllipticCurvePoint(
+            self.x,
+            -self.y - self._curve._a1*self.x - self._curve._a3,
+            self.z,
+            self._curve,
+        )
```

## Sympy versions tested on:
1.14.0

## Operating systems tested on:
Linux",gemini-2.5-pro,gemini,3546,260,2257,6063,72,70,1.83,1778,FALSE,FALSE,0,0,0,0,4,2,6,N/A,N/A,N/A,N/A,TRUE,12870,0,0,0,2,0,2,73,71,1.83,1787,3.068647623062134,22.898422956466675,N/A,692.5063676834106,718.4734382629395
2025-11-16T00:15:04.652015,sympy/sympy,5ac0484c401317fe5ff2e29430eb689ecae283f3,sympy/vector/coordsysrect.py,sympy/vector/tests/test_coordsysrect.py,1,1,5,1,5,5,14,Fix CoordSys3D simplification returning 0 for derivatives,simplified derivative reported as 0 when CoordSys coordinate is used as variable,"When using a transformation, simplification of a derivative involving a function in the coord variables gives 0 instead of the unevaluated result:

```python
from sympy import *
from sympy.vector import *
var('x y z')
f=Function('f')
S=CoordSys3D('S',transformation=((x,y,z),(x,y,z)),variable_names=('a','b','c'))
eq=gradient(f(S.a,S.b,S.c))

# fails
print(eq.simplify())

# method one hack
def q(self, **kw):
    return self
CoordSys3D._eval_simplify = q
print(eq.simplify())

# method two workaround (works in latest SymPy)

reps={S.a : Dummy(), S.b : Dummy(), S.c : Dummy()}
sper = {v:k for k,v in reps.items()}
print(eq.xreplace(reps).simplify().xreplace(sper))

## output
#0
#(Derivative(f(S.x, S.y, S.z), S.x))*S.i + (Derivative(f(S.x, S.y, S.z), S.y))*S.j + (Derivative(f(S.x, S.y, S.z), S.z))*S.k
#(Derivative(f(S.x, S.y, S.z), S.x))*S.i + (Derivative(f(S.x, S.y, S.z), S.y))*S.j + (Derivative(f(S.x, S.y, S.z), S.z))*S.k
```
The expected result is obtained if no transformation is used:, e.g. if `S = CoordSys3D('S',variable_names=('a','b','c'))`

Discussed [here on SO](https://stackoverflow.com/a/79811712/1089161).",gemini-2.5-pro,gemini,5528,718,19301,25547,108,98,3.26,3117,FALSE,FALSE,0,0,0,0,36,33,69,N/A,N/A,N/A,N/A,TRUE,12871,0,0,0,3,0,3,109,98,3.22,3127,3.068647623062134,157.99596428871155,N/A,553.6271688938141,714.6917808055878
2025-11-16T00:23:29.321906,sympy/sympy,f892f57a8afa8aae17517c7eb505c61f4768f9bc,sympy/series/gruntz.py,sympy/series/tests/test_limits.py,2,12,5,2,5,0,177,Fix TypeError in Limit.doit() due to missing cdir parameter,Limit doit raises TypeError,"`Limit(log(x)*cos(x), x, oo, dir='-').doit()`. raises `TypeError: Expr._eval_nseries() missing 1 required positional argument: 'cdir'`.",gemini-2.5-pro,gemini,5091,1295,19721,26107,138,139,2.38,2782,FALSE,FALSE,0,0,0,0,83,41,124,N/A,N/A,N/A,N/A,TRUE,12870,1,0,0,1,1,2,138,139,2.38,2786,3.068647623062134,159.60000920295715,N/A,504.51375102996826,667.1824078559875
